第七章 事务 

1.深入理解事务
1.1 ACID的含义
The safety guarantees provided by transactions are often described by the wellknown
acronym ACID, which stands for Atomicity, Consistency, Isolation, and Durability

原子性
By contrast, in the context of ACID, atomicity is not about concurrency. It does not
describe what happens if several processes try to access the same data at the same
time, because that is covered under the letter I, for isolation (see “Isolation” on page
225).

Rather, ACID atomicity describes what happens if a client wants to make several
writes, but a fault occurs after some of the writes have been processed—for example,
a process crashes, a network connection is interrupted, a disk becomes full, or some
integrity constraint is violated. If the writes are grouped together into an atomic
transaction, and the transaction cannot be completed (committed) due to a fault, then
the transaction is aborted and the database must discard or undo any writes it has
made so far in that transaction.

The ability to abort a transaction on error and have all writes from that transaction
discarded is the defining feature of ACID atomicity. Perhaps abortability would have
been a better term than atomicity, but we will stick with atomicity since that’s the
usual word.

一致性 Consistency
In the context of ACID, consistency refers to an application-specific notion of the
database being in a “good state.”

The idea of ACID consistency is that you have certain statements about your data
(invariants) that must always be true—for example, in an accounting system, credits
and debits across all accounts must always be balanced. If a transaction starts with a
database that is valid according to these invariants, and any writes during the transaction
preserve the validity, then you can be sure that the invariants are always satisfied.

However, this idea of consistency depends on the application’s notion of invariants,
and it’s the application’s responsibility to define its transactions correctly so that they
preserve consistency.

Atomicity, isolation, and durability are properties of the database, whereas consistency
(in the ACID sense) is a property of the application. The application may rely
on the database’s atomicity and isolation properties in order to achieve consistency,
but it’s not up to the database alone. Thus, the letter C doesn’t really belong in ACID.

隔离性 Isolation
Isolation in the sense of ACID means that concurrently executing transactions are
isolated from each other: they cannot step on each other’s toes. The classic database
textbooks formalize isolation as serializability, which means that each transaction can
pretend that it is the only transaction running on the entire database. The database
ensures that when the transactions have committed, the result is the same as if they
had run serially (one after another), even though in reality they may have run concurrently
[10].

However, in practice, serializable isolation is rarely used, because it carries a performance
penalty. Some popular databases, such as Oracle 11g, don’t even implement it.
In Oracle there is an isolation level called “serializable,” but it actually implements
something called snapshot isolation, which is a weaker guarantee than serializability
[8, 11]

持久性Durability
Durability is the promise that once a transaction has committed
successfully, any data it has written will not be forgotten, even if there is a
hardware fault or the database crashes

•When the power is suddenly cut, SSDs in particular have been shown to sometimes
violate the guarantees they are supposed to provide: even fsync isn’t guaranteed
to work correctly [12]. Disk firmware can have bugs, just like any other
kind of software [13, 14].
• Subtle interactions between the storage engine and the filesystem implementation
can lead to bugs that are hard to track down, and may cause files on disk to
be corrupted after a crash [15, 16].
• Data on disk can gradually become corrupted without this being detected [17]. If
data has been corrupted for some time, replicas and recent backups may also be
corrupted. In this case, you will need to try to restore the data from a historical
backup.
• One study of SSDs found that between 30% and 80% of drives develop at least
one bad block during the first four years of operation [18]. Magnetic hard drives
have a lower rate of bad sectors, but a higher rate of complete failure than SSDs.
• If an SSD is disconnected from power, it can start losing data within a few weeks,
depending on the temperature [19].

In practice, there is no one technique that can provide absolute guarantees. There are
only various risk-reduction techniques, including writing to disk, replicating to
remote machines, and backups—and they can and should be used together.As
always, it’s wise to take any theoretical “guarantees” with a healthy grain of salt

1.2单对象与多对象事务操作 Single-Object and Multi-Object Operations 

Multi-object transactions require some way of determining which read and write
operations belong to the same transaction. In relational databases, that is typically
done based on the client’s TCP connection to the database server: on any particular
connection, everything between a BEGIN TRANSACTION and a COMMIT statement is
considered to be part of the same transaction.

This is not ideal. If the TCP connection is interrupted, the transaction must be aborted. If the interruption
happens after the client has requested a commit but before the server acknowledges that the commit happened,
the client doesn’t know whether the transaction was committed or not. To solve this issue, a transaction
manager can group operations by a unique transaction identifier that is not bound to a particular TCP
connection. We will return to this topic in “The End-to-End Argument for Databases” on page 516.

单对象写入Single-object writes
Atomicity can be implemented using a log for crash recovery
(see “Making B-trees reliable” on page 82), and isolation can be implemented
using a lock on each object (allowing only one thread to access an object at any one
time)

Some databases also provide more complex atomic operations,iv such as an increment
operation, which removes the need for a read-modify-write cycle like that in
Figure 7-1. Similarly popular is a compare-and-set operation, which allows a write to
happen only if the value has not been concurrently changed by someone else


A transaction is usually understood as a mechanism for grouping multiple operations on multiple 
objects into one unit of execution.

多对象事务的需要The need for multi-object transactions
Many distributed datastores have abandoned multi-object transactions because they
are difficult to implement across partitions, and they can get in the way in some scenarios
where very high availability or performance is required.

However, in many other cases writes to several different objects need to be
coordinated:
• In a relational data model, a row in one table often has a foreign key reference to
a row in another table. (Similarly, in a graph-like data model, a vertex has edges
to other vertices.) Multi-object transactions allow you to ensure that these references
remain valid: when inserting several records that refer to one another, the
foreign keys have to be correct and up to date, or the data becomes nonsensical.
• In a document data model, the fields that need to be updated together are often
within the same document, which is treated as a single object—no multi-object
transactions are needed when updating a single document. However, document
databases lacking join functionality also encourage denormalization (see “Relational
Versus Document Databases Today” on page 38). When denormalized
information needs to be updated, like in the example of Figure 7-2, you need to
update several documents in one go. Transactions are very useful in this situation
to prevent denormalized data from going out of sync.
• In databases with secondary indexes (almost everything except pure key-value
stores), the indexes also need to be updated every time you change a value. These
indexes are different database objects from a transaction point of view: for example,
without transaction isolation, it’s possible for a record to appear in one index
but not another, because the update to the second index hasn’t happened yet.

处理错误与终止 Handling errors and aborts
ACID databases are based on this philosophy: if the database is in danger
of violating its guarantee of atomicity, isolation, or durability, it would rather abandon
the transaction entirely than allow it to remain half-finished.

the whole point of aborts is to enable safe retries

Although retrying an aborted transaction is a simple and effective error handling
mechanism, it isn’t perfect:
• If the transaction actually succeeded, but the network failed while the server tried
to acknowledge the successful commit to the client (so the client thinks it failed),
then retrying the transaction causes it to be performed twice—unless you have an
additional application-level deduplication mechanism in place.
• If the error is due to overload, retrying the transaction will make the problem
worse, not better. To avoid such feedback cycles, you can limit the number of
retries, use exponential backoff, and handle overload-related errors differently
from other errors (if possible).
• It is only worth retrying after transient errors (for example due to deadlock, isolation
violation, temporary network interruptions, and failover); after a permanent
error (e.g., constraint violation) a retry would be pointless.
• If the transaction also has side effects outside of the database, those side effects
may happen even if the transaction is aborted. For example, if you’re sending an
email, you wouldn’t want to send the email again every time you retry the transaction.
If you want to make sure that several different systems either commit or
abort together, two-phase commit can help (we will discuss this in “Atomic
Commit and Two-Phase Commit (2PC)” on page 354).
• If the client process fails while retrying, any data it was trying to write to the
database is lost.

2.若隔离级别 Weak Isolation Levels
If two transactions don’t touch the same data, they can safely be run in parallel,
because neither depends on the other. Concurrency issues (race conditions) only
come into play when one transaction reads data that is concurrently modified by
another transaction, or when two transactions try to simultaneously modify the same
data.

databases have long tried to hide concurrency issues from application
developers by providing transaction isolation. In theory, isolation should make
your life easier by letting you pretend that no concurrency is happening: serializable
isolation means that the database guarantees that transactions have the same effect as
if they ran serially (i.e., one at a time, without any concurrency).

Serializable isolation has a performance
cost, and many databases don’t want to pay that price [8]. It’s therefore
common for systems to use weaker levels of isolation, which protect against some
concurrency issues, but not all. Those levels of isolation are much harder to understand,
and they can lead to subtle bugs, but they are nevertheless used in practice
[23]

Concurrency bugs caused by weak transaction isolation are not just a theoretical
problem. They have caused substantial loss of money [24, 25], led to investigation by
financial auditors [26], and caused customer data to be corrupted [27].

In this section we will look at several weak (nonserializable) isolation levels that are
used in practice

2.1 读-提交 Read Committed
The most basic level of transaction isolation is read committed
1. When reading from the database, you will only see data that has been committed
(no dirty reads).
2. When writing to the database, you will only overwrite data that has been committed
(no dirty writes

No dirty reads防止脏读
There are a few reasons why it’s useful to prevent dirty reads:
• If a transaction needs to update several objects, a dirty read means that another
transaction may see some of the updates but not others. For example, in
Figure 7-2, the user sees the new unread email but not the updated counter. This
is a dirty read of the email. Seeing the database in a partially updated state is confusing
to users and may cause other transactions to take incorrect decisions.
• If a transaction aborts, any writes it has made need to be rolled back (like in
Figure 7-3). If the database allows dirty reads, that means a transaction may see
data that is later rolled back—i.e., which is never actually committed to the database.
Reasoning about the consequences quickly becomes mind-bending.

防止脏写No dirty writes
Transactions running at the read committed isolation level must prevent
dirty writes, usually by delaying the second write until the first write’s transaction has
committed or aborted.

实现读-提交 Implementing read committed
Read committed is a very popular isolation level. It is the default setting in Oracle
11g, PostgreSQL, SQL Server 2012, MemSQL, and many other databases [8].

Most commonly, databases prevent dirty writes by using row-level locks: when a
transaction wants to modify a particular object (row or document), it must first
acquire a lock on that object. It must then hold that lock until the transaction is committed
or aborted. Only one transaction can hold the lock for any given object; if
another transaction wants to write to the same object, it must wait until the first
transaction is committed or aborted before it can acquire the lock and continue. This
locking is done automatically by databases in read committed mode (or stronger isolation
levels).

How do we prevent dirty reads? One option would be to use the same lock, and to
require any transaction that wants to read an object to briefly acquire the lock and
then release it again immediately after reading. This would ensure that a read
couldn’t happen while an object has a dirty, uncommitted value (because during that
time the lock would be held by the transaction that has made the write).

However, the approach of requiring read locks does not work well in practice,
because one long-running write transaction can force many read-only transactions to
wait until the long-running transaction has completed. This harms the response time
of read-only transactions and is bad for operability: a slowdown in one part of an
application can have a knock-on effect in a completely different part of the application,
due to waiting for locks.

For that reason, most databasesvi prevent dirty reads using the approach illustrated in
Figure 7-4: for every object that is written, the database remembers both the old committed
value and the new value set by the transaction that currently holds the write
lock. While the transaction is ongoing, any other transactions that read the object are
simply given the old value. Only when the new value is committed do transactions
switch over to reading the new value.

2.2快照级别隔离和可重复读  Snapshot Isolation and Repeatable Read 
//看图片
nonrepeatable read or read skew (timing anomaly)
Read skew is considered acceptable under read committed isolation；

some situations cannot tolerate such temporary inconsistency:
Backups
Analytic queries and integrity checks

Snapshot isolation [28] is the most common solution to this problem. The idea is that
each transaction reads from a consistent snapshot of the database—that is, the transaction
sees all the data that was committed in the database at the start of the transaction.
Even if the data is subsequently changed by another transaction, each
transaction sees only the old data from that particular point in time.

Snapshot isolation is a boon for long-running, read-only queries such as backups and
analytics.

实现快照级别隔离 Implementing snapshot isolation
Like read committed isolation, implementations of snapshot isolation typically use
write locks to prevent dirty writes (see “Implementing read committed” on page 236),
which means that a transaction that makes a write can block the progress of another
transaction that writes to the same object. However, reads do not require any locks.
From a performance point of view, a key principle of snapshot isolation is readers
never block writers, and writers never block readers. This allows a database to handle
long-running read queries on a consistent snapshot at the same time as processing
writes normally, without any lock contention between the two.

The database must potentially keep
several different committed versions of an object, because various in-progress transactions
may need to see the state of the database at different points in time. Because it
maintains several versions of an object side by side, this technique is known as multiversion
concurrency control (MVCC).

//读-提交隔离与快照级别是两种不同的隔离，但是，mvcc技术能同时实现两者
If a database only needed to provide read committed isolation, but not snapshot isolation,
it would be sufficient to keep two versions of an object: the committed version
and the overwritten-but-not-yet-committed version. However, storage engines that
support snapshot isolation typically use MVCC for their read committed isolation
level as well. A typical approach is that read committed uses a separate snapshot for
each query, while snapshot isolation uses the same snapshot for an entire transaction.

//核心观念就是，快照，事务要看的是它开始时刻数据的快照
一致性快照的可见性规则 Visibility rules for observing a consistent snapshot
When a transaction reads from the database, transaction IDs are used to decide
which objects it can see and which are invisible. By carefully defining visibility rules,
the database can present a consistent snapshot of the database to the application. This
works as follows:
1. At the start of each transaction, the database makes a list of all the other transactions
that are in progress (not yet committed or aborted) at that time. Any writes
that those transactions have made are ignored, even if the transactions subsequently
commit.
2. Any writes made by aborted transactions are ignored.
3. Any writes made by transactions with a later transaction ID (i.e., which started
after the current transaction started) are ignored, regardless of whether those
transactions have committed.
4. All other writes are visible to the application’s queries.

Put another way, an object is visible if both of the following conditions are true:
• At the time when the reader’s transaction started, the transaction that created the
object had already committed.
• The object is not marked for deletion, or if it is, the transaction that requested
deletion had not yet committed at the time when the reader’s transaction started.

A long-running transaction may continue using a snapshot for a long time, continuing
to read values that (from other transactions’ point of view) have long been overwritten
or deleted. By never updating values in place but instead creating a new
version every time a value is changed, the database can provide a consistent snapshot
while incurring only a small overhead

//数据库的具体实现
索引与快照隔离 Indexes and snapshot isolation
//放到内存上
PostgreSQL has optimizations for avoiding
index updates if different versions of the same object can fit on the same page
[31].

//仅追加与写时复制技术 
Another approach is used in CouchDB, Datomic, and LMDB. Although they also use
B-trees (see “B-Trees” on page 79), they use an append-only/copy-on-write variant
that does not overwrite pages of the tree when they are updated, but instead creates a
new copy of each modified page.Parent pages, up to the root of the tree, are copied
and updated to point to the new versions of their child pages. Any pages that are not
affected by a write do not need to be copied, and remain immutable [33, 34, 35].

//没有严格的定义
可重复读与命名混淆 Repeatable read and naming confusion
Snapshot isolation is a useful isolation level, especially for read-only transactions.
However, many databases that implement it call it by different names. In Oracle it is
called serializable, and in PostgreSQL and MySQL it is called repeatable read [23].

IBM DB2 uses “repeatable read” to refer to serializability [8].

2.3 防止更新丢失 Preventing Lost Updates
The read committed and snapshot isolation levels we’ve discussed so far have been
primarily about the guarantees of what a read-only transaction can see in the presence
of concurrent writes.

There are several other interesting kinds of conflicts that can occur between concurrently
writing transactions. The best known of these is the lost update problem.

原子写操作 Atomic write operations
Many databases provide atomic update operations, which remove the need to implement
read-modify-write cycles in application code. They are usually the best solution
if your code can be expressed in terms of those operations.

Redis provides atomic
operations for modifying data structures such as priority queues

Not all writes can
easily be expressed in terms of atomic operations—for example, updates to a wiki
page involve arbitrary text editing.
注：
It is possible, albeit fairly complicated, to express the editing of a text document as a stream of atomic
mutations. See “Automatic Conflict Resolution” on page 174 for some pointers

Atomic operations are usually implemented by taking an exclusive lock on the object
when it is read so that no other transaction can read it until the update has been
applied. This technique is sometimes known as cursor stability [36, 37]. Another
option is to simply force all atomic operations to be executed on a single thread.

Unfortunately, object-relational mapping frameworks make it easy to accidentally
write code that performs unsafe read-modify-write cycles instead of using atomic
operations provided by the database [38].

显式锁定 Explicit locking
Another option for preventing lost updates, if the database’s built-in atomic operations
don’t provide the necessary functionality, is for the application to explicitly lock
objects that are going to be updated. Then the application can perform a readmodify-
write cycle, and if any other transaction tries to concurrently read the same
object, it is forced to wait until the first read-modify-write cycle has completed.

自动检测丢失更新 Automatically detecting lost updates
Atomic operations and locks are ways of preventing lost updates by forcing the read-modify-
write cycles to happen sequentially. An alternative is to allow them to execute
in parallel and, if the transaction manager detects a lost update, abort the transaction
and force it to retry its read-modify-write cycle.

An advantage of this approach is that databases can perform this check efficiently in
conjunction with snapshot isolation. Indeed, PostgreSQL’s repeatable read, Oracle’s
serializable, and SQL Server’s snapshot isolation levels automatically detect when a
lost update has occurred and abort the offending transaction. However, MySQL/
InnoDB’s repeatable read does not detect lost updates [23].

比较和设置 Compare-and-set
The purpose of this operation is to avoid lost updates by allowing an update to happen
only if the value has not changed since you last read it. If the current value does not
match what you previously read, the update has no effect, and the read-modify-write
cycle must be retried.

//多主节点与无主节点引出的问题
冲突解决与复制Conflict resolution and replication
In replicated databases (see Chapter 5), preventing lost updates takes on another
dimension: since they have copies of the data on multiple nodes, and the data can
potentially be modified concurrently on different nodes, some additional steps need
to be taken to prevent lost updates.

Locks and compare-and-set operations assume that there is a single up-to-date copy
of the data. However, databases with multi-leader or leaderless replication usually
allow several writes to happen concurrently and replicate them asynchronously, so
they cannot guarantee that there is a single up-to-date copy of the data. Thus, techniques
based on locks or compare-and-set do not apply in this context. (We will revisit
this issue in more detail in “Linearizability” on page 324.)

Instead, as discussed in “Detecting Concurrent Writes” on page 184, a common
approach in such replicated databases is to allow concurrent writes to create several
conflicting versions of a value (also known as siblings), and to use application code or
special data structures to resolve and merge these versions after the fact.

Atomic operations can work well in a replicated context, especially if they are commutative
(i.e., you can apply them in a different order on different replicas, and still
get the same result). For example, incrementing a counter or adding an element to a
set are commutative operations. That is the idea behind Riak 2.0 datatypes, which
prevent lost updates across replicas. When a value is concurrently updated by different
clients, Riak automatically merges together the updates in such a way that no
updates are lost [39].

On the other hand, the last write wins (LWW) conflict resolution method is prone to
lost updates, as discussed in “Last write wins (discarding concurrent writes)” on page
186. Unfortunately, LWW is the default in many replicated databases

2.4 写倾斜与幻读 Write Skew and Phantoms
定义写倾斜 Characterizing write skew

You can think of write skew as a generalization of the lost update problem. Write
skew can occur if two transactions read the same objects, and then update some of
those objects (different transactions may update different objects). In the special case
where different transactions update the same object, you get a dirty write or lost
update anomaly (depending on the timing).

We saw that there are various different ways of preventing lost updates. With write
skew, our options are more restricted
• Atomic single-object operations don’t help, as multiple objects are involved.
//自动防止写倾斜要求真正的串行化隔离
• The automatic detection of lost updates that you find in some implementations
of snapshot isolation unfortunately doesn’t help either: write skew is not automatically
detected in PostgreSQL’s repeatable read, MySQL/InnoDB’s repeatable
read, Oracle’s serializable, or SQL Server’s snapshot isolation level [23]. Automatically
preventing write skew requires true serializable isolation (see “Serializability”
on page 251).
//应用层面解决，采用触发器或物化视图
• Some databases allow you to configure constraints, which are then enforced by
the database (e.g., uniqueness, foreign key constraints, or restrictions on a particular
value). However, in order to specify that at least one doctor must be on call,
you would need a constraint that involves multiple objects. Most databases do
not have built-in support for such constraints, but you may be able to implement
them with triggers or materialized views, depending on the database [42].
//应用层面解决，对返回的结果加锁
• If you can’t use a serializable isolation level, the second-best option in this case is
probably to explicitly lock the rows that the transaction depends on. In the doctors
example, you could write something like the following:
BEGIN TRANSACTION;
SELECT * FROM doctors
WHERE on_call = true
AND shift_id = 1234 FOR UPDATE;
UPDATE doctors
SET on_call = false
WHERE name = 'Alice'
AND shift_id = 1234;
COMMIT;
As before, FOR UPDATE tells the database to lock all rows returned by this
query.

更多写倾斜的例子 More examples of write skew
Meeting room booking system
Multiplayer game

//重复的用户名 唯一性检查 
Claiming a username
Fortunately, a unique constraint is a simple solution here (the second transaction
that tries to register the username will be aborted due to violating the constraint

Preventing double-spending

This effect, where a write in one transaction changes the result of a search query in
another transaction, is called a phantom [3]

实体化冲突 Materializing conflicts
This approach is called materializing conflicts, because it takes a phantom and turns it
into a lock conflict on a concrete set of rows that exist in the database [11]. Unfortunately,
it can be hard and error-prone to figure out how to materialize conflicts, and
it’s ugly to let a concurrency control mechanism leak into the application data model.
For those reasons, materializing conflicts should be considered a last resort if no
alternative is possible. A serializable isolation level is much preferable in most cases.


3.串行化 Serializability
Serializable isolation is usually regarded as the strongest isolation level. It guarantees
that even though transactions may execute in parallel, the end result is the same as if
they had executed one at a time, serially, without any concurrency. Thus, the database
guarantees that if the transactions behave correctly when run individually, they continue
to be correct when run concurrently—in other words, the database prevents all
possible race conditions

3.1 实际串行执行 Actual Serial Execution 
The simplest way of avoiding concurrency problems is to remove the concurrency
entirely: to execute only one transaction at a time, in serial order, on a single thread.

Even though this seems like an obvious idea, database designers only fairly recently—
around 2007—decided that a single-threaded loop for executing transactions was feasible
[45].

//单线程执行事务，但是吞吐量受限于单个CPU，需要重新构造事务，
The approach of executing transactions serially is implemented in VoltDB/H-Store,
Redis, and Datomic [46, 47, 48]. A system designed for single-threaded execution can
sometimes perform better than a system that supports concurrency, because it can
avoid the coordination overhead of locking. However, its throughput is limited to
that of a single CPU core. In order to make the most of that single thread, transactions
need to be structured differently from their traditional form.

采用存储过程封装事务 Encapsulating transactions in stored procedures
Unfortunately, humans are very slow to make up their minds and respond. If a database
transaction needs to wait for input from a user, the database needs to support a
potentially huge number of concurrent transactions, most of them idle. Most databases
cannot do that efficiently, and so almost all OLTP applications keep transactions
short by avoiding interactively waiting for a user within a transaction. On the
web, this means that a transaction is committed within the same HTTP request—a
transaction does not span multiple requests. A new HTTP request starts a new transaction.

In this interactive style of transaction, a lot of time is spent in network communication
between the application and the database. If you were to disallow concurrency in
the database and only process one transaction at a time, the throughput would be
dreadful because the database would spend most of its time waiting for the application
to issue the next query for the current transaction. In this kind of database, it’s
necessary to process multiple transactions concurrently in order to get reasonable
performance.

For this reason, systems with single-threaded serial transaction processing don’t
allow interactive multi-statement transactions. Instead, the application must submit
the entire transaction code to the database ahead of time, as a stored procedure

存储过程优缺点 Pros and cons of stored procedures
//编写存储过程的SQL不是通用语言，代码运行在服务器中
Each database vendor has its own language for stored procedures (Oracle has PL/
SQL, SQL Server has T-SQL, PostgreSQL has PL/pgSQL, etc.). These languages
haven’t kept up with developments in general-purpose programming languages,
so they look quite ugly and archaic from today’s point of view, and they lack the
ecosystem of libraries that you find with most programming languages.
• Code running in a database is difficult to manage: compared to an application
server, it’s harder to debug, more awkward to keep in version control and deploy,
trickier to test, and difficult to integrate with a metrics collection system for
monitoring.
• A database is often much more performance-sensitive than an application server,
because a single database instance is often shared by many application servers. A
badly written stored procedure (e.g., using a lot of memory or CPU time) in a
database can cause much more trouble than equivalent badly written code in an
application server

However, those issues can be overcome. Modern implementations of stored procedures
have abandoned PL/SQL and use existing general-purpose programming languages
instead: VoltDB uses Java or Groovy, Datomic uses Java or Clojure, and Redis
uses Lua.

//Redis,内存
With stored procedures and in-memory data, executing all transactions on a single
thread becomes feasible. As they don’t need to wait for I/O and they avoid the overhead
of other concurrency control mechanisms, they can achieve quite good
throughput on a single thread.

//在节点上执行存储过程
VoltDB also uses stored procedures for replication: instead of copying a transaction’s
writes from one node to another, it executes the same stored procedure on each replica.
VoltDB therefore requires that stored procedures are deterministic (when run on
different nodes, they must produce the same result). If a transaction needs to use the
current date and time, for example, it must do so through special deterministic APIs.

分区 Partitioning
Executing all transactions serially makes concurrency control much simpler, but limits
the transaction throughput of the database to the speed of a single CPU core on a
single machine. Read-only transactions may execute elsewhere, using snapshot isolation,
but for applications with high write throughput, the single-threaded transaction
processor can become a serious bottleneck.

In order to scale to multiple CPU cores, and multiple nodes, you can potentially partition
your data (see Chapter 6), which is supported in VoltDB. If you can find a way
of partitioning your dataset so that each transaction only needs to read and write data
within a single partition, then each partition can have its own transaction processing
thread running independently from the others. In this case, you can give each CPU
core its own partition, which allows your transaction throughput to scale linearly
with the number of CPU cores [47].

However, for any transaction that needs to access multiple partitions, the database
must coordinate the transaction across all the partitions that it touches. The stored
procedure needs to be performed in lock-step across all partitions to ensure serializability
across the whole system.

Since cross-partition transactions have additional coordination overhead, they are
vastly slower than single-partition transactions. VoltDB reports a throughput of
about 1,000 cross-partition writes per second, which is orders of magnitude below its
single-partition throughput and cannot be increased by adding more machines [49].

Whether transactions can be single-partition depends very much on the structure of
the data used by the application. Simple key-value data can often be partitioned very
easily, but data with multiple secondary indexes is likely to require a lot of crosspartition
coordination (see “Partitioning and Secondary Indexes” on page 206).

总结 Summary of serial execution
Serial execution of transactions has become a viable way of achieving serializable isolation
within certain constraints:
• Every transaction must be small and fast, because it takes only one slow transaction
to stall all transaction processing.
• It is limited to use cases where the active dataset can fit in memory. Rarely
accessed data could potentially be moved to disk, but if it needed to be accessed
in a single-threaded transaction, the system would get very slow. //x
• Write throughput must be low enough to be handled on a single CPU core, or
else transactions need to be partitioned without requiring cross-partition coordination.
• Cross-partition transactions are possible, but there is a hard limit to the extent to
which they can be used.

注：x
If a transaction needs to access data that’s not in memory, the best solution may be to abort the transaction,
asynchronously fetch the data into memory while continuing to process other transactions, and then
restart the transaction when the data has been loaded. This approach is known as anti-caching, as previously
mentioned in “Keeping everything in memory” on page 88.

3.2 两阶段加锁 Two-Phase Locking (2PL)
For around 30 years, there was only one widely used algorithm for serializability in
databases: two-phase locking (2PL).
Sometimes called strong strict two-phase locking (SS2PL) to distinguish it from other variants of 2PL

实现Implementation of two-phase locking
//就是读写锁，读共享，写独占
The blocking of readers and writers is implemented by a having a lock on each object
in the database. The lock can either be in shared mode or in exclusive mode. 

This is where the name “twophase”
comes from: the first phase (while the transaction is executing) is when
the locks are acquired, and the second phase (at the end of the transaction) is
when all the locks are released.

性能 Performance of two-phase locking
The big downside of two-phase locking, and the reason why it hasn’t been used by
everybody since the 1970s, is performance: transaction throughput and response
times of queries are significantly worse under two-phase locking than under weak
isolation

This is partly due to the overhead of acquiring and releasing all those locks, but more
importantly due to reduced concurrency. By design, if two concurrent transactions
try to do anything that may in any way result in a race condition, one has to wait for
the other to complete

Traditional relational databases don’t limit the duration of a transaction, because
they are designed for interactive applications that wait for human input. Consequently,
when one transaction has to wait on another, there is no limit on how long it
may have to wait.

For this reason, databases running 2PL can have quite unstable latencies, and they
can be very slow at high percentiles (see “Describing Performance” on page 13) if
there is contention in the workload

Although deadlocks can happen with the lock-based read committed isolation level,
they occur much more frequently under 2PL serializable isolation (depending on the
access patterns of your transaction). This can be an additional performance problem:
when a transaction is aborted due to deadlock and is retried, it needs to do its work
all over again. If deadlocks are frequent, this can mean significant wasted effort

谓词锁 Predicate locks 
//类似于独占/共享锁
Conceptually, we need a predicate lock [3]. It works similarly
to the shared/exclusive lock described earlier, but rather than belonging to a
particular object (e.g., one row in a table), it belongs to all objects that match some
search condition

The key idea here is that a predicate lock applies even to objects that do not yet exist
in the database, but which might be added in the future (phantoms). If two-phase
locking includes predicate locks, the database prevents all forms of write skew and
other race conditions, and so its isolation becomes serializable.

索引区间锁 Index-range locks
Unfortunately, predicate locks do not perform well: if there are many locks by active
transactions, checking for matching locks becomes time-consuming. For that reason,
most databases with 2PL actually implement index-range locking (also known as nextkey
locking), which is a simplified approximation of predicate locking [41, 50].

It’s safe to simplify a predicate by making it match a greater set of objects

Index-range locks are not as precise as predicate locks would be (they may lock a bigger range of
objects than is strictly necessary to maintain serializability), but since they have much
lower overheads, they are a good compromise.objects than is strictly necessary to maintain serializability), but since they have much
lower overheads, they are a good compromise.

If there is no suitable index where a range lock can be attached, the database can fall
back to a shared lock on the entire table. This will not be good for performance, since
it will stop all other transactions writing to the table, but it’s a safe fallback position.

3.3 可串行化的快照隔离 Serializable Snapshot Isolation (SSI)
Perhaps not: an algorithm called serializable snapshot isolation (SSI) is very promising.
It provides full serializability, but has only a small performance penalty compared
to snapshot isolation. SSI is fairly new: it was first described in 2008 [40] and is
the subject of Michael Cahill’s PhD thesis [51].

Today SSI is used both in single-node databases (the serializable isolation level in
PostgreSQL since version 9.1 [41]) and distributed databases (FoundationDB uses a
similar algorithm). As SSI is so young compared to other concurrency control mechanisms,
it is still proving its performance in practice, but it has the possibility of being
fast enough to become the new default in the future.

By contrast, serializable snapshot isolation is an optimistic concurrency control technique.
Optimistic in this context means that instead of blocking if something potentially
dangerous happens, transactions continue anyway, in the hope that everything
will turn out all right. When a transaction wants to commit, the database checks
whether anything bad happened (i.e., whether isolation was violated); if so, the trans‐
action is aborted and has to be retried. Only transactions that executed serializably
are allowed to commit.

Optimistic concurrency control is an old idea [52], and its advantages and disadvantages
have been debated for a long time [53]. It performs badly if there is high contention
(many transactions trying to access the same objects), as this leads to a high
proportion of transactions needing to abort. If the system is already close to its maximum
throughput, the additional transaction load from retried transactions can make
performance worse.

//commutative atomic operations,并发原子增量？
However, if there is enough spare capacity, and if contention between transactions is
not too high, optimistic concurrency control techniques tend to perform better than
pessimistic ones. Contention can be reduced with commutative atomic operations:
for example, if several transactions concurrently want to increment a counter, it
doesn’t matter in which order the increments are applied (as long as the counter isn’t
read in the same transaction), so the concurrent increments can all be applied
without conflicting.

//额外增加算法检测写之间的串行冲突
As the name suggests, SSI is based on snapshot isolation—that is, all reads within a
transaction are made from a consistent snapshot of the database (see “Snapshot Isolation
and Repeatable Read” on page 237). This is the main difference compared to earlier
optimistic concurrency control techniques. On top of snapshot isolation, SSI adds
an algorithm for detecting serialization conflicts among writes and determining
which transactions to abort.

//上面算法的细节
基于过期条件 Decisions based on an outdated premise
How does the database know if a query result might have changed? There are two
cases to consider:
• Detecting reads of a stale MVCC object version (uncommitted write occurred
before the read)
• Detecting writes that affect prior reads (the write occurs after the read)

//细节一，因为采用MVCC技术，A事务在查询时会忽略B事务已经写入但是未提交的数据，之后，
//B事务提交，导致A事务的查询结果发生了变化
Detecting stale MVCC reads

//看图7-10 
Why wait until committing? Why not abort transaction 43 immediately when the
stale read is detected? Well, if transaction 43 was a read-only transaction, it wouldn’t
need to be aborted, because there is no risk of write skew. At the time when transaction
43 makes its read, the database doesn’t yet know whether that transaction is
going to later perform a write. Moreover, transaction 42 may yet abort or may still be
uncommitted at the time when transaction 43 is committed, and so the read may
turn out not to have been stale after all. By avoiding unnecessary aborts, SSI preserves
snapshot isolation’s support for long-running reads from a consistent snapshot.

//细节2
//A事务和B事务读取到相同的结果，都对查询结果进行了修改，但是A事务首先提交了结果，因此，B事务会
//中止自己所做的修改
Detecting writes that affect prior reads

可串行化快照隔离的性能 Performance of serializable snapshot isolation
//权衡跟踪读写事务的细粒度
As always, many engineering details affect how well an algorithm works in practice.
For example, one trade-off is the granularity at which transactions’ reads and writes
are tracked. If the database keeps track of each transaction’s activity in great detail, it
can be precise about which transactions need to abort, but the bookkeeping overhead
can become significant. Less detailed tracking is faster, but may lead to more transactions
being aborted than strictly necessary.

//没有读写锁，读写互不干扰
Compared to two-phase locking, the big advantage of serializable snapshot isolation
is that one transaction doesn’t need to block waiting for locks held by another transaction.
Like under snapshot isolation, writers don’t block readers, and vice versa. This
design principle makes query latency much more predictable and less variable. In
particular, read-only queries can run on a consistent snapshot without requiring any
locks, which is very appealing for read-heavy workloads.
Compared to serial execution, serializable snapshot isolation is not limited to the
throughput of a single CPU core: FoundationDB distributes the detection of serialization
conflicts across multiple machines, allowing it to scale to very high throughput.
Even though data may be partitioned across multiple machines, transactions can
read and write data in multiple partitions while ensuring serializable isolation [54].

//中止事务的操作越多，越影响性能，
The rate of aborts significantly affects the overall performance of SSI. For example, a
transaction that reads and writes data over a long period of time is likely to run into
conflicts and abort, so SSI requires that read-write transactions be fairly short (longrunning
read-only transactions may be okay). However, SSI is probably less sensitive
to slow transactions than two-phase locking or serial execution.