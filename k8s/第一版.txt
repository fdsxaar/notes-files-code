k8s in action 1st edition 

第七章 
7.3 为容器设置环境变量
//覆盖参数
kind: Pod
spec:
  containers:
  - image: some/image
    command: ["/bin/command"]
    args: ["arg1", "arg2", "arg3"]

//在器级别提供环境变量参数
kind: Pod
spec:
 containers:
 - image: luksa/fortune:env
   env:
   - name: INTERVAL
     value: "30"
   name: html-generator
...
//引用环境变量
env:
- name: FIRST_VAR
  value: "foo"
- name: SECOND_VAR
  value: "$(FIRST_VAR)bar"

Con句Map 中的键名必须是一个合法的DNS 子域，仅包含数字字母、破
折号、下画线以及园点。首位的圆，点符号是可选的
//从字面量创建congimap
$ kubectl create configmap fortune-config --from-literal=sleep-interval=25
configmap "fortune-config" created
$ kubectl create configmap myconfigmap
➥   --from-literal=foo=bar --from-literal=bar=baz --from-literal=one=two
$ kubectl create -f fortune-config.yaml

//从文件创建configmap，并将文件内容单独存储为ConfigMap 中的条目
$ kubectl create configmap my-config --from-file=customkey=config-file.conf
//从文件夹创建configmap, 每个文件是一个条目 
$ kubectl create configmap my-config --from-file=/path/to/dir #没有/

7.4.3 给容器传递ConfigMap 条目作为环境变量
//给容器的环境变量传递ConfigMap 的条目
//可以标记对ConfigMap 的引用是可选的（设直configMapKeyRef .
//optional: true ）。这样，即使ConfigMap 不存在，容器也能正常启动。
apiVersion: v1
kind: Pod
metadata:
  name: fortune-env-from-configmap
spec:
  containers:
  - image: luksa/fortune:env
    env:
    - name: INTERVAL             //环境变量
      valueFrom:
        configMapKeyRef:
          name: fortune-config
          key: sleep-interval    //为变量设定值 
...

//通过envFrom属性字段将所有条目暴露作为环境变量
spec:
  containers:
  - image: some-image
    envFrom:
    - prefix: CONFIG_           //为所有条目添加前缀
      configMapRef:
        name: my-config-map
...
config_foo-bar不是合法的环境变量名 

//使用条目作为参数值
apiVersion: v1
kind: Pod
metadata:
  name: fortune-args-from-configmap
spec:
  containers:
  - image: luksa/fortune:args
    env:
    - name: INTERVAL            //定义环境变量 
      valueFrom:
        configMapKeyRef:
          name: fortune-config
          key: sleep-interval  //给变量赋值
    args: ["$(INTERVAL)"]      //在参数中引用环境变量 
...

//configMap 卷会将Co nfigMap 中的每个条目均暴露成一个文件
//副作用：挂载引用configMap的卷会隐藏/etc/nginx/conf.d目录下的所有文件 
apiVersion: v1
kind: Pod
metadata:
  name: fortune-configmap-volume
spec:
  containers:
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    ...
    - name: config                          //挂载配置卷
      mountPath: /etc/nginx/conf.d
      readOnly: true
    ...
  volumes:
  ...
  - name: config                            //挂载卷引用configMap
    configMap:                              
      name: fortune-config
  ...

kubectl get configmap fortune-config -o yaml //考察configMap 
$ kubectl get configmap fortune-config -o yaml
apiVersion: v1
data:
  my-nginx-config.conf: |                    #管道符表示后续条目是多行字面量
    server {
      listen              80;
      server_name         www.kubia-example.com;

      gzip on;
      gzip_types text/plain application/xml;

      location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
      }
    }
  sleep-interval: |
    25
kind: ConfigMap
...

/卷内暴露指定的ConfigMap条目
//通过卷的items 属性能够指定哪些条目会被暴露作为configMap卷中的文件
volumes:
  - name: config
    configMap:
      name: fortune-config
      items:
      - key: my-nginx-config.conf
        path: gzip.conf                //指定对应的文件名
		
//volumeMount额外的subPath字段可以被用作挂载卷中的某个
//独立文件或者是文件夹，无须挂载完整卷
spec:
  containers:
  - image: some/image
    volumeMounts:
    - name: myvolume
      mountPath: /etc/someconfig.conf
      subPath: myconfig.conf              //仅挂载单个文件 
	  
	  
//设置权限
volumes:
  - name: config
    configMap:
      name: fortune-config
      defaultMode: "6600"   //设置

//ConfigMap被更新之后， 卷中引用它的所有文件也会相应更新， 进程发现文件
//被改变之后进行重载	
kubectl edit configmap fortune-config //热更新

//Secret也是资源对象;每个pod挂载了包含三个文件的secret卷
kubectl get secrets
kubectl describe secrets

//创建一个Secret
$ kubectl create secret generic fortune-https --from-file=https.key
➥   --from-file=https.cert --from-file=foo
secret "fortune-https" created

Base64 encoding allows you to include the binary data in YAML or JSON, 
which are both plain-text formats
kind: Secret
apiVersion: v1
stringData:
  foo: plain text           //写入纯文本；stringData的字段不可读，编码后显示在data中
data:
  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ...
  https.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE...

//Secret 卷存储于内存
$ kubectl exec fortune-https -c web-server -- mount | grep certs
tmpfs on /etc/nginx/certs type tmpfs (ro,relatime)

 apiVersion: v1
kind: Pod
metadata:
  name: fortune-https
spec:
  containers:
  - image: luksa/fortune:env
    name: html-generator
    env:
    - name: INTERVAL
      valueFrom:
        configMapKeyRef:
          name: fortune-config
          key: sleep-interval
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    - name: config
      mountPath: /etc/nginx/conf.d
      readOnly: true
    - name: certs
      mountPath: /etc/nginx/certs/   //挂载至此
      readOnly: true
    ports:
    - containerPort: 80
    - containerPort: 443
  volumes:
  - name: html
    emptyDir: {}
  - name: config
    configMap:
      name: fortune-config
      items:
      - key: my-nginx-config.conf
        path: https.conf
  - name: certs                    //要挂载的卷
    secret:
      secretName: fortune-https

//通过环境变量暴露Secret 条目
env:
    - name: FOO_SECRET
      valueFrom:
        secretKeyRef:
          name: fortune-https
          key: foo
Kubernetes 允许通过环境变量暴露Secret，然而此特性的使用往往不是一个好
主意。应用程序通常会在错误报告时转储环境变量，或者是启动时打印在应用日志
中，无意中暴露了Secret 信息。另外，子进程会继承父进程的所有环境变量，如果
是通过第三方二进制程序启动应用，你并不知道它使用敏感数据做了什么

运行一个镜像来源于私有仓库的pod 时，需要做以下两件事：
．创建包含Docker 镜像仓库证书的Secre t 。
• pod 定义中的imagePullSecrets 宇段引用该Secret 。
$ kubectl create secret docker-registry mydockerhubsecret \
  --docker-username=myusername --docker-password=mypassword \
  --docker-email=my.email@provider.com

为了Kub em etes 从私有镜像仓库拉取镜像时能够使用Secret ，需要在pod 定义
中指定docker - registry Secret 的名称
apiVersion: v1
kind: Pod
metadata:
  name: private-pod
spec:
  imagePullSecrets:
  - name: mydockerhubsecret
  containers:
  - image: username/private:tag
    name: main

==========================Chapter 8=================
Accessing pod metadata and other resources from applications
//通过环境变量暴露元数据
apiVersion: v1
kind: Pod
metadata:
  name: downward
spec:
  containers:
  - name: main
    image: busybox
    command: ["sleep", "9999999"]
    resources:
      requests:
        cpu: 15m
        memory: 100Ki
      limits:
        cpu: 100m
        memory: 4Mi
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: SERVICE_ACCOUNT
      valueFrom:
        fieldRef:
          fieldPath: spec.serviceAccountName
    - name: CONTAINER_CPU_REQUEST_MILLICORES
      valueFrom:
        resourceFieldRef:
          resource: requests.cpu
          divisor: 1m
    - name: CONTAINER_MEMORY_LIMIT_KIBIBYTES
      valueFrom:
        resourceFieldRef:
          resource: limits.memory
          divisor: 1Ki
kubectl exec downward env //查看环境变量


//通过downwardAPI卷暴露元数据
//必须使用downwardAPI卷来暴露pod标签或注解
apiVersion: v1
kind: Pod
metadata:
  name: downward
  labels:
    foo: bar
  annotations:
    key1: value1
    key2: |
      multi
      line
      value
spec:
  containers:
  - name: main
    image: busybox
    command: ["sleep", "9999999"]
    resources:
      requests:
        cpu: 15m                 //m=milli-core，千分之一核，1代表整个核
        memory: 100Ki
      limits:
        cpu: 100m
        memory: 4Mi
    volumeMounts:
    - name: downward
      mountPath: /etc/downward
  volumes:
  - name: downward
    downwardAPI:
      items:
      - path: "podName"
        fieldRef:
          fieldPath: metadata.name      //挂载到文件/etc/downward/podName,内容为metadata.name
      - path: "podNamespace"            //的域，是键值对
        fieldRef:
          fieldPath: metadata.namespace
      - path: "labels"

        fieldRef:

          fieldPath: metadata.labels

      - path: "annotations"
        fieldRef:
          fieldPath: metadata.annotations
      - path: "containerCpuRequestMilliCores"
        resourceFieldRef:
          containerName: main
          resource: requests.cpu
          divisor: 1m                       //除数，用上面的单位除以这个，15个单位的核
      - path: "containerMemoryLimitBytes"
        resourceFieldRef:
          containerName: main               //容器级元数据，指定容器名 
          resource: limits.memory
          divisor: 1ki
kubect1 exec downward ls -lL /etc/downward		  
与configMAp和secret卷一样，可以通过pod定义中downward.AP工
卷的defaultMode属性来改变文件的访问权限设置。
//自动更新吗
当标签和注解被修改后，Kubemetes会更新存有相关信息的文件
使用卷的方式，可以传递一个容器的资源字段到另一个容器，因为卷的定义
是基于pod，同一个pod的容器可以沟通

$ kubect1 proxy //访问proxy与api交互

从pod内部与API服务器进行交互,令牌、密钥等来访问服务器API

//最简单的方式，但是运行失败 
通过ambassador 容器简化与API 服务器的交互

//各种语言的库、包
使用客户端库与API服务器交互



-------------------------第九章----------------------------
Chapter 9. Deployments: updating applications declaratively
有以下两种方法可以更新所有pod:
• 直接删除所有现有的pod, 然后创建新的pod。
• 也可以先创建新的pod, 并等待它们成功运行之后， 再删除旧的pod。可以
先创建所有新的pod, 然后一次性删除所有旧的pod, 或者按顺序创建新的
pod, 然后逐渐删除旧的pod。

在使用Deployment 时， 实际的pod
是由Deployment 的Replicaset 创建和管理的， 而不是由Deployment 直接创建和管
理的
k create -f kubia-deployment-v1.yaml --record //--record 记录历史版本号
k rollout status deployment kubia //查看部署状态 
k get replicasets.apps //查看deployment的哈希值

默认策略是执行滚动更新（策略名为RollingUpdate)。另一种策略为Recreate,
它会一次性删除所有旧版本的pod, 然后创建新的pod,
kubectl patch deployment kubia -p '{"spec": {"minReadySeconds": 10}}' //更新一个值 
//重新设定镜像，触发滚动升级
kubectl set image deployment kubia nodejs=luksa/kubia:v2

如果Deployment 中的pod 模板引用了一个ConfigMap (或Secret), 那么
更改ConfigMap 资原本身将不会触发升级操作。如果真的需要修改应用程序的配置
并想触发更新的话， 可以通过创建一个新的ConfigMap 并修改pod 模板引用新的
ConfigMap。

kubectl get rs //列出资源

kubectl rollout status deployment kubia

//升级到错误版本
kubectl set image deployment kubia nodejs=luksa/kubia:v3
//回滚一次升级
k rollout undo deployment kubia 
//回滚历史；与教材有出入，只显示v1版本，未显示v2 v3 版本的历史
k rollout history deployment kubia
//通过指定Deployment 的revisionHistoryLimit 属性来限制历史版本数量。默认值是2
$ kubectl rollout undo deployment kubia - -to-revision=l //回滚到特定版本 
 
 spec:
  strategy:
    rollingUpdate:
      maxSurge: 1             //pod的总数目不能超过这个数目
      maxUnavailable: 0       //可用的数目不能少于期望数目减去maxUnavailable
    type: RollingUpdate

k rollout pause deployment kubia 

//使用kubectl apply -f 来升级Deployment
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  minReadySeconds: 10       //pod从状态ready-->available需要的时间/持续时间 
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v3
        name: nodejs
        readinessProbe:
          periodSeconds: 1
          httpGet:
            path: /
            port: 8080

//当一个pod的所有容器的readiness探测返回成功时，pod处于ready状态，在被
//视为available之前，ready状态之后，还要等待minReadySeconds秒数；
//如果readiness探测失败，新版本回滚就会被阻塞 
The minReadySeconds property specifies how long a newly created pod should 
be ready before the pod is treated as available. Until the pod is available, 
the rollout process will not continue (remember the maxUnavailable property?). 
A pod is ready when readiness probes of all its containers return a success. 
If a new pod isn’t functioning properly and its readiness probe starts failing 
before minReadySeconds have passed, the rollout of the new version will effectively 
be blocked.

//配置progressDeadlineSeconds属性，超过这个值后，就会认为Deployment失败
The time after which the Deployment is considered failed is configurable 
through the progressDeadlineSeconds property in the Deployment spec.
//yaml文件的标签为kubia,可以重用之前创建的service kubia,该service 的SELECTOR 为app=kubia
labels:
   app: kubia
   

-----------------------Chapter10----------------------------
//pod模板引用特定的持久卷声明，所有的副本将会使用相同的持久卷声明，共享底层的持久卷
Chapter 10. StatefulSets: deploying replicated stateful application
ReplicaSets create multiple pod replicas from a single pod template. 
These replicas don’t differ from each other, apart from their name and IP address. 
If the pod template includes a volume, which refers to a specific PersistentVolumeClaim, 
all replicas of the ReplicaSet will use the exact same PersistentVolumeClaim and 
therefore the same PersistentVolume bound by the claim.

//运行每个实例都有单独存储的多副本
但是在Kubemetes 中， 每次重新调度一个
pod, 这个新的pod就有一个新的主机名和IP地址， 这样就要求当集群中任何一个
成员被重新调度后， 整个应用集群都需要重新配置

Statefulset创建的pod副
本并不是完全一样的。每个pod都可以拥有一组独立的数据卷（持久化状态）

//发现服务 
For this reason, a StatefulSet requires you to create a corresponding governing 
headless Service that’s used to provide the actual network identity to each pod. 
Through this Service, each pod gets its own DNS entry, so its peers and possibly 
other clients in the cluster can address the pod by its hostname. For example, if 
the governing Service belongs to the default namespace and is called foo, and one 
of the pods is called A-0, you can reach the pod through its fully qualified domain 
name, which is a-0.foo.default.svc.cluster.local. You can’t do that with pods managed 
by a ReplicaSet.

通过Statefulset 部署应用
为了部署你的应用， 需要创建两个（或三个） 不同类型的对象：
• 存储你数据文件的持久卷（当集群不支持持久卷的动态供应时， 需要手动创
建）
• Statefulset必需的一个控制Service
• Statefulset本身

//伸缩statfulset
缩容一个Statefulset 将会最先删除最高索引值
的实例，所以缩容的结果是可预知的。

因为Statfulset 缩容任何时候只会操作一个pod 实例，所以有状态应用的缩容
不会很迅速;
StatefulSet在有实例不健康的情况下是不允许做缩容操作的。若
一个实例是不健康的，而这时再缩容一个实例的话，也就意味着你实际上同时失去
了两个集群成员。

扩容StatefulSet 增加一个副本数时， 会创建两个或更多的API 对象（一个pod
和与之关联的一个或多个持久卷声明） 。但是对缩容来说， 则只会删除一个pod ， 而
遗留下之前创建的声明;基于这个原因， 当你需要释放特定
的持久卷时， 需要手动删除对应的持久卷声明。
因为缩容Statefulset 时会保留持久卷声明， 所以在随后的扩容操作中， 新的pod
实例会使用绑定在持久卷上的相同声明和其上的数据（如图10.9 所示）。当你因为
误操作而缩容一个Statefulset 后，可以做一次扩容来弥补自己的过失， 新的pod 实
例会运行到与之前完全一致的状态（名字也是一样的〉。

//创建StatefulSet 
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: kubia
spec:
  serviceName: kubia
  replicas: 2
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia-pet
        ports:
        - name: http
          containerPort: 8080
        volumeMounts:
        - name: data
          mountPath: /var/data
  volumeClaimTemplates:           #卷声明模板为每个pod创建一个PersistentVolumeClaim 
  - metadata:                     #StatefulSet为每个PersistentVolumeClaim绑定一个卷 
      name: data
    spec:
      resources:
        requests:
          storage: 1Mi
      accessModes:
      - ReadWriteOnce
	  
//用于DNS SRV
apiVersion: v1
kind: Service
metadata:              
  name: kubia-2       
spec:
  clusterIP: None       #None代表headless服务
  selector:
    app: kubia          #标签为kubia的pod都属于这个service
  ports:
  - name: http
    port: 80
NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubia          ClusterIP   None           <none>        80/TCP    50m
//先要创建一个headless service 
$ kubectl run -it srvlookup --image=tutum/dnsutils --rm --restart=Never -- dig SRV kubia.default.svc.cluster.local
//在应用程序内调用函数访问集群内的pod
dns.resolveSrv();




//使用端口转发， 
k port-forward kubia-0  8888:8080

缩容一个Statefulset, 然后在完成后再扩容它， 与删除一个pod后让Statefulset
立马重新创建它的表现是没有区别的;
当缩容超过一个实例的时候， 会首先删除拥有最高索引
值的pod。只有当这个pod被完全终止后， 才会开始删除拥有次高索引值的pod;

//通过API服务器与pod通信
<apiServerHost>:<port>/api/vl/namespaces/default/pods/kubia-0/proxy/<path>
$ kubectl proxy
$curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
$ curl -X POST -d "Hey there! This greeting was submitted to kubia-0." localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
Data stored on pod kubia-0
$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/

//不是外部暴露的Service (它是一个常规的ClusterIPService, 不是一
//个NodePort或LoadBalancer typeService), 只能在你的集群内部访问它
apiVersion: v1
kind: Service
metadata:
  name: kubia-public
spec:
  selector:
    app: kubia
  ports:
  - port: 80
    targetPort: 8080
NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubia-public   ClusterIP   10.124.0.194   <none>        80/TCP    76m
//通过API服务器访问集群内部的服务
/api/v1/namespaces/<namespace>/services/<service name>/proxy/<path>
$k proxy
$ curl localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/ #随机访问pod

---更新Statefulset---
$ kubectl edit statefulsetkubia //k8s自动应用新的模板更新pod

$ gcloud compute ssh gke-kubia-default-pool-32a2cac8-m0g1
$ sudo ifconfig eth0 down //关闭节点网络适配器 
当一个节点状态转变为Unknown时，除非该节点的kubelet报告节点的pod已经停用，
否则k8s不会删除该节点上的pod;可以强制删除
$ kubectl delete po kubia-0 --force --grace-period 0
//恢复节点 
$ gcloud compute instances reset <node name>


-----------------------------C11-----------------------------------------
Chapter 11. Understanding Kubernetes internals
$ kubect1 get componentstatuses //查看组件状态 
尽管工作节点上的组件都需要运行在同一个节点上， 控制平面的组件可以被简
单地分割在多台服务器上。为了保证高可用性， 控制平面的每个组件可以有多个实
例。etcd和API服务器的多个实例可以同时并行工作， 但是， 调度器和控制器管理
器在给定时间内只能有一个实例起作用，其他实例处于待命模式。

//通过-o custom-columns选项自定义展示的列以及- -sort -by对资源列表进行排序
kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system

//etcd;乐观并发控制
//对照Java的非阻塞算法 
所有的Kubernetes 包含一个metadata.resourceVersion宇段，当更
新对象时，客户端需要返回该值到API服务器。如果版本值与etcd中存储的不
匹配，API服务器会拒绝该更新

pod按命名空间存储
//监听资源
kubect1 get pods --watch

可以在集群中运行多个调度器而非单个。然后， 对每一个pod, 可以通过在pod
特性中设置schedulerName 属性指定调度器来调度特定的pod。
未设置该属性的pod 由默认调度器调度， 因此其schedulerName 被设置为
default-scheduler 。其他设置了该属性的pod 会被默认调度器忽略掉， 它们要
么是手动调用， 要么被监听这类pod 的调度器调用。

控制器源代码

//The PersistentVolume controller
When a PersistentVolumeClaim pops up, the controller finds the best 
match for the claim by selecting the smallest PersistentVolume with 
the access mode matching the one requested in the claim and the declared 
capacity above the capacity requested in the claim. It does this by keeping 
an ordered list of PersistentVolumes for each access mode by ascending 
capacity and returning the first volume from the list.

//kubelet探测容器的是否存活，重启，终结容器，报告给API服务器 
The Kubelet is also the component that runs the container liveness probes, 
restarting containers when the probes fail. Lastly, it terminates containers 
when their Pod is deleted from the API server and notifies the server that the 
pod has terminated.
//kubelet可以以pods形式运行控制面板组件的容器化版本；DaemonSet
run the containerized versions of the Control Plane components as pods

如何部署插件
通过提交YAML清单文件到API服务器（本书的通用做法），这些组件会成为
插件并作为pod部署。有些组件是通过Deployment资源或者ReplicationController
资源部署的，有些是通过DaemonSet
kubect1 get rc -n kube-system
$ kubect1 get deploy -n kube-system

//DNS服务 
The DNS server pod is exposed through the kube-dns service, allowing 
the pod to be moved around the cluster, like any other pod. The service’s 
IP address is specified as the nameserver in the /etc/resolv.conf file inside 
every container deployed in the cluster. The kube-dns pod uses the API server’s 
watch mechanism to observe changes to Services and Endpoints and updates its 
DNS records with every change, allowing its clients to always get (fairly) 
up-to-date DNS information.

//--watch 选项观察事件 
$ kubectl get events --watch
//运行一个容器，观察基础容器；
//每个pod都有一个基础容器，保存命名空间，供pod里的其他容器使用
kubectl run nginx --image=nginx
gcloud compute ssh <node name>
minkube ssh 
docker ps

网络是由系统管理员或者ContainerNetwork Interface (CNI)插件建立的，
而非Kubemetes本身。

//基础设施容器创建了一个veth对, 一个接口在主机命名空间, 一个在容器命名空间 
Before the infrastructure container is started, a virtual Ethernet 
interface pair (a veth pair) is created for the container. One interface 
of the pair remains in the host’s namespace (you’ll see it listed as 
vethXXX when you run ifconfig on the node), whereas the other is moved 
into the container’s network namespace and renamed eth0. The two virtual 
interfaces are like two ends of a pipe (or like two network devices 
connected by an Ethernet cable)

//主机命名空间的接口附着到容器运行时的网络桥;
//从容器命名空间eth0接口接受到的信息会从网络桥出来
The interface in the host’s network namespace is attached to a network 
bridge that the container runtime is configured to use. The eth0 interface 
in the container is assigned an IP address from the bridge’s address range. 
Anything that an application running inside the container sends to the eth0 
network interface (the one in the container’s namespace), comes out at the 
other veth interface in the host’s namespace and is sent to the bridge. 
This means it can be received by any network interface that’s connected to the bridge.
//在节点上的所有pod通过相同的桥沟通 
If pod A sends a network packet to pod B, the packet first goes through 
pod A’s veth pair to the bridge and then through pod B’s veth pair. All 
containers on a node are connected to the same bridge, which means they 
can all communicate with each other. But to enable communication between 
containers running on different nodes, the bridges on those nodes need to 
be connected somehow

//网桥连接到节点上的物理适配器，如此，实现和别节点通信，但是节点要连接到相同
//网关，之间没有路由；因为pod Ip是私有的，路由会扔包  
有多种连接不同节点上的网桥的方式。可以通过overlay或underlay网络， 或
者常规的三层路由

跨整个集群的pod 的IP地址必须是唯一的， 所以跨节点的网桥必须使用非重叠
地址段， 防止不同节点上的pod拿到同一个IP

SDN可以让节点忽略底层网络拓扑，
Container Network Interface (CNI)
Calico
Flannel
Romana
Weave Net
And others

We’ve learned that each Service gets its own stable IP address and port. 
Clients (usually pods) use the service by connecting to this IP address 
and port. The IP address is virtual—it’s not assigned to any network 
interfaces and is never listed as either the source or the destination 
IP address in a network packet when the packet leaves the node. A key 
detail of Services is that they consist of an IP and port pair (or multiple 
IP and port pairs in the case of multi-port Services), so the service IP by 
itself doesn’t represent anything. That’s why you can’t ping them.

//kube-proxy拦截报文，重定向到pod
When a service is created in the API server, the virtual IP address is 
assigned to it immediately. Soon afterward, the API server notifies all 
kube-proxy agents running on the worker nodes that a new Service has been 
created. Then, each kube-proxy makes that service addressable on the node 
it’s running on. It does this by setting up a few iptables rules, which 
make sure each packet destined for the service IP/port pair is intercepted 
and its destination address modified, so the packet is redirected to one 
of the pods backing the service.
//Endpoints与service紧密相关
An Endpoints object holds the IP/port pairs of all the pods that back 
the service (an IP/port pair can also point to something other than a pod). 
That’s why the kube-proxy must also watch all Endpoints objects

//节点间的领导选举 
https://github.com/kubernetes-retired/contrib/tree/master/election

控制面板中的组件也可以使用领导选举机制;谁先到达就成为领导者 
kubectl get endpoints kube-scheduler -n kube-system -o yaml


----------------------C12-------------------------------

Chapter 12. Securing the Kubernetes API server
每个pod都与一个ServiceAccount相关
联，它代表了运行在pod中应用程序的身份证明。token文件持有ServiceAccount
的认证token。应用程序使用这个token连接API服务器时，身份认证插件会对
ServiceAccount进行身份认证，并将Set-viceAccount的用户名传回API服务器内部。
ServiceAccount用户名的格式像下面这样：
system:serviceaccount:<namespace>:<service accoun七name>
API服务器将这个用户名传给己配置好的授权插件，这决定该应用程序所尝试
执行的操作是否被ServiceAccount允许执行。
erviceAccount只不过是一种运行在pod中的应用程序和API服务器身份认证
的一种方式
#查看ServiceAccount列表
$ kubectl get sa

每个pod都与一个ServiceAccount相关联， 但
是多个pod可以使用同一个ServiceAccount；pod只能
使用同一个命名空间中的ServiceAccount。

在pod 的manifest 文件中， 可以用指定账户名称的方式将一个ServiceAccount
赋值给一个pod。如果不显式地指定ServiceAccount 的账户名称， pod 会使用在这个
命名空间中的默认ServiceAccount。
可以通过将不同的ServiceAccount 赋值给pod 来控制每个pod 可以访问的资源。
当API 服务器接收到一个带有认证token 的请求时， 服务器会用这个token 来验证
发送请求的客户端所关联的ServiceAccount 是否允许执行请求的操作。API 服务器
通过管理员配置好的系统级别认证插件来获取这些信息。其中一个现成的授权插件
是基千角色控制的插件(RBAC)

创建 serviceaccount
$ kubectl describe sa foo
$ kubectl describe sa foo
Name:               foo
Namespace:          default
Labels:             <none>

Image pull secrets: <none>        #自动添加到使用这个sa的pod中

Mountable secrets:  foo-token-qzq7j #如果mountable Secrets是强制的，则使用这个sa的pod只能
                                    #挂载这些secrets
Tokens:             foo-token-qzq7j #第一个被挂载进容器 

注解 kubernetes.io/enforce-mountable-secrets= "true" 表明，
使用这个sa的pod只能挂载sa的密钥 
#查看token;JSON Web Tokens (JWT)
$ kubectl describe secret foo-token-qzq7j

//将image Pull Secrets添加到每个使用这个sa的pod中 
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-service-account
imagePullSecrets:
- name: my-dockerhub-secret

kubectl exec -it curl-custom-sa -c main cat /var/run/secrets/kubernetes.io/serviceaccount/token
//403错误 
kubectl exec -it curl-custom-sa -c main -- curl localhost:8001/api/v1/pods
//可行
kubectl exec -it curl-custom-sa -c main -- curl localhost:8001/api/v1/

／／RBAC　
／／角色授予权限，绑定角色到具体的用户、组或ｓａ
The RBAC authorization rules are configured through four
resources, which can be grouped into two groups:
Roles and ClusterRoles, which specify which verbs can be performed on which resources.
RoleBindings and ClusterRoleBindings, which bind the above roles to specific users, 
groups, or ServiceAccounts.


//开启RBAC
minikube --extra-config=apiserver.Authorization.Mode=RBAC
//重启禁用的RBAC
$ kubectl delete clusterrolebinding permissive-binding

//创建不同命名空间的pod
$ kubectl create ns foo
namespace "foo" created
$ kubectl run test --image=luksa/kubectl-proxy -n foo
deployment "test" created
$ kubectl create ns bar
namespace "bar" created
$ kubectl run test --image=luksa/kubectl-proxy -n bar
deployment "test" created

$ kubectl exec -it test -n foo -- sh
/# curl localhost:8001/api/v1/namespaces/foo/services

//角色
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: foo            
  name: service-reader
rules:
- apiGroups: [""]          //service是核心apiGroup的资源，所以没有apiGroup,即为""
  verbs: ["get", "list"]
  resources: ["services"]  //复数 
  
//GKE创建roles时  
kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=sgydd001@gmail.com
$ kubectl create -f service-reader.yaml -n foo
$ kubectl create role service-reader --verb=get --verb=list --resource=services -n bar
role "service-reader" created

//绑定角色 
$ kubectl create rolebinding test --role=service-reader --serviceaccount=foo:default -n foo
$查看角色绑定
kubectl get rolebinding test -n foo -o yaml
注意如果要绑定一个角色到一个user （用户）而不是ServiceAccount 上， 使
用user 作为参数来指定用户名。如果要绑定角色到组，可以使用－－ group 参数。

//添加其他命名空间的serviceaccount
$ kubectl edit rolebinding test -n foo
subjects:
- kind: ServiceAccount
  name: default
  namespace: bar
  
12.2.4 使用ClusterRole 和C l usterRoleBinding
Cluster Role 是一种集群级资源，它允许访问没有命名空间的资源和非资源型的
URL ，或者作为单个命名空间内部绑定的公共角色，从而避免必须在每个命名空间
中重新定义相同的角色。
//集群角色要用集群角色绑定 进行绑定 
$ kubectl create clusterrole pv-reader --verb=get,list --resource=persistentvolumes
kubectl get clusterrole pv-reader -o yaml
$ kubectl create clusterrolebinding pv-test --clusterrole=pv-reader --serviceaccount=foo:default

/ # curl localhost:8001/api/v1/persistentvolumes

//允许访问非资源型的URL
API 服务器也会对外暴露非资源型的URL 。访问这些URL 也
必须要显式地授予权限－否则， API 服务器会拒绝客户端的请求。通常， 这个会通
过system : discovery ClusterRole 和相同命名的ClusterRoleBinding 帮你自动完
成，它出现在其他预定义的ClusterRoles 和ClusterRoleBindings
$ kubectl get clusterrole system:discovery -o yaml
//将Clusterrole绑定到所有认证过和没有认证过的用户上 
//上面命令显示对非资源url只能执行get操作 ，与下面说法矛盾 
对于非资源型URL ，使用普通的HTTP 动词，如post 、put 和patch,
而不是create 或update 。动词需要使用小写的形式指定

//这个角色访问的资源属于命名空间
$ kubectl get clusterrole view -o yaml
如果你创建了一个ClusterRoleBinding 并在它里面引用了ClusterRole,
在绑定中列出的主体可以在所有命名空间中查看指定的资源。相反， 如果你创建的
是一个RoleBinding, 那么在绑定中列出的主体只能查看在RoleBinding 命名空间中
的资源。现在可以尝试使用这两个选项

//使用ClusterRoleBinding绑定ClusterRole
curl localhost:8001/api/v1/pods  //查看集群级别资源
curl localhost:8001/api/v1/namespaces/foo/pods //查看命名空间级别资源 
kubectl create clusterrolebinding view-test --clusterrole=view --serviceaccount=foo:default
curl localhost:8001/api/v1/namespaces/bar/pods

//使用命名空间中的RoleBinding绑定ClusterRole
$ kubectl delete clusterrolebinding view-test
$ kubectl create rolebinding view-test --clusterrole=view --serviceaccount=foo:default -n foo

Kubemetes提供了一组默认的ClusterRole和ClusterRoleBinding
$ kubect1 get clusterrolebindings //查看
$ kubectl get clusterroles

view、ed江、adrnin和cluster-adrnin ClusterRole是最重要的角色， 它们
应该绑定到用户定义pod中的ServiceAccount上。
用view ClusterRole 允许对资源的只读访问

用editClusterRole允许对资源的修改
接下来是ed江ClusterRole, 它允许你修改一个命名空间中的资源， 同时允许
读取和修改Secret。但是，它也不允许查看或修改Role和RoleBinding, 这是为了
防止权限扩散。

用adminClusterRole赋予一个命名空间全部的控制权
ed江和adminClusterRole
一个命名空间中的资源的完全控制权是由admin ClusterRole赋予的。有这个
ClusterRole的主体可以读取和修改命名空间中的任何资源， 除了ResourceQuota (我
们会在第14章中了解它是什么）和命名空间资源本身。
之间的主要区别是能否在命名空间中查看和修改Role和RoleBinding。

用cluster-admin ClusterRole得到完全的控制
通过将cluster-adminClusterRole赋给主体，主体可以获得Kubernetes 集群
完全控制的权限。正如你前面了解的那样，adminClusterRole不允许用户修改命名
空间的ResourceQuota对象或者命名空间资源本身。如果你想允许用户这样做， 需
要创建一个指向cluster-adminClusterRole的RoleBinding。这使得RoleBinding
中包含的用户能够完全控制创建RoleBinding所在命名空间上的所有方面

了解其他默认的ClusterRole
默认的ClusterRole列表包含了大量其他的ClusterRole, 它们以sys 七em:
为前缀

---------------------------------C13-----------------------------
Chapter 13. Securing cluster nodes and the network

13.1 在pod中使用宿主节点的Linux命名空间
某个pod可能需要使用宿主节
点上的网络适配器，而不是自己的虚拟网络设备。这可以通过将pod spec中的
hostNetwork设置为true实现。
这意味着这个pod没有自己的IP地址；如果这个pod中的
某一进程绑定了某个端口，那么该进程将被绑定到宿主节点的端口上

13.1.2 绑定宿主节点上的端口而不使用宿主节点的网络命名空间
配置pod 的spec . containers . ports 字段中某个容器某一端口的hostPort 属性来实现。

//图13.2;NodePort-反向代理-服务发现
对于一个使用hostPort 的p o d ，到达宿主节点的端
口的连接会被直接转发到pod 的对应端口上：然而在NodePort 服务中，到达宿主
节点的端口的连接将被转发到随机选取的pod 上

很重要的一点是，如果一个pod 绑定了宿主节点上的一个特定端口，每个宿主
节点只能调度－个这样的pod 实例，因为两个进程不能绑定宿主机上的同一个端口。

//??If you have multiple nodes, you’ll see you can’t access the pod 
//through that port on the other nodes.
apiVersion: v1
kind: Pod
metadata:
  name: kubia-hostport
spec:
  containers:
  - image: luksa/kubia
    name: kubia
    ports:
    - containerPort: 8080
      hostPort: 9000
      protocol: TCP

pod spec 中的hostPID 和host IPC 选项与hostNetwork 相似。当它们被设
置为true 时， pod 中的容器会使用宿主节点的PID 和IPC 命名空间，分别允许它
们看到宿主机上的全部进程，或通过IPC 机制与它们通信。
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-host-pid-and-ipc
spec:
  hostPID: true
  hostIPC: true
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
k exec pod-with-host-pid-and-ipc -- ps aux

13.2 配置节点的安全上下文
容器运行时使用的用户在镜像中指定3 在Dockerfile 中，这是通过使用
USER 命令实现的。如果该命令被省略，容器将使用root 用户运行
$ kubectl run pod-with-defaults --image alpine --restart Never -- /bin/sleep 999999
//查看用户id
kubectl exec pod -with-defaults --  id
//设置该pod 的securityContext.runAsUser 选项
	apiVersion: v1
kind: Pod
metadata:
  name: pod-as-user-guest
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      runAsUser: 405                  //指定id而不是用户名，405对应guest客户 
	  
//使得pod 中的容器以非root 用户运行，
apiVersion: v1
kind: Pod
metadata:
  name: pod-run-as-non-root
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      runAsNonRoot: true	   //因为alpine以root运行，所以容器不能运行 
	  
//为获取宿主机内核的完整权限，该pod 需要在特权模式下运行。这可以通过将
//容器的securityContext 中的privileged设置为true 实现	  
apiVersion: v1
kind: Pod
metadata:
  name: pod-privileged
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      privileged: true
k exec pod-privileged -- ls /dev

13.2.4 为容器单独添加内核功能
Kubernetes允许为特定的容器添加内核
功能， 或禁用部分内核功能， 以允许对容器进行更加精细的权限控制， 限制攻击者
潜在侵入的影响。
apiVersion: v1
kind: Pod
metadata:
  name: pod-add-settime-capability
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      capabilities:
        add:
        - SYS_TIME  //允许修改时间 
如果需要允许容器修改系统时间， 可以在容器的capb辽1巨es里add 一项名
为CAP_SYS_TIME的功能；Linux内核功能的名称通常以CAP_开头。但在podspec中指定内核功
能时，必须省略CAP_前缀

在pod-with-defaults 中将/tmp 目录的所有
者改为guest 用户
$ kubectl exec pod-with-defaults chown guest /tmp
$ kubectl exec pod-with-defaults -- ls -la / | grep tmp
drwxrwxrwt    2 guest    root             6 May 25 15:18 tmp

在容器的
securityContext.capabilities.drop 列表中加入此项， 以禁用这个修改文
件所有者的内核功能
在容器的
apiVersion: v1
kind: Pod
metadata:
  name: pod-drop-chown-capability
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      capabilities:
        drop:
        - CHOWN
		
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-readonly-filesystem
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      readOnlyRootFilesystem: true    //不允许写入容器根文件系统
    volumeMounts:
    - name: my-volume                //写入卷可以
      mountPath: /volume
      readOnly: false
  volumes:
  - name: my-volume
    emptyDir:		
以上的例子都是对单独的容器设置安全上下文。这些选项中的一部分也可以从
pod级别设定（通过pod.spec.securityCon迳江属性）。它们会作为pod中每
一个容器的默认安全上下文， 但是会被容器级别的安全上下文覆盖;

Kubemetes允许为pod中所有容器指定supplemental组，以允许它们无
论以哪个用户ID运行都可以共享文件。这可以通过以下两个属性设置：
• fsGroup
• supplementalGroups
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-shared-volume-fsgroup
spec:
  securityContext:
    fsGroup: 555                   //加载的卷被组id 555所有 
    supplementalGroups: [666, 777]
  containers:
  - name: first
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      runAsUser: 1111
    volumeMounts:
    - name: shared-volume
      mountPath: /volume
      readOnly: false
  - name: second
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      runAsUser: 2222
    volumeMounts:
    - name: shared-volume
      mountPath: /volume
      readOnly: false
  volumes:
  - name: shared-volume
    emptyDir:
//有效组id 为 0 
$ kubectl exec -it pod-with-shared-volume-fsgroup -c first -- sh
/ $ id
uid=1111 gid=0(root) groups=555,666,777
/ $ ls -l / | grep volume  //group id 555拥有卷 	
drwxrwsrwx    2 root     555              6 May 29 12:23 volume
/ $ echo foo > /volume/foo   //在卷中创建文件
/ $ ls -l /volume
total 4
-rw-r--r--    1 1111     555              4 May 29 12:25 foo
//不在卷中创建文件 
/ $ echo foo > /tmp/foo
/ $ ls -l /tmp
total 4
-rw-r--r--    1 1111     root             4 May 29 12:41 foo

13.3 限制pod使用安全相关的特性
PodSecurityPolicy 是一种集群级别（无命名空间）的资源， 它定义了用户能否
在pod 中使用各种安全相关的特性。维护PodSecurityPolicy 资源中配置策略的工作
由集成在API 服务器中的PodSecurityPolicy 准入控制插件完成

当有人向API 服务器发送pod 资源时， PodSecurityPolicy 准入控制插件会将这
个pod 与已经配置的PodSecurityPolicy 进行校验。如果这个pod 符合集群中已有安
全策略， 它会被接收并存入etcd; 否则它会立即被拒绝。这个插件也会根据安全策
略中配置的默认值对pod 进行修改。

apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: default
spec:
  hostIPC: false
  hostPID: false
  hostNetwork: false
  hostPorts:
  - min: 10000
    max: 11000                  //闭区间
  - min: 13000
    max: 14000
  privileged: false
  readOnlyRootFilesystem: true
  runAsUser:

    rule: RunAsAny

  fsGroup:

    rule: RunAsAny

  supplementalGroups:

    rule: RunAsAny

  seLinux:

    rule: RunAsAny

  volumes:

  - '*'
 没有对容器运行时可以使用的用户和用户组施加任何限
制，因为它们在runAsUser、fsGroup、supplementalGroups等字段中使用
，了runAsAny规则。如果需要限制容器可以使用的用户和用户组ID, 可以将规则
改为MustRunAs, 并指定允许使用的ID范围。
runAsUser:
    rule: MustRunAs
    ranges:
    - min: 2
      max: 2
  fsGroup:
    rule: MustRunAs
    ranges:
    - min: 2
      max: 10
    - min: 20
      max: 30
  supplementalGroups:
    rule: MustRunAs
    ranges:
    - min: 2
      max: 10
    - min: 20
      max: 30
部署镜像中用户ID 在指定范围之外的pod，PSP策略会覆盖被硬编码在进行中的user ID;

以下三个字段会影响容器可以使用的
内核功能：
• allowedCapabilities
• defaultAddCapabilities
• requiredDropCapabilities

apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
spec:
  allowedCapabilities:    //允许容器spec.securityContext.capabilities添加内核功能
  - SYS_TIME
  defaultAddCapabilities:  //自动添加，容器可以显示禁用 
  - CHOWN
  requiredDropCapabilities: //禁用，security-Context.capabilities.drop 
  - SYS_ADMIN
  - SYS_MODULE
  ...


//允许在pod中使用哪些卷 
kind: PodSecurityPolicy
spec:
  volumes:
  - emptyDir
  - configMap
  - secret
  - downwardAPI
  - persistentVolumeClaim
  
13.3.5 对不同的用户与组分配不同的PodSe cu rityPol icy
Assigning different policies to different users is done through the 
RBAC mechanism described in the previous chapter. The idea is to create 
as many policies as you need and make them available to individual users 
or groups by creating ClusterRole resources and pointing them to the individual 
policies by name. By binding those ClusterRoles to specific users or groups 
with ClusterRoleBindings, when the PodSecurityPolicy Admission Control plugin 
needs to decide whether to admit a pod definition or not, it will only consider 
the policies accessible to the user creating the pod.

//minikube以cluster-admin登录,查看default psp 
//创建psp
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: privileged
spec:
  privileged: true
  runAsUser:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  volumes:
  - '*'
//创建default psp对应的role
$ kubectl create clusterrole psp-default --verb=use --resource=podsecuritypolicies --resource-name=default
clusterrole "psp-default" created
//另一个role 
$ kubectl create clusterrole psp-privileged --verb=use  --resource=podsecuritypolicies --resource-name=privileged
clusterrole "psp-privileged" created

//绑定role
$ kubectl create clusterrolebinding psp-all-users  --clusterrole=psp-default --group=system:authenticated
clusterrolebinding "psp-all-users" created
//绑定到system:authenticated， Admission Control plugin会使用该策略
You’re going to bind the psp-default ClusterRole to all authenticated users, 
not only to Alice. This is necessary because otherwise no one could create any 
pods, because the Admission Control plugin would complain that no policy is in 
place. Authenticated users all belong to the system:authenticated group, so you’ll 
bind the ClusterRole to the group:

//绑定bob 
$kubectl create clusterrolebinding psp-bob --clusterrole=psp-privileged --user=bob
clusterrolebinding "psp-bob" created

本书的附录
A 说明了如何在多个集群和多个上下文中使用kubectl
$ kubectl config set-credentials alice --username=alice --password=password
User "alice" set.
$ kubectl config set-credentials bob --username=bob --password=password
User "bob" set.

//失败！
$ kubectl --user alice create -f pod-privileged.yaml #Forbidden
$ kubectl --user bob create -f pod-privileged.yaml

13.4 隔离pod 的网络
限制pod 可以与其他哪
些pod 通信，来确保pod 之间的网络安全。
是否可以进行这些配置取决于集群中使用的容器网络插件。如果网络插件支持，
可以通过NetworkPolicy 资源配置网络隔离

13.4.1 在一个命名空间中启用网络隔离
在默认情况下， 某一命名空间中的pod 可以被任意来源访问。
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector:
在任何一个特定的命名空间中创建该N etworkPo li cy 之后，任何客户端都不能
访问该命名空间中的pod 。
注意集群中的CNI 插件或其他网络方案需要支持NetworkPolicy ，否则
NetworkPolicy 将不会影响pod 之间的可达性  

13.4.2 允许同一命名空间中的部分pod 访问一个服务端pod
为了允许同一命名空间中的客户端pod 访问该命名空间的pod ，需要指明哪些
pod 可以访问
//在数据库pod 所在的命名空间中创建
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: postgres-netpolicy
spec:
  podSelector:
    matchLabels:
      app: database
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: webserver
    ports:
    - port: 5432
NetworkPolicy 允许具有app=webserver 标签的pod 访问具有
app=database 的pod 的访问，并且仅限访问5432 端口

13.4.3 在不同Kubernetes 命名空间之间进行网络隔离
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: shoppingcart-netpolicy
spec:
  podSelector:
    matchLabels:
      app: shopping-cart     //目的app
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          tenant: manning    //筛选from app
    ports:
    - port: 80
以上NetworkPolicy保证了只有具有tenant：manning 标签的命名空间中运行
的pod 可以访问Shopping Cai1微服务；
注意在多租户的Kubernetes集群中，通常租户不能为他们的命名空间添加标
签（或注释）。否则，他们可以规避基于namespaceSelector的入向规则

13.4.4 使用CIDR隔离网络
only be accessible from IPs in the 192.168.1.1 to .255 range
ingress:
  - from:
    - ipBlock:
        cidr: 192.168.1.0/24
		
13.4.5 限制pod的对外访问流量		
spec:
  podSelector:
    matchLabels:
      app: webserver
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: database
以上的NetworkPolicy仅允许具有标签app=webserver的pod访问具有标签
app= database的pod, 除此之外不能访问任何地址（不论是其他pod, 还是任何
其他的IP, 无论在集群内部还是外部）。

-------------------------C14------------------------------
Chapter 14. Managing pods’ computational resources
14.1 为pod中的容器申请资源
创建一个pod 时， 可以指定容器对CPU 和内存的资源请求量（ 即
requests), 以及资源限制量（即Lim心）。它们并不在pod 里定义， 而是针对每个容
器单独指定。pod 对资源的请求量和限制量是它所包含的所有容器的请求量和限制
量之和。

apiVersion: v1
kind: Pod
metadata:
  name: requests-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main
    resources:
      requests:
        cpu: 200m
        memory: 10Mi
通过设置资源requests我们指定了pod对资源需求的最小值

调度器如何判断一个pod是否适合调度到某个节点
这里比较重要而且会令人觉得意外的是，调度器在调度时并不关注各类资源在
当前时刻的实际使用噩，而只关心节点上部署的所有pod 的资源申请量之和。尽管
现有pods 的资源实际使用量可能小于它的申请量，但如果使用基于实际资源消耗量
的调度算法将打破系统为这些已部署成功的pods提供足够资源的保证

两个基于资源请求量的优先级排序函数： LeastRequestedPriority和
MostReques七edPriority。前者优先将pod 调度到请求量少的节点上（也就是
拥有更多未分配资源的节点）， 而后者相反， 优先调度到请求量多的节点（拥有更少
未分配资源的节点）。但是， 正如我们刚刚解释的， 它们都只考虑资源请求量， 而
不关注实际使用资源量。

$ kubectl exec -it requests-pod top
//查看资源总量 
$ kubectl describe nodes
Name:       minikube
...
Capacity:
  cpu:           2
  memory:        2048484Ki
  pods:          110
Allocatable:
  cpu:           2
  memory:        1946084Ki
  pods:          110
...
$ kubectl run requests-pod-2 --image=busybox --restart Never
➥  --requests='cpu=800m,memory=20Mi' -- dd if=/dev/zero of=/dev/null
pod "requests-pod-2" created

14.1.3 CPU requests如何影响CPU时间分配
CPU requests不仅仅在调度时起作用，它还决定着剩余（未使用） 的CPU时间
如何在pod之间分配。正如图14.2描绘的那样，因为第一个pod 请求了200毫核，
另一个请求了1000毫核，所以未使用的CPU将按照1:5的比例来划分给这两个
pod。如果两个pod 都全力使用CPU,第一个pod 将获得16.7%的CPU时间，另一
个将获得83.3%的CPU时间

另一方面，如果一个容器能够跑满CPU,而另一个容器在该时段处于空闲状态，
那么前者将可以使用整个CPU时间（当然会减掉第二个容器消耗的少量时间）。

第二个容器需要CPU时间的时候就会获取到，同时第一个容器会被限制回来。

14.1.4 定义和申请自定义资源
Defining and requesting custom resources
一个自定义资源的例子就是节点上可用的GPU 单元数量。

14.2 限制容器的可用资源
CPU 是一种可压缩资源，意味着我们可以在不对容器内运行的进程产生不利影
响的同时，对其使用量进行限制。而内存明显不同一一是一种不可压缩资源。一旦
系统为进程分配了一块内存，这块内存在进程主动释放之前将无法被回收。这就是
我们为什么需要限制容器的最大内存分配量的根本原因。

apiVersion: v1
kind: Pod
metadata:
  name: limited-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main
    resources:
      limits:
        cpu: 1
        memory: 20Mi
resource requests等于 resource limits

Overcommitting limits
与资源requests 不同的是，资源limits 不受节点可分配资源量的约束。所有
limits 的总和允许超过节点资源总量的100%；如果节点资源使用量超过100% ， 一些容器将被杀掉， 这是一个很重要的
结果

//对于容器而言，当获取内存而不得时，相当于系统内存耗尽，程序出现错误，
//因此，k8s重启是合理的
而内存却有所不同。当进程尝试申请分配比限额更多的内存时会被杀掉（我们
会说这个容器被OOMKilled 了， OOM 是Out 仁’ Memory 的缩写〉。如果pod 的重
启策略为Always 或OnFailure ，进程将会立即重启，因此用户可能根本察觉不
到它被杀掉。但是如果它继续超限并被杀死， Kub erne tes 会再次尝试重启，并开始
增加下次重启的间隔时间。这种情况下用户会看到pod 处于CrashLoopBackOf f
状态：
$ kubectl get po
NAME        READY     STATUS             RESTARTS   AGE
memoryhog   0/1       CrashLoopBackOff   3          1m

一旦间隔时间达到3 00 秒， Kubelet 将以5 分钟为间隔时间对容器进行无限重启，直到容器正常运行
或被删除。

要定位容器crash 的原因，可以通过查看pod 日志以及kubectl describe
pod 命令

14.2.3 容器中的应用如何看待li m i t s
k apply -f limited-pod.yaml
k exec -it limited-pod -- top
//貌似内存limit没什么用 
在容器内看到的始终是节点的内存， 而不是容器本身的内存
The top command shows the memory amounts of the whole node the container is running on. 
Even though you set a limit on how much memory is available to a container, the container 
will not be aware of this limit.

不要依赖应用程序从系统获取的CPU 数量，你可能需要使用Downward API 将
CPU 限额传递至容器并使用这个值。也可以通过cgroup 系统直接获取配置的CPU
限制，请查看下面的文件：
• /sys/fs/cgroup/cpu/cpu .cfs quota_ us
• /sys/fs/cgroup/cpu/cpu.cfs period_ us

14.3 了解pod QoS等级
Kubernetes 将
pod 划分为3 种QoS 等级：
• BestEffort （优先级最低）
• Burstable
• Guaranteed （优先级最高）

没有设置任何requests 和limits 的pod的优先级是BestEffort；

Guaranteed级别的pod，对于pod内的每个container:
• CPU 和内存都要设置requests 和limits
． 每个容器都需要设置资源量
· 它们必须相等（每个容器的每种资源的requests 和limits 必须相等）
如果容器的资源requests 没有显式设置，默认与limits 相同

Burstable：不能归入上述两种的情况 

如果一开始从容器
级别考虑QoS C 尽管它并不是容器的属性，而是pod 的属性），然后从容器Q oS 推
导出pod QoS ，这样可能更容易理解。

对千多容器pod, 如果所有的容器的QoS等级相同， 那么这个等级就是pod的
QoS等级。如果至少有一个容器的QoS等级与其他不同，无论这个容器是什么等级，
这个pod的QoS等级都是Burstable等级


注意运行kubectl describe pod以及通过pod的YAML/JSON描述的
Status.qosClass字段都可以查看pod的QoS等级。

//杀死pod
在一个超卖的系统， QoS等级决定着哪个容器第一个被杀掉， 这样释放出的资
源可以提供给高优先级的pod使用。BestEffort等级的pod首先被杀掉， 其次是
Burstable pod, 最后是Guaranteed pod。Guaranteedpod只有在系统进程
需要内存时才会被杀掉。

如何处理相同QoS等级的容器
每个运行中的进程都有一个称为OutOfMemory (OOM)分数的值。系统通过比
较所有运行进程的OOM分数来选择要杀掉的进程。当需要释放内存时， 分数最高
的进程将被杀死。

OOM scores are calculated from two things: the percentage of the available memory 
the process is consuming and a fixed OOM score adjustment, which is based on the pod’s 
QoS class and the container’s requested memory.

对于两个属于Burstable等级的单容器的pod, 系统会杀掉内存实际使用量占内存申请量比例
更高的pod

14.4 为命名空间中的pod 设置默认的requests 和limits
用户可以通过创建一个LimitRange资源来避免必须配置每个容器。L血itRange
资源不仅允许用户（为每个命名空间）指定能给容器配置的每种资源的最小和最大
限额， 还支持在没有显式指定资源requests 时为容器设置默认值，

L皿itRange资源被LimitRanger准入控制插件（我们在第11章介绍过这种插件）。
API服务器接收到带有pod描述信息的POST请求时， LimitRanger插件对pod spec
进行校验。如果校验失败， 将直接拒绝。因此， LimitRange对象的一个广泛应用场
景就是阻止用户创建大千单个节点资源量的pod。如果没有LimitRange, API服务
器将欣然接收pod创建请求， 但永远无法调度成功。

LimtRange资源中的limit s应用于同一个命名空间中每个独立的pod、容器，
或者其他类型的对象。它并不会限制这个命名空间中所有pod可用资源的总量， 总
量是通过Resou rceQuota对象指定的

apiVersion: v1
kind: LimitRange
metadata:
  name: example
spec:
  limits:
  - type: Pod
    min:
      cpu: 50m
      memory: 5Mi
    max:
      cpu: 1
      memory: 1Gi
  - type: Container
    defaultRequest:     //为没有设置requests的容器设定默认的cpu和内存requests

      cpu: 100m

      memory: 10Mi

    default:          //为没有设置limits的容器设定默认的cpu和内存limits

      cpu: 200m

      memory: 100Mi

    min:

      cpu: 50m

      memory: 5Mi

    max:

      cpu: 1

      memory: 1Gi

    maxLimitRequestRatio:
      cpu: 4
      memory: 10
  - type: PersistentVolumeClaim
    min:
      storage: 1Gi
    max:
      storage: 10Gi

由于L血itRange对象中配置的校验（和默认值）信息在API服务器接收到新的
pod或PVC创建请求时执行，如果之后修改了限制，已经存在的pod和PVC 将不
会再次进行校验，新的限制只会应用于之后创建的pod和PVC 。

在每个命
名空间中定义不同的LimitRange就可以确保只在特定的命名空间中可以创建资源需
求大的pod, 而在另一些命名空间中只能创建资源需求小的pod。

14.5 限制命名空间中的可用资源总量
ResourceQuota的接纳控制插件会检查将要
创建的pod是否会引起总资源量超出ResourceQuota。如果那样，创建请求会被拒绝。
因为资源配额在pod 创建时进行检查，所以ResourceQuota对象仅仅作用于在其后
创建的pod— — 并不影响已经存在的pod。
资源配额限制了一个命名空间中pod和PVC存储最多可以使用的资源总址。同
时也可以限制用户允许在该命名空间中创建pod、PVC, 以及其他API对象的数量，
因为到目前为止我们处理最多的资源是CPU和内存

LimitRange应用于单独的pod ; ResourceQuota应用千命名空间中所有的pod
$ kubectl describe po kubia-manual
Name:           kubia-manual
...
Containers:
  kubia:
    Limits:
      cpu:      200m
      memory:   100Mi
    Requests:
      cpu:      100m
      memory:   10Mi

$ kubectl describe quota

创建ResourceQuota时往往还需要随之创建一个
LimitRange对象。当特定资源(CPU或内存）配置了(requests或limits)配额， 在pod中
必须为这些资源（分别〉指定requests 或limits ，否则API 服务器不会接收该pod 的
创建请求。

apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage
spec:
  hard:
    requests.storage: 500Gi      #全部可声明的存储为500G
    ssd.storageclass.storage.k8s.io/requests.storage: 300Gi //可声明的SSD为300G
    standard.storageclass.storage.k8s.io/requests.storage: 1Ti  //hdd存储

apiVersion: v1
kind: ResourceQuota
metadata:
  name: objects
spec:
  hard:
    pods: 10
    replicationcontrollers: 5
    secrets: 10
    configmaps: 10
    persistentvolumeclaims: 4
    services: 5
    services.loadbalancers: 1
    services.nodeports: 2
    ssd.storageclass.storage.k8s.io/persistentvolumeclaims: 2 //只有两个pvc能声明ssd类别存储


目前配额作用范围共有4 种：
BestEff ort 、NotBestEffort 、Termination 矛日NotTerminatingo
Best Ef f ort 和NotBestEffort 范围决定配额是否应用于BestEffo rt
QoS 等级或者其他两种等级（ B ur stable 和Guaranteed ） 的pod

//Terminating quto应用于设定了active-Deadline-Seconds的pod,Not-Terminating反之
you can specify how long each pod is allowed to run before it’s terminated and marked as 
Failed. This is done by setting the active-Deadline-Seconds field in the pod spec. This 
property defines the number of seconds a pod is allowed to be active on the node relative 
to its start time before it’s marked as Failed and then terminated. The Terminating quota 
scope applies to pods that have the active-DeadlineSeconds set, whereas the Not-Terminating 
applies to those that don’t.
apiVersion: v1
kind: ResourceQuota
metadata:
  name: besteffort-notterminating-pods
spec:
  scopes:
  - BestEffort
  - NotTerminating
  hard:
    pods: 4

允许最多创建4 个属于BestEffort QoS 等级，并没有设置active
deadline 的pod

14.6 监控pod 的资源使用量
Kubelet 自身就
包含了一个名为cAdvisor 的agent ，它会收集整个节点和节点上运行的所有单独容
器的资源消耗情况。集中统计整个集群的监控信息需要运行一个叫作Heapster 的附
加组件

//Heapster已被废除
minikube addons enable heapster

$ kubect1 top node //GKE 可行 
$ k top pod --all-namespaces
$ k top pod --container=false --all-namespaces

如果使用Google Container Engine, 可以通过
Google Cloud Monitoring 来对集群进行监控， 但是如果是本地Kubemetes 集群（通
过Minikube 或其他方式创建）， 人们往往使用InfluxDB 来存储统计数据， 然后使用
Grafana 对数据进行可视化和分析

lnfluxDB和Grafana 介绍
lnfluxDB 是一个用于存储应用指标， 以及其他监控数据的开源的时序数据库。
Grafana 是一个拥有着华丽的web 控制台的数据分析和可视化套件，同样也是开源的，
它允许用户对InfluxDB 中存储的数据进行可视化， 同时发现应用程序的资源使用行
为是如何随时间变化的

在集群中运行lnfluxDB和Grafana
InfluxDB 和Grafana 都可以以pod 运行， 部署简单方便。所有需要的部署文
件可以在Heapster Git 仓库中获取：http://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb
如果使用Minik:ube 就无须手动部署， 因为启用Heapster 插件时便会随之部署
Heapster

//使用Grafana 分析资源使用量
$ kubectl cluster-info
$ minikube service monitoring-grafana -n kube-system




————————————————————————————————————————————C15————————————————————————————————————————————————
Chapter 15. Automatic scaling of pods and cluster nodes

15.1 pod的横向自动伸缩
//HPA
Horizontal pod autoscaling is the automatic scaling of the number of pod replicas managed by a 
controller. It’s performed by the Horizontal controller, which is enabled and configured by creating 
a HorizontalPodAutoscaler (HPA) resource. The controller periodically checks pod metrics, calculates 
the number of replicas required to meet the target metric value configured in the HorizontalPodAutoscaler 
resource, and adjusts the replicas field on the target resource (Deployment, ReplicaSet, Replication-Controller, 
or StatefulSet).

15.1.1 了解自动伸缩过程
自动伸缩的过程可以分为三个步骤：
• 获取被伸缩资源对象所管理的所有pod度量。
• 计算使度量数值到达（或接近）所指定目标数值所需的pod数量。
• 更新被伸缩资源的rep巨cas字段。

HPA控制器向Heapster 发起REST调用来获取所有pod度量数据

当Autoscaler配置为只考虑单个度量时， 计算所需副本数很简单。只要将所有
pod的度量求和后除以HPA资源上配置的目标值， 再向上取整即可;
基于多个pod度量的自动伸缩（例如： CPU使用率和每秒查询率[QPS])的计
算也并不复杂。Autoscaler单独计算每个度量的副本数， 然后取最大值（例如：如
果需要4个pod达到目标CPU使用率， 以及需要3个pod来达到目标QPS, 那么
Autoscaler 将扩展到4个pod)

Autoscaler控制器通过Scale子资源来修改被伸缩资源的rep巨cas字段。这
样Autoscaler不必了解它所管理资源的细节，而只需要通过Scale子资源暴露的界面，

15.1.2 基于CPU使用率进行自动伸缩
As far as the Autoscaler is concerned, only the pod’s guaranteed CPU amount (the CPU requests) 
is important when determining the CPU utilization of a pod. The Autoscaler compares the pod’s 
actual CPU consumption and its CPU requests, which means the pods you’re autoscaling need to have 
CPU requests set (either directly or indirectly through a LimitRange object) for the Autoscaler to 
determine the CPU utilization percentage.

$ kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5
deployment "kubia" autoscaled
$k get hpa kubia -o yaml

//观察
$ watch -n 1 kubectl get hpa,deployment

$ kubectl run -it --rm --restart=Never loadgenerator --image=busybox -- sh -c "while true; do wget -O - -q http://kubia.default; done"

$while true; do curl 34.92.157.173:8080; done

修改一个已有HPA 对象的目标度量值,
使用kubectl edit 文本编辑器打开之后，把targetAverageUtilization 字段改为60,

15.1.3 基于内存使用进行自动伸缩
基于内存的自动伸缩比基于CPU 的困难很多。主要原因在于，扩容之后原有的
pod 需要有办法释放内存。这只能由应用完成，系统无法代芳。系统所能做的只有
杀死并重启应用，希望它能比之前少占用一些内存；但如果应用使用了跟之前一样
多的内存， Autoscaler 就会扩容、扩容， 再扩容， 直到达到HPA 资源上配置的最大
pod 数量。

//
you have three types of metrics you can use in an HPA object:
Resource
Pods
Object
...
spec:
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      targetAverageUtilization: 30
...
//与pod相关的metric
//The Pods type is used to refer to any other (including custom) metric related to the pod directly
...
spec:
  metrics:
  - type: Pods
    resource:
      metricName: qps
      targetAverageValue: 100
...

//基于其他的object做出决策
//The Object metric type is used when you want to make the autoscaler scale pods based on a 
metric that doesn’t pertain directly to those pods.
...
spec:
  metrics:
  - type: Object
    resource:
      metricName: latencyMillis
      target:
        apiVersion: extensions/v1beta1
        kind: Ingress
        name: frontend
      targetValue: 20
  scaleTargetRef:

    apiVersion: extensions/v1beta1

    kind: Deployment

    name: kubia

...

空载（ idling ）与解除空载（un-idling），即允许提供特定服务的pod 被缩
容量到0副 本。在新的请求到来时，请求会先被阻塞，直到pod 被启动，从而请求被
转发到新的pod 为止。

15.2 pod 的纵向自动伸缩
并不是所有应用都能被横向伸缩。对这些应用而言，唯一的
选项是纵向伸缩一－给它们更多CPU 和（或）内存

这是一个实验性的特性， 如果新创建的pod的容器没有明确设置CPU与内
存请求， 该特性即会代为设置。这一特性由一个叫作lnitialResources的准入控制
(Ad missionC ontrol)插件提供。 当一个没有资源请求的pod被创建时， 该插件会根
据pod 容器的历史资源使用数据（随容器镜像、tag而变）来设置资源请求。

15.3 集群节点的横向伸缩
ClusterA utoscaler负责在由千节点资源不足， 而无法调度某pod到已有节点时，
自动部署新节点。它也会在节点长时间使用率低下的情况下下线节点。
云服务提供者通常把相同规格（或者有相同特性）的节点聚合成组。因此
Cluster Autoscaler不能单纯地说“给我多一个节点”，它还需要指明节点类型。
Cluster Autoscaler 通过检查可用的节点分组来确定是否有至少一种节点类型能
容纳未被调度的pod。如果只存在唯一一个此种节点分组，ClusterAutoscaler就可以
增加节点分组的大小，让云服务提供商给分组中增加一个节点。但如果存在多个满
足条件的节点分组，ClusterAutoscaler就必须挑一个最合适的。这里“ 最合适” 的
精确含义显然必须是可配置的。在最坏的情况下，它会随机挑选一个。图15.5简单
描述了ClusterAutoscaler面对一个不可调度pod时是如何反应的。

归还节点
当节点利用率不足时， Cluster Autoscaler 也需要能够减少节点的数目。Cluster
Autoscaler 通过监控所有节点上请求的CPU 与内存来实现这一点。如果某个节点上
所有pod请求的CPU 、内存都不到50%, 该节点即被认定为不再需要。
//如果系统pod仅仅运行在这个节点，节点不会下线；
//如果未管理的pod或带有本地存储的pod运行在这个节点，节点也不会下线；
//只有pods能调度其他节点的 节点才能下线 
The Autoscaler also checks to see if any system pods are running (only) on that node 
(apart from those that are run on every node, because they’re deployed by a DaemonSet, for example). 
If a system pod is running on a node, the node won’t be relinquished. The same is also true if an 
unmanaged pod or a pod with local storage is running on the node, because that would cause disruption 
to the service the pod is providing. In other words, a node will only be returned to the cloud provider 
if the Cluster Autoscaler knows the pods running on the node will be rescheduled to other nodes.

当一个节点被选中下线，它首先会被标记为不可调度， 随后运行其上的pod 将
被疏散至其他节点

Manually cordoning and draining nodes
//拉起警戒线
kubectl cordon <node> marks the node as unschedulable (but doesn’t do anything with pods running on that node).
kubectl drain <node> marks the node as unschedulable and then evicts all the pods from the node.
In both cases, no new pods are scheduled to the node until you uncordon it again with kubectl uncordon <node>

Kubemetes可以指定下线等操作时需要保待的最少pod数量，
我们通过创建一个podDisruptionBudget资源的方式来利用这一特性。

$ kubectl create pdb kubia-pdb --selector=app=kubia --min-available=3
poddisruptionbudget "kubia-pdb" created

$ kubectl get pdb kubia-pdb -o yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: kubia-pdb
spec:
  minAvailable: 3
  selector:
    matchLabels:
      app: kubia
status:
  ...

也可以用一个百分比而非绝对数值来写minAvailable字段。比方说，可以
指定60%带app=kubia标签的pod应当时刻保持运行。
注意从Kubemetes 1. 7开始，podDismptionBudget资源也支持maxUnavailable。
如果当很多pod不可用而想要阻止pod被剔除时，就可以用maxUnavailable字段
而不是minAvailable。


———————————————————————————————————————————————————C16——————————————————————————————————————
|            Chapter 16. Advanced scheduling                                                |
—————————————————————————————————————————————————————————————————————————————————————————————

16.1. Using taints and tolerations to repel pods from certain nodes

//查看taints
$ kubectl describe node master.k8s #Taints:输出 
污点包含了一个key、value, 以及一个effect, 表现为<key>=<value>:<effect>,
$ kubectl describe node master.k8s
Name:         master.k8s
Role:
Labels:       beta.kubernetes.io/arch=amd64
              beta.kubernetes.io/os=linux
              kubernetes.io/hostname=master.k8s
              node-role.kubernetes.io/master=
Annotations:  node.alpha.kubernetes.io/ttl=0
              volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:       node-role.kubernetes.io/master:NoSchedule
...
上例中的污点包含：
the key node-role.kubernetes.io/master, a null value (not shown in the taint), and the effect of NoSchedule.

#pod容忍度
$ kubect1 describe po kube-proxy-80wqm -n kube-system #gke上去掉 kube-system 

#当节点处在not-ready或 unreachable状态时，pod被允许运行多长时间 
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s

每一个污点都可以关联一个效果， 效果包含了以下三种：
• NoSchedule 表示如果pod 没有容忍这些污点， pod 则不能被调度到包含这
些污点的节点上。
• PreferNoSchedule 是NoSchedule 的一个宽松的版本， 表示尽量阻止
pod 被调度到这个节点上， 但是如果没有其他节点可以调度， pod 依然会被调
度到这个节点上。
• NoExecute 不同于NoSchedule 以及PreferNoSchedule, 后两者只在
调度期间起作用， 而NoExecute 也会影响正在节点上运行着的pod。如果
在一个节点上添加了NoExecute 污点， 那些在该节点上运行着的pod, 如
果没有容忍这个NoExecute 污点， 将会从这个节点去除。

//给节点添加污点
$ kubectl taint node node1.k8s node-type=production:NoSchedule
node "node1.k8s" tainted
//To remove the taint added by the command above, you can run:
kubectl taint nodes node1 key=value:NoSchedule- //后面一个破折后

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: prod
spec:
  replicas: 5
  template:
    spec:
      ...
      tolerations:
      - key: node-type
        operator: Equal
        value: production
        effect: NoSchedule


//Equal操作符匹配污点的key的特定值；Exists操作符能容忍任何操作符 
taints can only have a key and an effect and don’t require a value. Tolerations can tolerate a 
specific value by specifying the Equal operator (that’s also the default operator if you don’t specify one), 
or they can tolerate any value for a specific taint key if you use the Exists operator.

//配置时间; 默认5分钟 
You can also use a toleration to specify how long Kubernetes should wait before rescheduling a pod to 
another node if the node the pod is running on becomes unready or unreachable.
$ kubectl get po prod-350605-1ph5h -o yaml
...
  tolerations:
  - effect: NoExecute
    key: node.alpha.kubernetes.io/notReady
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.alpha.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
注意当前这是一个alpha阶段的特性，在未来的Kubemetes版本中可能会有所
改变。基于污点信息的pod剔除也不是默认启用的， 如果要启用这个特性， 需要在
运行控制器管理器时使用--feature-gates=TaintBasedEvictions=true
选项。

节点亲缘性根据节点的标签来进行选择
failure-domain.beta.kubernetes.io/region=asia-east2 //地理区域
failure-domain.beta.kubernetes.io/zone=asia-east2-a //可用性区域 
kubernetes.io/hostname=gke-k8s-learn-default-pool-ae3320ed-pf25 //主机名 

node affinity, which allows you to tell Kubernetes to schedule pods only to specific subsets of nodes.
apiVersion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution: //required,强制性亲缘性规则 
        nodeSelectorTerms:
        - matchExpressions:
          - key: gpu         //标签为 gpu 值为 true 
            operator: In
            values:
            - "true"

nodeSelectorTerms 和matchExpressions 字段，这两个宇段定义了节点的标签必须满足哪一种表达式，
才能满足pod 调度的条件

给节点加上标签，首先，节点必须加上合适的标签。每个节点需要包含两个标签， 一个用于表示
所在的这个节点所归属的可用性区域，另一个用于表示这是一个独占的节点还是一
个共享的节点。
$ kubectl label node node1.k8s availability-zone=zone1
node "node1.k8s" labeled
$ kubectl label node node1.k8s share-type=dedicated
node "node1.k8s" labeled
$ kubectl label node node2.k8s availability-zone=zone2
node "node2.k8s" labeled
$ kubectl label node node2.k8s share-type=shared
node "node2.k8s" labeled
$ kubectl get node -L availability-zone -L share-type
NAME         STATUS    AGE       VERSION   AVAILABILITY-ZONE   SHARE-TYPE
master.k8s   Ready     4d        v1.6.4    <none>              <none>
node1.k8s    Ready     4d        v1.6.4    zone1               dedicated
node2.k8s    Ready     4d        v1.6.4    zone2               shared

//p475 gke运行 出错 
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: pref
spec:
  template:
    ...
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution: #优先级节点亲缘
          - weight: 80
            preference:
              matchExpressions:
              - key: availability-zone
                operator: In
                values:
                - zone1
          - weight: 20
            preference:
              matchExpressions:
              - key: share-type
                operator: In
                values:
                - dedicated
      ...
按照优先级，节点可被调度到带有这些标签的节点(排列组合）：
zone1/dedicated,zone1/shared,zone2/dedicated,zone2/shared
除了节点亲缘性的优先
级函数，调度器还是使用其他的优先级函数来决定节点被调度到哪。其中之一就是
Selector SpreadPriority函数，这个函数确保了属于同一个ReplicaSet或者
Service 的pod,将分散部署在不同节点上，以避免单个节点失效导致这个服务也宅机。
这就是有1个pod被调度到node2 的最大可能。

16.3 使用pod亲缘性与非亲缘性对pod进行协同部署
$ kubectl run backend -l app=backend --image busybox -- sleep 999999
deployment "backend" created

如果现在你删除了后端pod, 调度器会将该pod 调度到node2, 即
便后端pod 本身没有定义任何pod 亲缘性规则（只有前端pod 设置了规则）。这种
情况很合理， 因为假设后端pod 被误删除而被调度到其他节点上， 前端pod 的亲缘
性规则就被打破了。

16.3.2 将pod部署在同一机柜、可用性区域或者地理地域
了解topologyKey是如何工作的
//选择一个与正在被部署pod的label selector匹配的pod, 再查看结果pod所在的node的lablel,该node
//lable要匹配topologyKey域
When the Scheduler is deciding where to deploy a pod, it checks the pod’s pod-Affinity config, 
finds the pods that match the label selector, and looks up the nodes they’re running on. Specifically, 
it looks up the nodes’ label whose key matches the topologyKey field specified in podAffinity. 
Then it selects all the nodes whose label matches the values of the pods it found earlie

在同一个可用性区域中协同部署pod
topologyKey属性设置为failure-domain.beta.kubernetes.io/zone,

在同—个地域中协同部署pod
topologyKey属性设置为failure-domain.beta.kubernetes.io/region

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 5
  template:
    ...
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app: backend
      containers: ...

16.3.4 利用pod的非亲缘性分开调度pod
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 5
  template:
    ...
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            podAffinityTerm:                 #podAffinityTerm
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app: backend
      containers: ...