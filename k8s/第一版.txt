k8s in action 1st edition 

—————————————————————————————————————————————C3——————————————————————————————————
3.3.1 介绍标签
标签是可以附加到资源的任意键值对，用以选择具有该确切标签的资源（这是通过标答选择器完成的）
apiVersion: v1
kind: Pod
metadata:
  name: kubia-manual-v2
  labels:
    creation_method: manual //标签键值对 
    env: prod

spec:
  containers:
  - image: luksa/kubia
    name: kubia
    ports:
    - containerPort: 8080
      protocol: TCP
//显示标签
$ kubectl get po --show-labels
NAME            READY  STATUS   RESTARTS  AGE LABELS
kubia-manual    1/1    Running  0         16m <none>
kubia-manual-v2 1/1    Running  0         2m  creat_method=manual,env=prod
kubia-zxzij     1/1    Running  0         1d  run=kubia

$ kubectl get po -L creation_method,env //-L 按列列出标签 
//手动打标签
$ kubectl label po kubia-manual creation_method=manual
pod "kubia-manual" labeled

3.4 通过标签选择器列出pod子集
标签要与标签选
择器结合在一起。标签选择器允许我们选择标记有特定标签的pod子集， 并对这些
pod执行操作。
$ kubectl get po -l creation_method=manual
$ kubectl get po -l '!env'  #用单引号 
kubectl get po -l creation_method!=manual
kubectl get po -l 'env in (prod,devel)'
kubectl get po -l 'env notin (prod,devel)' #即使pod没有env标签也能筛选 
//下面的说法与上面一行的实践不符 
//env notin (prod,devel) to select pods with the env label set to any value other than prod or devel

在包含多个逗号分隔的清况下， 可以在标签选择器中同时使用多个条件， 此时
资源需要全部匹配才算成功匹配了选择器
app=pc, rel=beta
3.5 使用标签和选择器来约束pod调度
#给节点打标签
$ kubectl label node gke-kubia-85f6-node-Orrx gpu=true

apiVersion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
  nodeSelector:          //节点选择器,将pod调度到标签为 gpu=ture的节点上 

    gpu: "true"

  containers:
  - image: luksa/kubia
    name: kubia

3.6 注解pod
$ kubectl annotate pod kubia-manual mycompany.com/someannotation="foo bar"
pod "kubia-manual" annotated

3.7 使用命名空间对资源进行分组
$ kubectl get ns
$ kubectl get po --namespace kube-system  # -n=--namespace 

#用yaml文件创建命名空间 
apiVersion: v1
kind: Namespace
metadata:
  name: custom-namespace

$ kubectl create namespace custom-namespace
namespace "custom-namespace" created
Although most objects’ names must conform to the naming conventions specified in RFC 1035 
(Domain names), which means they may contain only letters, digits, dashes, and dots, namespaces 
(and a few others) aren’t allowed to contain dots
如果想要在刚创建的命名空间中创建资源，可以选择在metadata 宇段中添加
一个namespace : custom-namespace 属性，也可以在使用kubectl create
命令创建资源时指定命名空间：
$ kubectl create -f kubia-manual.yam1 -n custom-nameespace
pod "kubia-manual" created

#切换命名空间
To quickly switch to a different namespace, you can set up the following alias: 
alias kcd='kubectl config set-context $(kubectl config current-context) --namespace '. 
You can then switch between namespaces using kcd some-namespace.

尽管命名空间将对象分隔到不同的组，只允许你对属于特定命
名空间的对象进行操作， 但实际上命名空间之间并不提供对正在运行的对象的任何
隔离。

———————————————————————————————————————————————————————C4————————————————————————————————————————
4.2 了解ReplicationController
4.2.1 ReplicationController廿勺扲令作
ReplicationController会持续监控正在运行的pod列表， 并保证相应” 类型” 的
pod的数目与期望相符。如正在运行的pod太少， 它会根据pod模板创建新的副本。
如正在运行的pod太多， 它将删除多余的副本
了解ReplicationController的三部分
一个ReplicationController有三个主要部分（如图4.3所示）：
• label selector (标签选择器）， 用于确定ReplicationController作用域中有哪些
pod
• replica count (副本个数）， 指定应运行的pod数量
• pod template (pod模板）， 用于创建新的pod 副本

更改标签选择器和pod 模板对现有pod 没有影响。更改标签选择器会使现有的
pod 脱离ReplicationController 的范围， 因此控制器会停止关注它们。在创建pod 后，
ReplicationController 也不关心其pod的实际“ 内容”（容器镜像、环境变量及其他）。
因此， 该模板仅影响由此ReplicationController 创建的新pod。可以将其视为创建新
pod的曲奇切模(cookie cutter) 。
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3
  selector:     #selector
    app: kubia
  template:

    metadata:

      labels:

        app: kubia

    spec:

      containers:

      - name: kubia

        image: luksa/kubia

        ports:

        - containerPort: 8080
4.3 使用ReplicaSet而不是ReplicationController
ReplicaSet 的行为与ReplicationController 完全相同， 但pod 选择器的表达能力
更强。

apiVersion: apps/v1beta2
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:  #matchLabels 
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
ReplicaSet 相对于ReplicationController 的主要改进是它更具表达力的标签选
择器；
selector:
   matchExpressions:
     - key: app
       operator: In
       values:
         - kubia
 四个有效的运算符：
• In : Label的值必须与其中一个指定的values 匹配。
• Notln : Label的值与任何指定的values 不匹配。
• Exists : pod 必须包含一个指定名称的标签（值不重要）。使用此运算符时，
不应指定values字段。
• DoesNotExist : pod不得包含有指定名称的标签。values属性不得指定。        

4.4 使用DaemonSet在每个节点上运行一个pod

To run a pod on all cluster nodes, you create a DaemonSet object, which is much like a ReplicationController 
or a ReplicaSet, except that pods created by a DaemonSet already have a target node specified and skip the 
Kubernetes Scheduler. They aren’t scattered around the cluster randomly.

4.4.2 使用DaemonSet 只在特定的节点上运行pod
DaemonSet 将pod 部署到集群中的所有节点上，除非指定这些pod 只在部分节
点上运行。这是通过pod 模板中的nodeSelector 属性指定的，这是DaemonS et
定义的一部分

//Daemon Set的特殊性
注意在本书的后面，你将了解到节，点可以被设置为不可调度的， 防止pod 被部
署到节，是上。DaemonSet 甚至会将pod 部署到这些节，主上，因为无法调度的属性只
会被调度器使用，而Daemon Set 管理的pod 则完全绕过调度器。这是预期的，因为
DaemonSet 的目的是运行系统服务，即使是在不可调度的节点上，系统服务通常也
需要运行。
apiVersion: apps/v1beta2
kind: DaemonSet
metadata:
  name: ssd-monitor
spec:
  selector:
    matchLabels:
      app: ssd-monitor #节点选择器 
  template:
    metadata:
      labels:
        app: ssd-monitor 
    spec:
      nodeSelector:
        disk: ssd
      containers:
      - name: main
        image: luksa/ssd-monitor
#覆写label会删除之前部署的pod  
$ kubectl label node minikube disk=ssd
node "minikube" labeled

4.5 运行执行单个任务的pod
到目前为止， 我们只谈论了需要持续运行的pod。你会遇到只想运行完成工作
后就终止任务的清况。ReplicationController 、ReplicaSet和DaemonSet 会持续运行任
务， 永远达不到完成态。这些pod 中的进程在退出时会重新启动。但是在一个可完
成的任务中， 其进程终止后， 不应该再重新启动。

Kubemetes 通过Job 资源提供了对此的支持， 这与我们在本章中讨论的其他资
源类似， 但它允许你运行一种pod, 该pod 在内部进程成功结束时， 不重启容器。
一旦任务完成， pod 就被认为处千完成状态。

在发生节点故障时，该节点上由Job 管理的pod将按照ReplicaSet 的pod 的方式，
重新安排到其他节点。如果进程本身异常退出（进程返回错误退出代码时）， 可以
将Job 配置为重新启动容器。

可以在未
托管的pod 中运行任务并等待它完成， 但是如果发生节点异常或pod 在执行任务时
被从节点中逐出， 则需要手动重新创建该任务。

apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        app: batch-job  #不是标签选择器，将会基于模板中的标签被创建
    spec:
      restartPolicy: OnFailure #不能重启，设为OnFailure或Never 
      containers:
      - name: main
        image: luksa/batch-job

$k get pod jobs 
完成后pod未被删除的原因是允许你查阅其日志。例如：
$ kubectl logs batch-job-28qf4
Fri Apr 29 09:58:22 UTC 2016 Batch job starting
Fri Apr 29 10: 00: 22 UTC 2016 Finished succesfully
pod 可以被直接删除， 或者在删除创建它的Job时被删除

4.5.4 在Job中运行多个pod实例
作业可以配置为创建多个pod实例， 并以并行或串行方式运行它们。这是通过
在Job配置中设置comple巨ons和parallelism属性来完成的
apiVersion: batch/v1
kind: Job
metadata:
  name: multi-completion-batch-job
spec:
  completions: 5             #顺序运行5个pod 
  template:
    <template is the same as in listing 4.11>

apiVersion: batch/v1
kind: Job
metadata:
  name: multi-completion-batch-job
spec:
  completions: 5
  parallelism: 2               #最多并行两个pod
  template:
    <same as in listing 4.11>

通过在pod 配置中设置activeDeadlineSeconds 属性，可以限制pod的时间。
如果pod 运行时间超过此时间， 系统将尝试终止pod, 并将Job 标记为失败。
注意通过指定Job manifest 中的spec.backoff巨m辽字段， 可以配置Job
在被标记为失败之前可以重试的次数。如果你没有明确指定它， 则默认为6。

4.6 安排Job定期运行或在将来运行一次
运行任务的时间表以知名的cron 格式指定

apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: batch-job-every-fifteen-minutes
spec:
  schedule: "0,15,30,45 * * * *"  #每天 每小时 的 0 15 30 45 分钟运行
  jobTemplate:                    # 分钟 小时 天 月 周 命令 , 从右向左看，剩下的都是分钟 
    spec:
      template:
        metadata:
          labels:
            app: periodic-batch-job
        spec:
          restartPolicy: OnFailure
          containers:
          - name: main
            image: luksa/batch-job

在计划的时间内，CronJob资源会创建Job资源，然后Job创建pod。
apiVersion: batch/v1beta1
kind: CronJob
spec:
  schedule: "0,15,30,45 * * * *"
  startingDeadlineSeconds: 15  #任务启动的截至日期，若不启动则失败
  ...

#幂等任务 
In normal circumstances, a CronJob always creates only a single Job for each execution configured in 
the schedule, but it may happen that two Jobs are created at the same time, or none at all. To combat 
the first problem, your jobs should be idempotent (running them multiple times instead of once shouldn’t 
lead to unwanted results). For the second problem, make sure that the next job run performs any work that 
should have been done by the previous (missed) run.

——————————————————————————————————————————————————C5——————————————————————————————————————————————
Chapter 5. Services: enabling clients to discover and talk to pods
#给端口命名 
kind: Pod
spec:
  containers:
  - name: kubia
    ports:
    - name: http
      containerPort: 8080
    - name: https
      containerPort: 8443

#在服务中引用端口 
apiVersion: v1
kind: Service
spec:
  ports:
  - name: http
    port: 80
    targetPort: http
  - name: https
    port: 443
    targetPort: https
即使更换端口号也无须更改服务spec 

5.1.2 服务发现
通过环境变量发现服务
#服务要先于pod创建或在服务创建后，pod重新启动 
$ kubectl exec kubia-3inly env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=kubia-3inly
KUBERNETES_SERVICE_HOST=10.111.240.1
KUBERNETES_SERVICE_PORT=443
...
KUBIA_SERVICE_HOST=10.111.249.153
KUBIA_SERVICE_PORT=80
...

通过DNS 发现服务
还记得第3 章中在kube - system 命名空间下列出的所有pod 的名称吗？其中
一个po d 被称作kube-dns ，当前的kube - system 的命名空间中也包含了一个具
有相同名字的响应服务。
就像名字的暗示， 这个pod 运行DNS 服务，在集群中的其他pod 都被配置成
使用其作为dns ( Kubemetes 通过修改每个容器的／ etc/reso l v . conf 文件实现）。
运行在pod 上的进程DNS 查询都会被Kubemetes 自身的DNS 服务器响应， 该服务
器知道系统中运行的所有服务。
注意pod 是否使用内部的DNS 服务器是根据pod 中spec 的dnsPolicy 属性
来决定

每个服务从内部DNS 服务器中获得一个DNS 条目， 客户端的pod 在知道服务
名称的情况下可以通过全限定域名CFQDN ）来访问，而不是诉诸于环境变量。

通过FQDN 连接服务
前端p o d 可以通过打开以下FQDN 的连接来访问
后端数据库服务：
backend database.default svc cluster. l ocal
backend- database 对应于服务名称， default 表示服务在其中定义的名称
空间，而svc.cluster.l ocal 是在所有集群本地服务名称中使用的可配置集群
域后缀。
如果前端pod 和数据库pod 在同一个命名
空间下，可以省略svc.cluster . local 后缀，甚至命名空间
#实践失败, 要创建另一个pod
root@kubia-3inly:/# curl http://kubia.default.svc.cluster.local
You've hit kubia-5asi2

root@kubia-3inly:/# curl http://kubia.default
You've hit kubia-3inly

root@kubia-3inly:/# curl http://kubia
You've hit kubia-8awf3

$ kubect1 get endpoints kubia
$ kubectl describe svc kubia
Name:                kubia
Namespace:           default
Labels:              <none>
Selector:            app=kubia
Type:                ClusterIP
IP:                  10.111.249.153
Port:                <unset> 80/TCP
Endpoints:           10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080 #pod 的IP 
Session Affinity:    None
No events.

5.2.2 手动配置服务的endpoint

apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  ports:
  - port: 80
上面external-service与下面的external-service必须一致 
apiVersion: v1
kind: Endpoints
metadata:
  name: external-service
subsets:
  - addresses:
    - ip: 11.11.11.11
    - ip: 22.22.22.22
    ports:
    - port: 80

5.2.3 为外部服务创建别名fully qualified domain name (FQDN).
apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  type: ExternalName  #类型为ExternalName
  externalName: someapi.somecompany.com #实际的服务FQDN
  ports:
  - port: 80

After the service is created, pods can connect to the external service through the 
external-service.default.svc.cluster.local domain name (or even external-service) 
instead of using the service’s actual FQDN. This hides the actual service name and its location 
from pods consuming the service, allowing you to modify the service definition and point it to a 
different service any time later, by only changing the externalName attribute
//如何做到
 or by changing the
 type back to ClusterIP and creating an Endpoints object for the service—either manually or by 
 specifying a label selector on the service and having it created automatically.
//??
 ExternalName services are implemented solely at the DNS level—a simple CNAME DNS record is created for 
 the service. Therefore, clients connecting to the service will connect to the external service directly, 
 bypassing the service proxy completely. For this reason, these types of services don’t even get a cluster IP.

注意CNAME记录指向完全限定的域名而不是数字IP地址。

5.3 将服务暴露给外部客户端
将服务的类型设置成NodePort-每个集群节点都会在节点上打开一
个端口， 对于NodePort服务， 每个集群节点在节点本身（因此得名叫
NodePort)上打开一个端口，并将在该端口上接收到的流量重定向到基础服务。
该服务仅在内部集群IP 和端口上才可访间， 但也可通过所有节点上的专用端
口访问。
• 将服务的类型设置成LoadBalance, NodePort类型的一种扩展一—这使得
服务可以通过一个专用的负载均衡器来访问， 这是由Kubernetes中正在运行
的云基础设施提供的。负载均衡器将流量重定向到跨所有节点的节点端口。
客户端通过负载均衡器的IP 连接到服务。
• 创建一个Ingress资源， 这是一个完全不同的机制， 通过一个IP地址公开多
个服务——它运行在HTTP 层（网络协议第7 层）上， 因此可以提供比工作
在第4层的服务更多的功能。我们将在5.4节介绍Ingress资源。

#本质是在所有节点上曝光了一个相同的端口，不好的实践，应该采用负载均衡svc
apiVersion: v1
kind: Service
metadata:
  name: kubia-nodeport
spec:
  type: NodePort
  ports:
  - port: 80    #服务的internal cluster IP 的port 
    targetPort: 8080 #背后的pod的port
    nodePort: 30123 #通过集群的每个nodes的30123端口和获取到服务 
  selector:
    app: kubia

$ kubectl get svc kubia-nodeport
NAME             CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubia-nodeport   10.111.254.223   <nodes>       80:30123/TCP   2m
#找出节点的external IP 通过30123访问 
The service is accessible at the following addresses:
10.111.254.223:80 #集群IP 
<1st node's IP>:30123
<2nd node's IP>:30123, and so on.


 k apply -f kubia-svc-nodeport.yaml
  gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123 #设置GKE防火墙 

 kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'
 https://kubernetes.io/docs/reference/kubectl/jsonpath/

使用Minikube时，可以运行minikube sevrvice <service-name> [-n
<name space>]命令， 通过浏览器轻松访问NodePort服务

5.3.2 通过负载均衡器将服务暴露出来
LoadBadancer服务是NodePo江服务的扩展

可以通过将服务配置为仅将外部通信重定向到接收连接的节点上运行的pod来
阻止此额外跳数。这是通过在服务的spec部分中设置externalTrafficPo巨cy
字段来完成的：
spec:
externalTrafficPolicy: Local 
如果没有本地pod存在， 则连接将挂起;
// Source Network Address Translation
//??
记住客户端IP是不记录的
通常， 当集群内的客户端连接到服务时， 支持服务的pod可以获取客户端的IP
地址。但是， 当通过节点端口接收到连接时， 由于对数据包执行了源网络地址转换
(SNAT), 因此数据包的源IP将发生更改。
后端的pod无法看到实际的客户端IP, 这对于某些需要了解客户端IP的应用
程序来说可能是个问题。例如， 对千Web服务器， 这意味着访问日志无法显示浏览
器的IP。
上一节中描述的local外部流量策略会影响客户端IP的保留， 因为在接收连
接的节点和托管目标pod的节点之间没有额外的跳跃（不执行SNAT)。

5.4 通过Ingress暴露服务
为什么需要Ingress
一个重要的原因是每个LoadBalancer 服务都需要自己的负载均衡器， 以及
独有的公有IP 地址， 而Ingress 只需要一个公网IP 就能为许多服务提供访问。当客
户端向Ingress 发送HTTP 请求时， Ingress 会根据请求的主机名和路径决定请求转
发到的服务，

只有Ingress控制器在集群中运行，
Ingress 资源才能正常工作。

$ minikube addons list
$ minikube addons enable ingress
$ kubectl get po --all-namespaces

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: kubia-nodeport // 到达kubia.example.com的连接全部转发到 kubia-nodeport的80端口 
          servicePort: 80
k get ingresses.networking.k8s.io kubia
在／ ect / h o sts 文件（ Windows 系统为C : \windows\system32\
drivers\etc\hosts ） 中添加下面一行内容：
192 168.99.100 kubia.example.com
echo 34.98.107.163 kubia.example.com >> /etc/hosts #GKE 授权失败 

图5.1 0 显示了客户端如何通过Ingres s 控制器连接到其中一个po d 。客户端首
先对kubi a. example.com 执行DNS 查找， DNS 服务器（或本地操作系统）返回了
Ingress 控制器的IP 。客户端然后向Ingress 控制器发送HTTP 请求，并在Host 头
中指定kubia . example.com 。控制器从该头部确定客户端尝试访问哪个服务，通
过与该服务关联的E ndpo int 对象查看pod IP ， 并将客户端的请求转发给其中一个
p od 。
如你所见， Ingress 控制器不会将请求转发给该服务，只用它来选择一个pod 。
大多数（即使不是全部）控制器都是这样工作的。

将不同的服务映射到相同主机的不同路径
...
  - host: kubia.example.com
    http:
      paths:
      - path: /kubia
        backend:
          serviceName: kubia
          servicePort: 80
      - path: /bar
        backend:
          serviceName: bar
          servicePort: 80

spec:
  rules:
  - host: foo.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: foo
          servicePort: 80
  - host: bar.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: bar
          servicePort: 80

5.4.4 配置Ingress 处理TLS 传输
当客户端创建到Ingress 控制器的TLS 连接时，控制器将终止TLS 连接。客户
端和控制器之间的通信是加密的，而控制器和后端pod 之间的通信则不是。运行在
pod 上的应用程序不需要支持TLS 。例如，如果pod 运行web 服务器，则它只能接
收HTTP 通信，并让Ingress 控制器负责处理与TLS 相关的所有内容。要使控制器
能够这样做，需要将证书和私钥附加到Ingress 。这两个必需资源存储在称为Secret
的Kubernetes 资源中，然后在Ingress manifest 中引用它。

5.5.1 介绍就绪探针
Kubernetes 只能检查在容器
中运行的应用程序是否响应一个简单的GET／请求，或者它可以响应特定的URL 路
径（该URL 导致应用程序执行一系列检查以确定它是否准备就绪） 。

与存活探针不同，如果容器未通过准备检查，则不会被终止或重新启动。这是
存活探针与就绪探针之间的重要区别。存活探针通过杀死异常的容器并用新的正常
容器替代它们来保持pod 正常工作，而就绪探针确保只有准备好处理请求的pod 才
可以接收它们（请求）。这在容器启动时最为必要， 当然在容器运行一段时间后也
是有用的。

可以通过kubectl ed江命令来向已存在的ReplicationController中的pod模
板添加探针。
$ kubectl edit re kubia

更改ReplicationController的
pod模板对现有的pod没有影响。
换句话说， 现有的所有pod仍没有定义准备就绪探针。可以通过使用
kubectl get pods列出pod并查看READY列。需要删除pod并让它们通过
ReplicationController重新创建。新的pod将进行就绪检查会一直失败，并且不会将
其作为服务的端点，直到在每个pod中创建/var/ready文件
$ kubectl edit re kubia
apiVersion: v1
kind: ReplicationController
...
spec:
  ...
  template:
    ...
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        readinessProbe:
          exec:
            command:   #文件存在则返回退出码0 
            - ls
            - /var/ready
        ...

$ kubect1 exec kubia-2rlqb -- touch /var/ready
提示如果想要从某个服务中手动添加或删除pod, 请将enabled=true 作为
标签添加到pod, 以及服务的标签选择器中。当想要从服务中移除pod 时，删除标签

5.6 使用headless服务来发现独立的pod
幸运的是， Kubemetes 允许客户通过DNS 查找发现pod IP。通常， 当执行服
务的DNS 查找时， DNS 服务器会返回单个IP一服务的集群IP。但是， 如果告诉
Kubemetes, 不需要为服务提供集群IP (通过在服务spec 中将clusterIP 字段设
置为None 来完成此操作）， 则DNS 服务器将返回pod IP 而不是单个服务IP

DNS 服务器不会返回单个DNS A记录， 而是会为该服务返回多个A记录， 每
个记录指向当时支持该服务的单个pod的IP。客户端因此可以做一个简单的DNS A
记录查找并获取属于该服务一部分的所有pod 的IP。客户端可以使用该信息连接到
其中的一个、多个或全部。

apiVersion: v1
kind: Service
metadata:
  name: kubia-headless  #使得服务headless 
spec:
  clusterIP: None
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia

nslookup (or the dig) binary
Docker Hub上提供的tutum/dnsutils 容器镜像， 它包含nslookup和dig二
进制文
$ kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1 --command -- sleep infinity
pod "dnsutils" created
--generator=run-pod/vl 选项中， 该选项让kubect1直接创建pod
理解headless 服务的DNSA记录解析
$ kubectl exec dnsutils nslookup kubia-headless #返回pod ip
Server:         10.124.0.10
Address:        10.124.0.10#53
Non-authoritative answer:
Name:   kubia-headless.default.svc.cluster.local
Address: 10.120.2.35
Name:   kubia-headless.default.svc.cluster.local
Address: 10.120.0.42

$ kubectl exec dnsutils nslookup kubia
Server:         10.124.0.10
Address:        10.124.0.10#53

Name:   kubia.default.svc.cluster.local
Address: 10.124.3.80

使用DNS查找机制来查
找那些未准备好的pod。要告诉Kubemetes无论pod的准备状态如何， 希望将所有
pod 添加到服务中。必须将以下注解添加到服务中：
kind: Service
metadata:
annotations:
service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
警告就像说的那样， 注解名称表明了这是一个alpha功能。KubernetesService
API已经支待一个名为publishNotReadyAddresses 的新服务规范字段， 它将
替换tolerate-unready-endpoints注

———————————————————————————————————————————————————————C7————————————————————————————————————————
第七章 
7.3 为容器设置环境变量
//覆盖参数
kind: Pod
spec:
  containers:
  - image: some/image
    command: ["/bin/command"]
    args: ["arg1", "arg2", "arg3"]

//在器级别提供环境变量参数
kind: Pod
spec:
 containers:
 - image: luksa/fortune:env
   env:
   - name: INTERVAL
     value: "30"
   name: html-generator
...
//引用环境变量
env:
- name: FIRST_VAR
  value: "foo"
- name: SECOND_VAR
  value: "$(FIRST_VAR)bar"

Con句Map 中的键名必须是一个合法的DNS 子域，仅包含数字字母、破
折号、下画线以及园点。首位的圆，点符号是可选的
//从字面量创建congimap
$ kubectl create configmap fortune-config --from-literal=sleep-interval=25
configmap "fortune-config" created
$ kubectl create configmap myconfigmap
➥   --from-literal=foo=bar --from-literal=bar=baz --from-literal=one=two
$ kubectl create -f fortune-config.yaml

//从文件创建configmap，并将文件内容单独存储为ConfigMap 中的条目
$ kubectl create configmap my-config --from-file=customkey=config-file.conf
//从文件夹创建configmap, 每个文件是一个条目 
$ kubectl create configmap my-config --from-file=/path/to/dir #没有/

7.4.3 给容器传递ConfigMap 条目作为环境变量
//给容器的环境变量传递ConfigMap 的条目
//可以标记对ConfigMap 的引用是可选的（设直configMapKeyRef .
//optional: true ）。这样，即使ConfigMap 不存在，容器也能正常启动。
apiVersion: v1
kind: Pod
metadata:
  name: fortune-env-from-configmap
spec:
  containers:
  - image: luksa/fortune:env
    env:
    - name: INTERVAL             //环境变量
      valueFrom:
        configMapKeyRef:
          name: fortune-config
          key: sleep-interval    //为变量设定值 
...

//通过envFrom属性字段将所有条目暴露作为环境变量
spec:
  containers:
  - image: some-image
    envFrom:
    - prefix: CONFIG_           //为所有条目添加前缀
      configMapRef:
        name: my-config-map
...
config_foo-bar不是合法的环境变量名 

//使用条目作为参数值
apiVersion: v1
kind: Pod
metadata:
  name: fortune-args-from-configmap
spec:
  containers:
  - image: luksa/fortune:args
    env:
    - name: INTERVAL            //定义环境变量 
      valueFrom:
        configMapKeyRef:
          name: fortune-config
          key: sleep-interval  //给变量赋值
    args: ["$(INTERVAL)"]      //在参数中引用环境变量 
...

//configMap 卷会将Co nfigMap 中的每个条目均暴露成一个文件
//副作用：挂载引用configMap的卷会隐藏/etc/nginx/conf.d目录下的所有文件 
apiVersion: v1
kind: Pod
metadata:
  name: fortune-configmap-volume
spec:
  containers:
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    ...
    - name: config                          //挂载配置卷
      mountPath: /etc/nginx/conf.d
      readOnly: true
    ...
  volumes:
  ...
  - name: config                            //挂载卷引用configMap
    configMap:                              
      name: fortune-config
  ...

kubectl get configmap fortune-config -o yaml //考察configMap 
$ kubectl get configmap fortune-config -o yaml
apiVersion: v1
data:
  my-nginx-config.conf: |                    #管道符表示后续条目是多行字面量
    server {
      listen              80;
      server_name         www.kubia-example.com;

      gzip on;
      gzip_types text/plain application/xml;

      location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
      }
    }
  sleep-interval: |
    25
kind: ConfigMap
...

/卷内暴露指定的ConfigMap条目
//通过卷的items 属性能够指定哪些条目会被暴露作为configMap卷中的文件
volumes:
  - name: config
    configMap:
      name: fortune-config
      items:
      - key: my-nginx-config.conf
        path: gzip.conf                //指定对应的文件名
		
//volumeMount额外的subPath字段可以被用作挂载卷中的某个
//独立文件或者是文件夹，无须挂载完整卷
spec:
  containers:
  - image: some/image
    volumeMounts:
    - name: myvolume
      mountPath: /etc/someconfig.conf
      subPath: myconfig.conf              //仅挂载单个文件 
	  
	  
//设置权限
volumes:
  - name: config
    configMap:
      name: fortune-config
      defaultMode: "6600"   //设置

//ConfigMap被更新之后， 卷中引用它的所有文件也会相应更新， 进程发现文件
//被改变之后进行重载	
kubectl edit configmap fortune-config //热更新

//Secret也是资源对象;每个pod挂载了包含三个文件的secret卷
kubectl get secrets
kubectl describe secrets

//创建一个Secret
$ kubectl create secret generic fortune-https --from-file=https.key
➥   --from-file=https.cert --from-file=foo
secret "fortune-https" created

Base64 encoding allows you to include the binary data in YAML or JSON, 
which are both plain-text formats
kind: Secret
apiVersion: v1
stringData:
  foo: plain text           //写入纯文本；stringData的字段不可读，编码后显示在data中
data:
  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ...
  https.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE...

//Secret 卷存储于内存
$ kubectl exec fortune-https -c web-server -- mount | grep certs
tmpfs on /etc/nginx/certs type tmpfs (ro,relatime)

 apiVersion: v1
kind: Pod
metadata:
  name: fortune-https
spec:
  containers:
  - image: luksa/fortune:env
    name: html-generator
    env:
    - name: INTERVAL
      valueFrom:
        configMapKeyRef:
          name: fortune-config
          key: sleep-interval
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    - name: config
      mountPath: /etc/nginx/conf.d
      readOnly: true
    - name: certs
      mountPath: /etc/nginx/certs/   //挂载至此
      readOnly: true
    ports:
    - containerPort: 80
    - containerPort: 443
  volumes:
  - name: html
    emptyDir: {}
  - name: config
    configMap:
      name: fortune-config
      items:
      - key: my-nginx-config.conf
        path: https.conf
  - name: certs                    //要挂载的卷
    secret:
      secretName: fortune-https

//通过环境变量暴露Secret 条目
env:
    - name: FOO_SECRET
      valueFrom:
        secretKeyRef:
          name: fortune-https
          key: foo
Kubernetes 允许通过环境变量暴露Secret，然而此特性的使用往往不是一个好
主意。应用程序通常会在错误报告时转储环境变量，或者是启动时打印在应用日志
中，无意中暴露了Secret 信息。另外，子进程会继承父进程的所有环境变量，如果
是通过第三方二进制程序启动应用，你并不知道它使用敏感数据做了什么

运行一个镜像来源于私有仓库的pod 时，需要做以下两件事：
．创建包含Docker 镜像仓库证书的Secre t 。
• pod 定义中的imagePullSecrets 宇段引用该Secret 。
$ kubectl create secret docker-registry mydockerhubsecret \
  --docker-username=myusername --docker-password=mypassword \
  --docker-email=my.email@provider.com

为了Kub em etes 从私有镜像仓库拉取镜像时能够使用Secret ，需要在pod 定义
中指定docker - registry Secret 的名称
apiVersion: v1
kind: Pod
metadata:
  name: private-pod
spec:
  imagePullSecrets:
  - name: mydockerhubsecret
  containers:
  - image: username/private:tag
    name: main

==========================Chapter 8=================
Accessing pod metadata and other resources from applications
//通过环境变量暴露元数据
apiVersion: v1
kind: Pod
metadata:
  name: downward
spec:
  containers:
  - name: main
    image: busybox
    command: ["sleep", "9999999"]
    resources:
      requests:
        cpu: 15m
        memory: 100Ki
      limits:
        cpu: 100m
        memory: 4Mi
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: SERVICE_ACCOUNT
      valueFrom:
        fieldRef:
          fieldPath: spec.serviceAccountName
    - name: CONTAINER_CPU_REQUEST_MILLICORES
      valueFrom:
        resourceFieldRef:
          resource: requests.cpu
          divisor: 1m
    - name: CONTAINER_MEMORY_LIMIT_KIBIBYTES
      valueFrom:
        resourceFieldRef:
          resource: limits.memory
          divisor: 1Ki
kubectl exec downward env //查看环境变量


//通过downwardAPI卷暴露元数据
//必须使用downwardAPI卷来暴露pod标签或注解
apiVersion: v1
kind: Pod
metadata:
  name: downward
  labels:
    foo: bar
  annotations:
    key1: value1
    key2: |
      multi
      line
      value
spec:
  containers:
  - name: main
    image: busybox
    command: ["sleep", "9999999"]
    resources:
      requests:
        cpu: 15m                 //m=milli-core，千分之一核，1代表整个核
        memory: 100Ki
      limits:
        cpu: 100m
        memory: 4Mi
    volumeMounts:
    - name: downward
      mountPath: /etc/downward
  volumes:
  - name: downward
    downwardAPI:
      items:
      - path: "podName"
        fieldRef:
          fieldPath: metadata.name      //挂载到文件/etc/downward/podName,内容为metadata.name
      - path: "podNamespace"            //的域，是键值对
        fieldRef:
          fieldPath: metadata.namespace
      - path: "labels"

        fieldRef:

          fieldPath: metadata.labels

      - path: "annotations"
        fieldRef:
          fieldPath: metadata.annotations
      - path: "containerCpuRequestMilliCores"
        resourceFieldRef:
          containerName: main
          resource: requests.cpu
          divisor: 1m                       //除数，用上面的单位除以这个，15个单位的核
      - path: "containerMemoryLimitBytes"
        resourceFieldRef:
          containerName: main               //容器级元数据，指定容器名 
          resource: limits.memory
          divisor: 1ki
kubect1 exec downward ls -lL /etc/downward		  
与configMAp和secret卷一样，可以通过pod定义中downward.AP工
卷的defaultMode属性来改变文件的访问权限设置。
//自动更新吗
当标签和注解被修改后，Kubemetes会更新存有相关信息的文件
使用卷的方式，可以传递一个容器的资源字段到另一个容器，因为卷的定义
是基于pod，同一个pod的容器可以沟通

$ kubect1 proxy //访问proxy与api交互

从pod内部与API服务器进行交互,令牌、密钥等来访问服务器API

//最简单的方式，但是运行失败 
通过ambassador 容器简化与API 服务器的交互

//各种语言的库、包
使用客户端库与API服务器交互



-------------------------第九章----------------------------
Chapter 9. Deployments: updating applications declaratively
有以下两种方法可以更新所有pod:
• 直接删除所有现有的pod, 然后创建新的pod。
• 也可以先创建新的pod, 并等待它们成功运行之后， 再删除旧的pod。可以
先创建所有新的pod, 然后一次性删除所有旧的pod, 或者按顺序创建新的
pod, 然后逐渐删除旧的pod。

在使用Deployment 时， 实际的pod
是由Deployment 的Replicaset 创建和管理的， 而不是由Deployment 直接创建和管
理的
k create -f kubia-deployment-v1.yaml --record //--record 记录历史版本号
k rollout status deployment kubia //查看部署状态 
k get replicasets.apps //查看deployment的哈希值

默认策略是执行滚动更新（策略名为RollingUpdate)。另一种策略为Recreate,
它会一次性删除所有旧版本的pod, 然后创建新的pod,
kubectl patch deployment kubia -p '{"spec": {"minReadySeconds": 10}}' //更新一个值 
//重新设定镜像，触发滚动升级
kubectl set image deployment kubia nodejs=luksa/kubia:v2

如果Deployment 中的pod 模板引用了一个ConfigMap (或Secret), 那么
更改ConfigMap 资原本身将不会触发升级操作。如果真的需要修改应用程序的配置
并想触发更新的话， 可以通过创建一个新的ConfigMap 并修改pod 模板引用新的
ConfigMap。

kubectl get rs //列出资源

kubectl rollout status deployment kubia

//升级到错误版本
kubectl set image deployment kubia nodejs=luksa/kubia:v3
//回滚一次升级
k rollout undo deployment kubia 
//回滚历史；与教材有出入，只显示v1版本，未显示v2 v3 版本的历史
k rollout history deployment kubia
//通过指定Deployment 的revisionHistoryLimit 属性来限制历史版本数量。默认值是2
$ kubectl rollout undo deployment kubia - -to-revision=l //回滚到特定版本 
 
 spec:
  strategy:
    rollingUpdate:
      maxSurge: 1             //pod的总数目不能超过这个数目
      maxUnavailable: 0       //可用的数目不能少于期望数目减去maxUnavailable
    type: RollingUpdate

k rollout pause deployment kubia 

//使用kubectl apply -f 来升级Deployment
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  minReadySeconds: 10       //pod从状态ready-->available需要的时间/持续时间 
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v3
        name: nodejs
        readinessProbe:
          periodSeconds: 1
          httpGet:
            path: /
            port: 8080

//当一个pod的所有容器的readiness探测返回成功时，pod处于ready状态，在被
//视为available之前，ready状态之后，还要等待minReadySeconds秒数；
//如果readiness探测失败，新版本回滚就会被阻塞 
The minReadySeconds property specifies how long a newly created pod should 
be ready before the pod is treated as available. Until the pod is available, 
the rollout process will not continue (remember the maxUnavailable property?). 
A pod is ready when readiness probes of all its containers return a success. 
If a new pod isn’t functioning properly and its readiness probe starts failing 
before minReadySeconds have passed, the rollout of the new version will effectively 
be blocked.

//配置progressDeadlineSeconds属性，超过这个值后，就会认为Deployment失败
The time after which the Deployment is considered failed is configurable 
through the progressDeadlineSeconds property in the Deployment spec.
//yaml文件的标签为kubia,可以重用之前创建的service kubia,该service 的SELECTOR 为app=kubia
labels:
   app: kubia
   

-----------------------Chapter10----------------------------
//pod模板引用特定的持久卷声明，所有的副本将会使用相同的持久卷声明，共享底层的持久卷
Chapter 10. StatefulSets: deploying replicated stateful application
ReplicaSets create multiple pod replicas from a single pod template. 
These replicas don’t differ from each other, apart from their name and IP address. 
If the pod template includes a volume, which refers to a specific PersistentVolumeClaim, 
all replicas of the ReplicaSet will use the exact same PersistentVolumeClaim and 
therefore the same PersistentVolume bound by the claim.

//运行每个实例都有单独存储的多副本
但是在Kubemetes 中， 每次重新调度一个
pod, 这个新的pod就有一个新的主机名和IP地址， 这样就要求当集群中任何一个
成员被重新调度后， 整个应用集群都需要重新配置

Statefulset创建的pod副
本并不是完全一样的。每个pod都可以拥有一组独立的数据卷（持久化状态）

//发现服务 
For this reason, a StatefulSet requires you to create a corresponding governing 
headless Service that’s used to provide the actual network identity to each pod. 
Through this Service, each pod gets its own DNS entry, so its peers and possibly 
other clients in the cluster can address the pod by its hostname. For example, if 
the governing Service belongs to the default namespace and is called foo, and one 
of the pods is called A-0, you can reach the pod through its fully qualified domain 
name, which is a-0.foo.default.svc.cluster.local. You can’t do that with pods managed 
by a ReplicaSet.

通过Statefulset 部署应用
为了部署你的应用， 需要创建两个（或三个） 不同类型的对象：
• 存储你数据文件的持久卷（当集群不支持持久卷的动态供应时， 需要手动创
建）
• Statefulset必需的一个控制Service
• Statefulset本身

//伸缩statfulset
缩容一个Statefulset 将会最先删除最高索引值
的实例，所以缩容的结果是可预知的。

因为Statfulset 缩容任何时候只会操作一个pod 实例，所以有状态应用的缩容
不会很迅速;
StatefulSet在有实例不健康的情况下是不允许做缩容操作的。若
一个实例是不健康的，而这时再缩容一个实例的话，也就意味着你实际上同时失去
了两个集群成员。

扩容StatefulSet 增加一个副本数时， 会创建两个或更多的API 对象（一个pod
和与之关联的一个或多个持久卷声明） 。但是对缩容来说， 则只会删除一个pod ， 而
遗留下之前创建的声明;基于这个原因， 当你需要释放特定
的持久卷时， 需要手动删除对应的持久卷声明。
因为缩容Statefulset 时会保留持久卷声明， 所以在随后的扩容操作中， 新的pod
实例会使用绑定在持久卷上的相同声明和其上的数据（如图10.9 所示）。当你因为
误操作而缩容一个Statefulset 后，可以做一次扩容来弥补自己的过失， 新的pod 实
例会运行到与之前完全一致的状态（名字也是一样的〉。

//创建StatefulSet 
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: kubia
spec:
  serviceName: kubia
  replicas: 2
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia-pet
        ports:
        - name: http
          containerPort: 8080
        volumeMounts:
        - name: data
          mountPath: /var/data
  volumeClaimTemplates:           #卷声明模板为每个pod创建一个PersistentVolumeClaim 
  - metadata:                     #StatefulSet为每个PersistentVolumeClaim绑定一个卷 
      name: data
    spec:
      resources:
        requests:
          storage: 1Mi
      accessModes:
      - ReadWriteOnce
	  
//用于DNS SRV
apiVersion: v1
kind: Service
metadata:              
  name: kubia-2       
spec:
  clusterIP: None       #None代表headless服务
  selector:
    app: kubia          #标签为kubia的pod都属于这个service
  ports:
  - name: http
    port: 80
NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubia          ClusterIP   None           <none>        80/TCP    50m
//先要创建一个headless service 
$ kubectl run -it srvlookup --image=tutum/dnsutils --rm --restart=Never -- dig SRV kubia.default.svc.cluster.local
//在应用程序内调用函数访问集群内的pod
dns.resolveSrv();

//使用端口转发， 
k port-forward kubia-0  8888:8080


缩容一个Statefulset, 然后在完成后再扩容它， 与删除一个pod后让Statefulset
立马重新创建它的表现是没有区别的;
当缩容超过一个实例的时候， 会首先删除拥有最高索引
值的pod。只有当这个pod被完全终止后， 才会开始删除拥有次高索引值的pod;

//通过API服务器与pod通信
<apiServerHost>:<port>/api/vl/namespaces/default/pods/kubia-0/proxy/<path>
$ kubectl proxy
$curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
$ curl -X POST -d "Hey there! This greeting was submitted to kubia-0." localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
Data stored on pod kubia-0
$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/

//不是外部暴露的Service (它是一个常规的ClusterIPService, 不是一
//个NodePort或LoadBalancer typeService), 只能在你的集群内部访问它
apiVersion: v1
kind: Service
metadata:
  name: kubia-public
spec:
  selector:
    app: kubia
  ports:
  - port: 80
    targetPort: 8080
NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubia-public   ClusterIP   10.124.0.194   <none>        80/TCP    76m
//通过API服务器访问集群内部的服务
/api/v1/namespaces/<namespace>/services/<service name>/proxy/<path>
$k proxy
$ curl localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/ #随机访问pod

---更新Statefulset---
$ kubectl edit statefulsetkubia //k8s自动应用新的模板更新pod

$ gcloud compute ssh gke-kubia-default-pool-32a2cac8-m0g1
$ sudo ifconfig eth0 down //关闭节点网络适配器 
当一个节点状态转变为Unknown时，除非该节点的kubelet报告节点的pod已经停用，
否则k8s不会删除该节点上的pod;可以强制删除
$ kubectl delete po kubia-0 --force --grace-period 0
//恢复节点 
$ gcloud compute instances reset <node name>


-----------------------------C11-----------------------------------------
Chapter 11. Understanding Kubernetes internals
$ kubect1 get componentstatuses //查看组件状态 
尽管工作节点上的组件都需要运行在同一个节点上， 控制平面的组件可以被简
单地分割在多台服务器上。为了保证高可用性， 控制平面的每个组件可以有多个实
例。etcd和API服务器的多个实例可以同时并行工作， 但是， 调度器和控制器管理
器在给定时间内只能有一个实例起作用，其他实例处于待命模式。

//通过-o custom-columns选项自定义展示的列以及- -sort -by对资源列表进行排序
kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system

//etcd;乐观并发控制
//对照Java的非阻塞算法 
所有的Kubernetes 包含一个metadata.resourceVersion宇段，当更
新对象时，客户端需要返回该值到API服务器。如果版本值与etcd中存储的不
匹配，API服务器会拒绝该更新

pod按命名空间存储
//监听资源
kubect1 get pods --watch

可以在集群中运行多个调度器而非单个。然后， 对每一个pod, 可以通过在pod
特性中设置schedulerName 属性指定调度器来调度特定的pod。
未设置该属性的pod 由默认调度器调度， 因此其schedulerName 被设置为
default-scheduler 。其他设置了该属性的pod 会被默认调度器忽略掉， 它们要
么是手动调用， 要么被监听这类pod 的调度器调用。

控制器源代码

//The PersistentVolume controller
When a PersistentVolumeClaim pops up, the controller finds the best 
match for the claim by selecting the smallest PersistentVolume with 
the access mode matching the one requested in the claim and the declared 
capacity above the capacity requested in the claim. It does this by keeping 
an ordered list of PersistentVolumes for each access mode by ascending 
capacity and returning the first volume from the list.

//kubelet探测容器的是否存活，重启，终结容器，报告给API服务器 
The Kubelet is also the component that runs the container liveness probes, 
restarting containers when the probes fail. Lastly, it terminates containers 
when their Pod is deleted from the API server and notifies the server that the 
pod has terminated.
//kubelet可以以pods形式运行控制面板组件的容器化版本；DaemonSet
run the containerized versions of the Control Plane components as pods

如何部署插件
通过提交YAML清单文件到API服务器（本书的通用做法），这些组件会成为
插件并作为pod部署。有些组件是通过Deployment资源或者ReplicationController
资源部署的，有些是通过DaemonSet
kubect1 get rc -n kube-system
$ kubect1 get deploy -n kube-system

//DNS服务 
The DNS server pod is exposed through the kube-dns service, allowing 
the pod to be moved around the cluster, like any other pod. The service’s 
IP address is specified as the nameserver in the /etc/resolv.conf file inside 
every container deployed in the cluster. The kube-dns pod uses the API server’s 
watch mechanism to observe changes to Services and Endpoints and updates its 
DNS records with every change, allowing its clients to always get (fairly) 
up-to-date DNS information.

//--watch 选项观察事件 
$ kubectl get events --watch
//运行一个容器，观察基础容器；
//每个pod都有一个基础容器，保存命名空间，供pod里的其他容器使用
kubectl run nginx --image=nginx
gcloud compute ssh <node name>
minkube ssh 
docker ps

网络是由系统管理员或者ContainerNetwork Interface (CNI)插件建立的，
而非Kubemetes本身。

//基础设施容器创建了一个veth对, 一个接口在主机命名空间, 一个在容器命名空间 
Before the infrastructure container is started, a virtual Ethernet 
interface pair (a veth pair) is created for the container. One interface 
of the pair remains in the host’s namespace (you’ll see it listed as 
vethXXX when you run ifconfig on the node), whereas the other is moved 
into the container’s network namespace and renamed eth0. The two virtual 
interfaces are like two ends of a pipe (or like two network devices 
connected by an Ethernet cable)

//主机命名空间的接口附着到容器运行时的网络桥;
//从容器命名空间eth0接口接受到的信息会从网络桥出来
The interface in the host’s network namespace is attached to a network 
bridge that the container runtime is configured to use. The eth0 interface 
in the container is assigned an IP address from the bridge’s address range. 
Anything that an application running inside the container sends to the eth0 
network interface (the one in the container’s namespace), comes out at the 
other veth interface in the host’s namespace and is sent to the bridge. 
This means it can be received by any network interface that’s connected to the bridge.
//在节点上的所有pod通过相同的桥沟通 
If pod A sends a network packet to pod B, the packet first goes through 
pod A’s veth pair to the bridge and then through pod B’s veth pair. All 
containers on a node are connected to the same bridge, which means they 
can all communicate with each other. But to enable communication between 
containers running on different nodes, the bridges on those nodes need to 
be connected somehow

//网桥连接到节点上的物理适配器，如此，实现和别节点通信，但是节点要连接到相同
//网关，之间没有路由；因为pod Ip是私有的，路由会扔包  
有多种连接不同节点上的网桥的方式。可以通过overlay或underlay网络， 或
者常规的三层路由

跨整个集群的pod 的IP地址必须是唯一的， 所以跨节点的网桥必须使用非重叠
地址段， 防止不同节点上的pod拿到同一个IP

SDN可以让节点忽略底层网络拓扑，
Container Network Interface (CNI)
Calico
Flannel
Romana
Weave Net
And others

We’ve learned that each Service gets its own stable IP address and port. 
Clients (usually pods) use the service by connecting to this IP address 
and port. The IP address is virtual—it’s not assigned to any network 
interfaces and is never listed as either the source or the destination 
IP address in a network packet when the packet leaves the node. A key 
detail of Services is that they consist of an IP and port pair (or multiple 
IP and port pairs in the case of multi-port Services), so the service IP by 
itself doesn’t represent anything. That’s why you can’t ping them.

//kube-proxy拦截报文，重定向到pod
When a service is created in the API server, the virtual IP address is 
assigned to it immediately. Soon afterward, the API server notifies all 
kube-proxy agents running on the worker nodes that a new Service has been 
created. Then, each kube-proxy makes that service addressable on the node 
it’s running on. It does this by setting up a few iptables rules, which 
make sure each packet destined for the service IP/port pair is intercepted 
and its destination address modified, so the packet is redirected to one 
of the pods backing the service.
//Endpoints与service紧密相关
An Endpoints object holds the IP/port pairs of all the pods that back 
the service (an IP/port pair can also point to something other than a pod). 
That’s why the kube-proxy must also watch all Endpoints objects

//节点间的领导选举 
https://github.com/kubernetes-retired/contrib/tree/master/election

控制面板中的组件也可以使用领导选举机制;谁先到达就成为领导者 
kubectl get endpoints kube-scheduler -n kube-system -o yaml


----------------------C12-------------------------------

Chapter 12. Securing the Kubernetes API server
每个pod都与一个ServiceAccount相关
联，它代表了运行在pod中应用程序的身份证明。token文件持有ServiceAccount
的认证token。应用程序使用这个token连接API服务器时，身份认证插件会对
ServiceAccount进行身份认证，并将Set-viceAccount的用户名传回API服务器内部。
ServiceAccount用户名的格式像下面这样：
system:serviceaccount:<namespace>:<service accoun七name>
API服务器将这个用户名传给己配置好的授权插件，这决定该应用程序所尝试
执行的操作是否被ServiceAccount允许执行。
erviceAccount只不过是一种运行在pod中的应用程序和API服务器身份认证
的一种方式
#查看ServiceAccount列表
$ kubectl get sa

每个pod都与一个ServiceAccount相关联， 但
是多个pod可以使用同一个ServiceAccount；pod只能
使用同一个命名空间中的ServiceAccount。

在pod 的manifest 文件中， 可以用指定账户名称的方式将一个ServiceAccount
赋值给一个pod。如果不显式地指定ServiceAccount 的账户名称， pod 会使用在这个
命名空间中的默认ServiceAccount。
可以通过将不同的ServiceAccount 赋值给pod 来控制每个pod 可以访问的资源。
当API 服务器接收到一个带有认证token 的请求时， 服务器会用这个token 来验证
发送请求的客户端所关联的ServiceAccount 是否允许执行请求的操作。API 服务器
通过管理员配置好的系统级别认证插件来获取这些信息。其中一个现成的授权插件
是基千角色控制的插件(RBAC)

创建 serviceaccount
$ kubectl describe sa foo
$ kubectl describe sa foo
Name:               foo
Namespace:          default
Labels:             <none>

Image pull secrets: <none>        #自动添加到使用这个sa的pod中

Mountable secrets:  foo-token-qzq7j #如果mountable Secrets是强制的，则使用这个sa的pod只能
                                    #挂载这些secrets
Tokens:             foo-token-qzq7j #第一个被挂载进容器 

注解 kubernetes.io/enforce-mountable-secrets= "true" 表明，
使用这个sa的pod只能挂载sa的密钥 
#查看token;JSON Web Tokens (JWT)
$ kubectl describe secret foo-token-qzq7j

//将image Pull Secrets添加到每个使用这个sa的pod中 
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-service-account
imagePullSecrets:
- name: my-dockerhub-secret

kubectl exec -it curl-custom-sa -c main cat /var/run/secrets/kubernetes.io/serviceaccount/token
//403错误 
kubectl exec -it curl-custom-sa -c main -- curl localhost:8001/api/v1/pods
//可行
kubectl exec -it curl-custom-sa -c main -- curl localhost:8001/api/v1/

／／RBAC　
／／角色授予权限，绑定角色到具体的用户、组或ｓａ
The RBAC authorization rules are configured through four
resources, which can be grouped into two groups:
Roles and ClusterRoles, which specify which verbs can be performed on which resources.
RoleBindings and ClusterRoleBindings, which bind the above roles to specific users, 
groups, or ServiceAccounts.


//开启RBAC
minikube --extra-config=apiserver.Authorization.Mode=RBAC
//重启禁用的RBAC
$ kubectl delete clusterrolebinding permissive-binding

//创建不同命名空间的pod
$ kubectl create ns foo
namespace "foo" created
$ kubectl run test --image=luksa/kubectl-proxy -n foo
deployment "test" created
$ kubectl create ns bar
namespace "bar" created
$ kubectl run test --image=luksa/kubectl-proxy -n bar
deployment "test" created

$ kubectl exec -it test -n foo -- sh
/# curl localhost:8001/api/v1/namespaces/foo/services

//角色
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: foo            
  name: service-reader
rules:
- apiGroups: [""]          //service是核心apiGroup的资源，所以没有apiGroup,即为""
  verbs: ["get", "list"]
  resources: ["services"]  //复数 
  
//GKE创建roles时  
kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=sgydd001@gmail.com
$ kubectl create -f service-reader.yaml -n foo
$ kubectl create role service-reader --verb=get --verb=list --resource=services -n bar
role "service-reader" created

//绑定角色 
$ kubectl create rolebinding test --role=service-reader --serviceaccount=foo:default -n foo
$查看角色绑定
kubectl get rolebinding test -n foo -o yaml
注意如果要绑定一个角色到一个user （用户）而不是ServiceAccount 上， 使
用user 作为参数来指定用户名。如果要绑定角色到组，可以使用－－ group 参数。

//添加其他命名空间的serviceaccount
$ kubectl edit rolebinding test -n foo
subjects:
- kind: ServiceAccount
  name: default
  namespace: bar
  
12.2.4 使用ClusterRole 和C l usterRoleBinding
Cluster Role 是一种集群级资源，它允许访问没有命名空间的资源和非资源型的
URL ，或者作为单个命名空间内部绑定的公共角色，从而避免必须在每个命名空间
中重新定义相同的角色。
//集群角色要用集群角色绑定 进行绑定 
$ kubectl create clusterrole pv-reader --verb=get,list --resource=persistentvolumes
kubectl get clusterrole pv-reader -o yaml
$ kubectl create clusterrolebinding pv-test --clusterrole=pv-reader --serviceaccount=foo:default

/ # curl localhost:8001/api/v1/persistentvolumes

//允许访问非资源型的URL
API 服务器也会对外暴露非资源型的URL 。访问这些URL 也
必须要显式地授予权限－否则， API 服务器会拒绝客户端的请求。通常， 这个会通
过system : discovery ClusterRole 和相同命名的ClusterRoleBinding 帮你自动完
成，它出现在其他预定义的ClusterRoles 和ClusterRoleBindings
$ kubectl get clusterrole system:discovery -o yaml
//将Clusterrole绑定到所有认证过和没有认证过的用户上 
//上面命令显示对非资源url只能执行get操作 ，与下面说法矛盾 
对于非资源型URL ，使用普通的HTTP 动词，如post 、put 和patch,
而不是create 或update 。动词需要使用小写的形式指定

//这个角色访问的资源属于命名空间
$ kubectl get clusterrole view -o yaml
如果你创建了一个ClusterRoleBinding 并在它里面引用了ClusterRole,
在绑定中列出的主体可以在所有命名空间中查看指定的资源。相反， 如果你创建的
是一个RoleBinding, 那么在绑定中列出的主体只能查看在RoleBinding 命名空间中
的资源。现在可以尝试使用这两个选项

//使用ClusterRoleBinding绑定ClusterRole
curl localhost:8001/api/v1/pods  //查看集群级别资源
curl localhost:8001/api/v1/namespaces/foo/pods //查看命名空间级别资源 
kubectl create clusterrolebinding view-test --clusterrole=view --serviceaccount=foo:default
curl localhost:8001/api/v1/namespaces/bar/pods

//使用命名空间中的RoleBinding绑定ClusterRole
$ kubectl delete clusterrolebinding view-test
$ kubectl create rolebinding view-test --clusterrole=view --serviceaccount=foo:default -n foo

Kubemetes提供了一组默认的ClusterRole和ClusterRoleBinding
$ kubect1 get clusterrolebindings //查看
$ kubectl get clusterroles

view、ed江、adrnin和cluster-adrnin ClusterRole是最重要的角色， 它们
应该绑定到用户定义pod中的ServiceAccount上。
用view ClusterRole 允许对资源的只读访问

用editClusterRole允许对资源的修改
接下来是ed江ClusterRole, 它允许你修改一个命名空间中的资源， 同时允许
读取和修改Secret。但是，它也不允许查看或修改Role和RoleBinding, 这是为了
防止权限扩散。

用adminClusterRole赋予一个命名空间全部的控制权
ed江和adminClusterRole
一个命名空间中的资源的完全控制权是由admin ClusterRole赋予的。有这个
ClusterRole的主体可以读取和修改命名空间中的任何资源， 除了ResourceQuota (我
们会在第14章中了解它是什么）和命名空间资源本身。
之间的主要区别是能否在命名空间中查看和修改Role和RoleBinding。

用cluster-admin ClusterRole得到完全的控制
通过将cluster-adminClusterRole赋给主体，主体可以获得Kubernetes 集群
完全控制的权限。正如你前面了解的那样，adminClusterRole不允许用户修改命名
空间的ResourceQuota对象或者命名空间资源本身。如果你想允许用户这样做， 需
要创建一个指向cluster-adminClusterRole的RoleBinding。这使得RoleBinding
中包含的用户能够完全控制创建RoleBinding所在命名空间上的所有方面

了解其他默认的ClusterRole
默认的ClusterRole列表包含了大量其他的ClusterRole, 它们以sys 七em:
为前缀

---------------------------------C13-----------------------------
Chapter 13. Securing cluster nodes and the network

13.1 在pod中使用宿主节点的Linux命名空间
某个pod可能需要使用宿主节
点上的网络适配器，而不是自己的虚拟网络设备。这可以通过将pod spec中的
hostNetwork设置为true实现。
这意味着这个pod没有自己的IP地址；如果这个pod中的
某一进程绑定了某个端口，那么该进程将被绑定到宿主节点的端口上

13.1.2 绑定宿主节点上的端口而不使用宿主节点的网络命名空间
配置pod 的spec . containers . ports 字段中某个容器某一端口的hostPort 属性来实现。

//图13.2;NodePort-反向代理-服务发现
对于一个使用hostPort 的p o d ，到达宿主节点的端
口的连接会被直接转发到pod 的对应端口上：然而在NodePort 服务中，到达宿主
节点的端口的连接将被转发到随机选取的pod 上

很重要的一点是，如果一个pod 绑定了宿主节点上的一个特定端口，每个宿主
节点只能调度－个这样的pod 实例，因为两个进程不能绑定宿主机上的同一个端口。

//??If you have multiple nodes, you’ll see you can’t access the pod 
//through that port on the other nodes.
apiVersion: v1
kind: Pod
metadata:
  name: kubia-hostport
spec:
  containers:
  - image: luksa/kubia
    name: kubia
    ports:
    - containerPort: 8080
      hostPort: 9000
      protocol: TCP

pod spec 中的hostPID 和host IPC 选项与hostNetwork 相似。当它们被设
置为true 时， pod 中的容器会使用宿主节点的PID 和IPC 命名空间，分别允许它
们看到宿主机上的全部进程，或通过IPC 机制与它们通信。
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-host-pid-and-ipc
spec:
  hostPID: true
  hostIPC: true
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
k exec pod-with-host-pid-and-ipc -- ps aux

13.2 配置节点的安全上下文
容器运行时使用的用户在镜像中指定3 在Dockerfile 中，这是通过使用
USER 命令实现的。如果该命令被省略，容器将使用root 用户运行
$ kubectl run pod-with-defaults --image alpine --restart Never -- /bin/sleep 999999
//查看用户id
kubectl exec pod -with-defaults --  id
//设置该pod 的securityContext.runAsUser 选项
	apiVersion: v1
kind: Pod
metadata:
  name: pod-as-user-guest
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      runAsUser: 405                  //指定id而不是用户名，405对应guest客户 
	  
//使得pod 中的容器以非root 用户运行，
apiVersion: v1
kind: Pod
metadata:
  name: pod-run-as-non-root
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      runAsNonRoot: true	   //因为alpine以root运行，所以容器不能运行 
	  
//为获取宿主机内核的完整权限，该pod 需要在特权模式下运行。这可以通过将
//容器的securityContext 中的privileged设置为true 实现	  
apiVersion: v1
kind: Pod
metadata:
  name: pod-privileged
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      privileged: true
k exec pod-privileged -- ls /dev

13.2.4 为容器单独添加内核功能
Kubernetes允许为特定的容器添加内核
功能， 或禁用部分内核功能， 以允许对容器进行更加精细的权限控制， 限制攻击者
潜在侵入的影响。
apiVersion: v1
kind: Pod
metadata:
  name: pod-add-settime-capability
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      capabilities:
        add:
        - SYS_TIME  //允许修改时间 
如果需要允许容器修改系统时间， 可以在容器的capb辽1巨es里add 一项名
为CAP_SYS_TIME的功能；Linux内核功能的名称通常以CAP_开头。但在podspec中指定内核功
能时，必须省略CAP_前缀

在pod-with-defaults 中将/tmp 目录的所有
者改为guest 用户
$ kubectl exec pod-with-defaults chown guest /tmp
$ kubectl exec pod-with-defaults -- ls -la / | grep tmp
drwxrwxrwt    2 guest    root             6 May 25 15:18 tmp

在容器的
securityContext.capabilities.drop 列表中加入此项， 以禁用这个修改文
件所有者的内核功能
在容器的
apiVersion: v1
kind: Pod
metadata:
  name: pod-drop-chown-capability
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      capabilities:
        drop:
        - CHOWN
		
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-readonly-filesystem
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      readOnlyRootFilesystem: true    //不允许写入容器根文件系统
    volumeMounts:
    - name: my-volume                //写入卷可以
      mountPath: /volume
      readOnly: false
  volumes:
  - name: my-volume
    emptyDir:		
以上的例子都是对单独的容器设置安全上下文。这些选项中的一部分也可以从
pod级别设定（通过pod.spec.securityCon迳江属性）。它们会作为pod中每
一个容器的默认安全上下文， 但是会被容器级别的安全上下文覆盖;

Kubemetes允许为pod中所有容器指定supplemental组，以允许它们无
论以哪个用户ID运行都可以共享文件。这可以通过以下两个属性设置：
• fsGroup
• supplementalGroups
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-shared-volume-fsgroup
spec:
  securityContext:
    fsGroup: 555                   //加载的卷被组id 555所有 
    supplementalGroups: [666, 777]
  containers:
  - name: first
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      runAsUser: 1111
    volumeMounts:
    - name: shared-volume
      mountPath: /volume
      readOnly: false
  - name: second
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      runAsUser: 2222
    volumeMounts:
    - name: shared-volume
      mountPath: /volume
      readOnly: false
  volumes:
  - name: shared-volume
    emptyDir:
//有效组id 为 0 
$ kubectl exec -it pod-with-shared-volume-fsgroup -c first -- sh
/ $ id
uid=1111 gid=0(root) groups=555,666,777
/ $ ls -l / | grep volume  //group id 555拥有卷 	
drwxrwsrwx    2 root     555              6 May 29 12:23 volume
/ $ echo foo > /volume/foo   //在卷中创建文件
/ $ ls -l /volume
total 4
-rw-r--r--    1 1111     555              4 May 29 12:25 foo
//不在卷中创建文件 
/ $ echo foo > /tmp/foo
/ $ ls -l /tmp
total 4
-rw-r--r--    1 1111     root             4 May 29 12:41 foo

13.3 限制pod使用安全相关的特性
PodSecurityPolicy 是一种集群级别（无命名空间）的资源， 它定义了用户能否
在pod 中使用各种安全相关的特性。维护PodSecurityPolicy 资源中配置策略的工作
由集成在API 服务器中的PodSecurityPolicy 准入控制插件完成

当有人向API 服务器发送pod 资源时， PodSecurityPolicy 准入控制插件会将这
个pod 与已经配置的PodSecurityPolicy 进行校验。如果这个pod 符合集群中已有安
全策略， 它会被接收并存入etcd; 否则它会立即被拒绝。这个插件也会根据安全策
略中配置的默认值对pod 进行修改。

apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: default
spec:
  hostIPC: false
  hostPID: false
  hostNetwork: false
  hostPorts:
  - min: 10000
    max: 11000                  //闭区间
  - min: 13000
    max: 14000
  privileged: false
  readOnlyRootFilesystem: true
  runAsUser:

    rule: RunAsAny

  fsGroup:

    rule: RunAsAny

  supplementalGroups:

    rule: RunAsAny

  seLinux:

    rule: RunAsAny

  volumes:

  - '*'
 没有对容器运行时可以使用的用户和用户组施加任何限
制，因为它们在runAsUser、fsGroup、supplementalGroups等字段中使用
，了runAsAny规则。如果需要限制容器可以使用的用户和用户组ID, 可以将规则
改为MustRunAs, 并指定允许使用的ID范围。
runAsUser:
    rule: MustRunAs
    ranges:
    - min: 2
      max: 2
  fsGroup:
    rule: MustRunAs
    ranges:
    - min: 2
      max: 10
    - min: 20
      max: 30
  supplementalGroups:
    rule: MustRunAs
    ranges:
    - min: 2
      max: 10
    - min: 20
      max: 30
部署镜像中用户ID 在指定范围之外的pod，PSP策略会覆盖被硬编码在进行中的user ID;

以下三个字段会影响容器可以使用的
内核功能：
• allowedCapabilities
• defaultAddCapabilities
• requiredDropCapabilities

apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
spec:
  allowedCapabilities:    //允许容器spec.securityContext.capabilities添加内核功能
  - SYS_TIME
  defaultAddCapabilities:  //自动添加，容器可以显示禁用 
  - CHOWN
  requiredDropCapabilities: //禁用，security-Context.capabilities.drop 
  - SYS_ADMIN
  - SYS_MODULE
  ...


//允许在pod中使用哪些卷 
kind: PodSecurityPolicy
spec:
  volumes:
  - emptyDir
  - configMap
  - secret
  - downwardAPI
  - persistentVolumeClaim
  
13.3.5 对不同的用户与组分配不同的PodSe cu rityPol icy
Assigning different policies to different users is done through the 
RBAC mechanism described in the previous chapter. The idea is to create 
as many policies as you need and make them available to individual users 
or groups by creating ClusterRole resources and pointing them to the individual 
policies by name. By binding those ClusterRoles to specific users or groups 
with ClusterRoleBindings, when the PodSecurityPolicy Admission Control plugin 
needs to decide whether to admit a pod definition or not, it will only consider 
the policies accessible to the user creating the pod.

//minikube以cluster-admin登录,查看default psp 
//创建psp
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: privileged
spec:
  privileged: true
  runAsUser:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  volumes:
  - '*'
//创建default psp对应的role
$ kubectl create clusterrole psp-default --verb=use --resource=podsecuritypolicies --resource-name=default
clusterrole "psp-default" created
//另一个role 
$ kubectl create clusterrole psp-privileged --verb=use  --resource=podsecuritypolicies --resource-name=privileged
clusterrole "psp-privileged" created

//绑定role
$ kubectl create clusterrolebinding psp-all-users  --clusterrole=psp-default --group=system:authenticated
clusterrolebinding "psp-all-users" created
//绑定到system:authenticated， Admission Control plugin会使用该策略
You’re going to bind the psp-default ClusterRole to all authenticated users, 
not only to Alice. This is necessary because otherwise no one could create any 
pods, because the Admission Control plugin would complain that no policy is in 
place. Authenticated users all belong to the system:authenticated group, so you’ll 
bind the ClusterRole to the group:

//绑定bob 
$kubectl create clusterrolebinding psp-bob --clusterrole=psp-privileged --user=bob
clusterrolebinding "psp-bob" created

本书的附录
A 说明了如何在多个集群和多个上下文中使用kubectl
$ kubectl config set-credentials alice --username=alice --password=password
User "alice" set.
$ kubectl config set-credentials bob --username=bob --password=password
User "bob" set.

//失败！
$ kubectl --user alice create -f pod-privileged.yaml #Forbidden
$ kubectl --user bob create -f pod-privileged.yaml

13.4 隔离pod 的网络
限制pod 可以与其他哪
些pod 通信，来确保pod 之间的网络安全。
是否可以进行这些配置取决于集群中使用的容器网络插件。如果网络插件支持，
可以通过NetworkPolicy 资源配置网络隔离

13.4.1 在一个命名空间中启用网络隔离
在默认情况下， 某一命名空间中的pod 可以被任意来源访问。
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector:
在任何一个特定的命名空间中创建该N etworkPo li cy 之后，任何客户端都不能
访问该命名空间中的pod 。
注意集群中的CNI 插件或其他网络方案需要支持NetworkPolicy ，否则
NetworkPolicy 将不会影响pod 之间的可达性  

13.4.2 允许同一命名空间中的部分pod 访问一个服务端pod
为了允许同一命名空间中的客户端pod 访问该命名空间的pod ，需要指明哪些
pod 可以访问
//在数据库pod 所在的命名空间中创建
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: postgres-netpolicy
spec:
  podSelector:
    matchLabels:
      app: database
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: webserver
    ports:
    - port: 5432
NetworkPolicy 允许具有app=webserver 标签的pod 访问具有
app=database 的pod 的访问，并且仅限访问5432 端口

13.4.3 在不同Kubernetes 命名空间之间进行网络隔离
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: shoppingcart-netpolicy
spec:
  podSelector:
    matchLabels:
      app: shopping-cart     //目的app
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          tenant: manning    //筛选from app
    ports:
    - port: 80
以上NetworkPolicy保证了只有具有tenant：manning 标签的命名空间中运行
的pod 可以访问Shopping Cai1微服务；
注意在多租户的Kubernetes集群中，通常租户不能为他们的命名空间添加标
签（或注释）。否则，他们可以规避基于namespaceSelector的入向规则

13.4.4 使用CIDR隔离网络
only be accessible from IPs in the 192.168.1.1 to .255 range
ingress:
  - from:
    - ipBlock:
        cidr: 192.168.1.0/24
		
13.4.5 限制pod的对外访问流量		
spec:
  podSelector:
    matchLabels:
      app: webserver
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: database
以上的NetworkPolicy仅允许具有标签app=webserver的pod访问具有标签
app= database的pod, 除此之外不能访问任何地址（不论是其他pod, 还是任何
其他的IP, 无论在集群内部还是外部）。

-------------------------C14------------------------------
Chapter 14. Managing pods’ computational resources
14.1 为pod中的容器申请资源
创建一个pod 时， 可以指定容器对CPU 和内存的资源请求量（ 即
requests), 以及资源限制量（即Lim心）。它们并不在pod 里定义， 而是针对每个容
器单独指定。pod 对资源的请求量和限制量是它所包含的所有容器的请求量和限制
量之和。

apiVersion: v1
kind: Pod
metadata:
  name: requests-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main
    resources:
      requests:
        cpu: 200m
        memory: 10Mi
通过设置资源requests我们指定了pod对资源需求的最小值

调度器如何判断一个pod是否适合调度到某个节点
这里比较重要而且会令人觉得意外的是，调度器在调度时并不关注各类资源在
当前时刻的实际使用噩，而只关心节点上部署的所有pod 的资源申请量之和。尽管
现有pods 的资源实际使用量可能小于它的申请量，但如果使用基于实际资源消耗量
的调度算法将打破系统为这些已部署成功的pods提供足够资源的保证

两个基于资源请求量的优先级排序函数： LeastRequestedPriority和
MostReques七edPriority。前者优先将pod 调度到请求量少的节点上（也就是
拥有更多未分配资源的节点）， 而后者相反， 优先调度到请求量多的节点（拥有更少
未分配资源的节点）。但是， 正如我们刚刚解释的， 它们都只考虑资源请求量， 而
不关注实际使用资源量。

$ kubectl exec -it requests-pod top
//查看资源总量 
$ kubectl describe nodes
Name:       minikube
...
Capacity:
  cpu:           2
  memory:        2048484Ki
  pods:          110
Allocatable:
  cpu:           2
  memory:        1946084Ki
  pods:          110
...
$ kubectl run requests-pod-2 --image=busybox --restart Never
➥  --requests='cpu=800m,memory=20Mi' -- dd if=/dev/zero of=/dev/null
pod "requests-pod-2" created

14.1.3 CPU requests如何影响CPU时间分配
CPU requests不仅仅在调度时起作用，它还决定着剩余（未使用） 的CPU时间
如何在pod之间分配。正如图14.2描绘的那样，因为第一个pod 请求了200毫核，
另一个请求了1000毫核，所以未使用的CPU将按照1:5的比例来划分给这两个
pod。如果两个pod 都全力使用CPU,第一个pod 将获得16.7%的CPU时间，另一
个将获得83.3%的CPU时间

另一方面，如果一个容器能够跑满CPU,而另一个容器在该时段处于空闲状态，
那么前者将可以使用整个CPU时间（当然会减掉第二个容器消耗的少量时间）。

第二个容器需要CPU时间的时候就会获取到，同时第一个容器会被限制回来。

14.1.4 定义和申请自定义资源
Defining and requesting custom resources
一个自定义资源的例子就是节点上可用的GPU 单元数量。

14.2 限制容器的可用资源
CPU 是一种可压缩资源，意味着我们可以在不对容器内运行的进程产生不利影
响的同时，对其使用量进行限制。而内存明显不同一一是一种不可压缩资源。一旦
系统为进程分配了一块内存，这块内存在进程主动释放之前将无法被回收。这就是
我们为什么需要限制容器的最大内存分配量的根本原因。

apiVersion: v1
kind: Pod
metadata:
  name: limited-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main
    resources:
      limits:
        cpu: 1
        memory: 20Mi
resource requests等于 resource limits

Overcommitting limits
与资源requests 不同的是，资源limits 不受节点可分配资源量的约束。所有
limits 的总和允许超过节点资源总量的100%；如果节点资源使用量超过100% ， 一些容器将被杀掉， 这是一个很重要的
结果

//对于容器而言，当获取内存而不得时，相当于系统内存耗尽，程序出现错误，
//因此，k8s重启是合理的
而内存却有所不同。当进程尝试申请分配比限额更多的内存时会被杀掉（我们
会说这个容器被OOMKilled 了， OOM 是Out 仁’ Memory 的缩写〉。如果pod 的重
启策略为Always 或OnFailure ，进程将会立即重启，因此用户可能根本察觉不
到它被杀掉。但是如果它继续超限并被杀死， Kub erne tes 会再次尝试重启，并开始
增加下次重启的间隔时间。这种情况下用户会看到pod 处于CrashLoopBackOf f
状态：
$ kubectl get po
NAME        READY     STATUS             RESTARTS   AGE
memoryhog   0/1       CrashLoopBackOff   3          1m

一旦间隔时间达到3 00 秒， Kubelet 将以5 分钟为间隔时间对容器进行无限重启，直到容器正常运行
或被删除。

要定位容器crash 的原因，可以通过查看pod 日志以及kubectl describe
pod 命令

14.2.3 容器中的应用如何看待li m i t s
k apply -f limited-pod.yaml
k exec -it limited-pod -- top
//貌似内存limit没什么用 
在容器内看到的始终是节点的内存， 而不是容器本身的内存
The top command shows the memory amounts of the whole node the container is running on. 
Even though you set a limit on how much memory is available to a container, the container 
will not be aware of this limit.

不要依赖应用程序从系统获取的CPU 数量，你可能需要使用Downward API 将
CPU 限额传递至容器并使用这个值。也可以通过cgroup 系统直接获取配置的CPU
限制，请查看下面的文件：
• /sys/fs/cgroup/cpu/cpu .cfs quota_ us
• /sys/fs/cgroup/cpu/cpu.cfs period_ us

14.3 了解pod QoS等级
Kubernetes 将
pod 划分为3 种QoS 等级：
• BestEffort （优先级最低）
• Burstable
• Guaranteed （优先级最高）

没有设置任何requests 和limits 的pod的优先级是BestEffort；

Guaranteed级别的pod，对于pod内的每个container:
• CPU 和内存都要设置requests 和limits
． 每个容器都需要设置资源量
· 它们必须相等（每个容器的每种资源的requests 和limits 必须相等）
如果容器的资源requests 没有显式设置，默认与limits 相同

Burstable：不能归入上述两种的情况 

如果一开始从容器
级别考虑QoS C 尽管它并不是容器的属性，而是pod 的属性），然后从容器Q oS 推
导出pod QoS ，这样可能更容易理解。

对千多容器pod, 如果所有的容器的QoS等级相同， 那么这个等级就是pod的
QoS等级。如果至少有一个容器的QoS等级与其他不同，无论这个容器是什么等级，
这个pod的QoS等级都是Burstable等级


注意运行kubectl describe pod以及通过pod的YAML/JSON描述的
Status.qosClass字段都可以查看pod的QoS等级。

//杀死pod
在一个超卖的系统， QoS等级决定着哪个容器第一个被杀掉， 这样释放出的资
源可以提供给高优先级的pod使用。BestEffort等级的pod首先被杀掉， 其次是
Burstable pod, 最后是Guaranteed pod。Guaranteedpod只有在系统进程
需要内存时才会被杀掉。

如何处理相同QoS等级的容器
每个运行中的进程都有一个称为OutOfMemory (OOM)分数的值。系统通过比
较所有运行进程的OOM分数来选择要杀掉的进程。当需要释放内存时， 分数最高
的进程将被杀死。

OOM scores are calculated from two things: the percentage of the available memory 
the process is consuming and a fixed OOM score adjustment, which is based on the pod’s 
QoS class and the container’s requested memory.

对于两个属于Burstable等级的单容器的pod, 系统会杀掉内存实际使用量占内存申请量比例
更高的pod

14.4 为命名空间中的pod 设置默认的requests 和limits
用户可以通过创建一个LimitRange资源来避免必须配置每个容器。L血itRange
资源不仅允许用户（为每个命名空间）指定能给容器配置的每种资源的最小和最大
限额， 还支持在没有显式指定资源requests 时为容器设置默认值，

L皿itRange资源被LimitRanger准入控制插件（我们在第11章介绍过这种插件）。
API服务器接收到带有pod描述信息的POST请求时， LimitRanger插件对pod spec
进行校验。如果校验失败， 将直接拒绝。因此， LimitRange对象的一个广泛应用场
景就是阻止用户创建大千单个节点资源量的pod。如果没有LimitRange, API服务
器将欣然接收pod创建请求， 但永远无法调度成功。

LimtRange资源中的limit s应用于同一个命名空间中每个独立的pod、容器，
或者其他类型的对象。它并不会限制这个命名空间中所有pod可用资源的总量， 总
量是通过Resou rceQuota对象指定的

apiVersion: v1
kind: LimitRange
metadata:
  name: example
spec:
  limits:
  - type: Pod
    min:
      cpu: 50m
      memory: 5Mi
    max:
      cpu: 1
      memory: 1Gi
  - type: Container
    defaultRequest:     //为没有设置requests的容器设定默认的cpu和内存requests

      cpu: 100m

      memory: 10Mi

    default:          //为没有设置limits的容器设定默认的cpu和内存limits

      cpu: 200m

      memory: 100Mi

    min:

      cpu: 50m

      memory: 5Mi

    max:

      cpu: 1

      memory: 1Gi

    maxLimitRequestRatio:
      cpu: 4
      memory: 10
  - type: PersistentVolumeClaim
    min:
      storage: 1Gi
    max:
      storage: 10Gi

由于L血itRange对象中配置的校验（和默认值）信息在API服务器接收到新的
pod或PVC创建请求时执行，如果之后修改了限制，已经存在的pod和PVC 将不
会再次进行校验，新的限制只会应用于之后创建的pod和PVC 。

在每个命
名空间中定义不同的LimitRange就可以确保只在特定的命名空间中可以创建资源需
求大的pod, 而在另一些命名空间中只能创建资源需求小的pod。

14.5 限制命名空间中的可用资源总量
ResourceQuota的接纳控制插件会检查将要
创建的pod是否会引起总资源量超出ResourceQuota。如果那样，创建请求会被拒绝。
因为资源配额在pod 创建时进行检查，所以ResourceQuota对象仅仅作用于在其后
创建的pod— — 并不影响已经存在的pod。
资源配额限制了一个命名空间中pod和PVC存储最多可以使用的资源总址。同
时也可以限制用户允许在该命名空间中创建pod、PVC, 以及其他API对象的数量，
因为到目前为止我们处理最多的资源是CPU和内存

LimitRange应用于单独的pod ; ResourceQuota应用千命名空间中所有的pod
$ kubectl describe po kubia-manual
Name:           kubia-manual
...
Containers:
  kubia:
    Limits:
      cpu:      200m
      memory:   100Mi
    Requests:
      cpu:      100m
      memory:   10Mi

$ kubectl describe quota

创建ResourceQuota时往往还需要随之创建一个
LimitRange对象。当特定资源(CPU或内存）配置了(requests或limits)配额， 在pod中
必须为这些资源（分别〉指定requests 或limits ，否则API 服务器不会接收该pod 的
创建请求。

apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage
spec:
  hard:
    requests.storage: 500Gi      #全部可声明的存储为500G
    ssd.storageclass.storage.k8s.io/requests.storage: 300Gi //可声明的SSD为300G
    standard.storageclass.storage.k8s.io/requests.storage: 1Ti  //hdd存储

apiVersion: v1
kind: ResourceQuota
metadata:
  name: objects
spec:
  hard:
    pods: 10
    replicationcontrollers: 5
    secrets: 10
    configmaps: 10
    persistentvolumeclaims: 4
    services: 5
    services.loadbalancers: 1
    services.nodeports: 2
    ssd.storageclass.storage.k8s.io/persistentvolumeclaims: 2 //只有两个pvc能声明ssd类别存储


目前配额作用范围共有4 种：
BestEff ort 、NotBestEffort 、Termination 矛日NotTerminatingo
Best Ef f ort 和NotBestEffort 范围决定配额是否应用于BestEffo rt
QoS 等级或者其他两种等级（ B ur stable 和Guaranteed ） 的pod

//Terminating quto应用于设定了active-Deadline-Seconds的pod,Not-Terminating反之
you can specify how long each pod is allowed to run before it’s terminated and marked as 
Failed. This is done by setting the active-Deadline-Seconds field in the pod spec. This 
property defines the number of seconds a pod is allowed to be active on the node relative 
to its start time before it’s marked as Failed and then terminated. The Terminating quota 
scope applies to pods that have the active-DeadlineSeconds set, whereas the Not-Terminating 
applies to those that don’t.
apiVersion: v1
kind: ResourceQuota
metadata:
  name: besteffort-notterminating-pods
spec:
  scopes:
  - BestEffort
  - NotTerminating
  hard:
    pods: 4

允许最多创建4 个属于BestEffort QoS 等级，并没有设置active
deadline 的pod

14.6 监控pod 的资源使用量
Kubelet 自身就
包含了一个名为cAdvisor 的agent ，它会收集整个节点和节点上运行的所有单独容
器的资源消耗情况。集中统计整个集群的监控信息需要运行一个叫作Heapster 的附
加组件

//Heapster已被废除
minikube addons enable heapster

$ kubect1 top node //GKE 可行 
$ k top pod --all-namespaces
$ k top pod --container=false --all-namespaces

如果使用Google Container Engine, 可以通过
Google Cloud Monitoring 来对集群进行监控， 但是如果是本地Kubemetes 集群（通
过Minikube 或其他方式创建）， 人们往往使用InfluxDB 来存储统计数据， 然后使用
Grafana 对数据进行可视化和分析

lnfluxDB和Grafana 介绍
lnfluxDB 是一个用于存储应用指标， 以及其他监控数据的开源的时序数据库。
Grafana 是一个拥有着华丽的web 控制台的数据分析和可视化套件，同样也是开源的，
它允许用户对InfluxDB 中存储的数据进行可视化， 同时发现应用程序的资源使用行
为是如何随时间变化的

在集群中运行lnfluxDB和Grafana
InfluxDB 和Grafana 都可以以pod 运行， 部署简单方便。所有需要的部署文
件可以在Heapster Git 仓库中获取：http://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb
如果使用Minik:ube 就无须手动部署， 因为启用Heapster 插件时便会随之部署
Heapster

//使用Grafana 分析资源使用量
$ kubectl cluster-info
$ minikube service monitoring-grafana -n kube-system




————————————————————————————————————————————C15————————————————————————————————————————————————
Chapter 15. Automatic scaling of pods and cluster nodes

15.1 pod的横向自动伸缩
//HPA
Horizontal pod autoscaling is the automatic scaling of the number of pod replicas managed by a 
controller. It’s performed by the Horizontal controller, which is enabled and configured by creating 
a HorizontalPodAutoscaler (HPA) resource. The controller periodically checks pod metrics, calculates 
the number of replicas required to meet the target metric value configured in the HorizontalPodAutoscaler 
resource, and adjusts the replicas field on the target resource (Deployment, ReplicaSet, Replication-Controller, 
or StatefulSet).

15.1.1 了解自动伸缩过程
自动伸缩的过程可以分为三个步骤：
• 获取被伸缩资源对象所管理的所有pod度量。
• 计算使度量数值到达（或接近）所指定目标数值所需的pod数量。
• 更新被伸缩资源的rep巨cas字段。

HPA控制器向Heapster 发起REST调用来获取所有pod度量数据

当Autoscaler配置为只考虑单个度量时， 计算所需副本数很简单。只要将所有
pod的度量求和后除以HPA资源上配置的目标值， 再向上取整即可;
基于多个pod度量的自动伸缩（例如： CPU使用率和每秒查询率[QPS])的计
算也并不复杂。Autoscaler单独计算每个度量的副本数， 然后取最大值（例如：如
果需要4个pod达到目标CPU使用率， 以及需要3个pod来达到目标QPS, 那么
Autoscaler 将扩展到4个pod)

Autoscaler控制器通过Scale子资源来修改被伸缩资源的rep巨cas字段。这
样Autoscaler不必了解它所管理资源的细节，而只需要通过Scale子资源暴露的界面，

15.1.2 基于CPU使用率进行自动伸缩
As far as the Autoscaler is concerned, only the pod’s guaranteed CPU amount (the CPU requests) 
is important when determining the CPU utilization of a pod. The Autoscaler compares the pod’s 
actual CPU consumption and its CPU requests, which means the pods you’re autoscaling need to have 
CPU requests set (either directly or indirectly through a LimitRange object) for the Autoscaler to 
determine the CPU utilization percentage.

$ kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5
deployment "kubia" autoscaled
$k get hpa kubia -o yaml

//观察
$ watch -n 1 kubectl get hpa,deployment

$ kubectl run -it --rm --restart=Never loadgenerator --image=busybox -- sh -c "while true; do wget -O - -q http://kubia.default; done"

$while true; do curl 34.92.157.173:8080; done

修改一个已有HPA 对象的目标度量值,
使用kubectl edit 文本编辑器打开之后，把targetAverageUtilization 字段改为60,

15.1.3 基于内存使用进行自动伸缩
基于内存的自动伸缩比基于CPU 的困难很多。主要原因在于，扩容之后原有的
pod 需要有办法释放内存。这只能由应用完成，系统无法代芳。系统所能做的只有
杀死并重启应用，希望它能比之前少占用一些内存；但如果应用使用了跟之前一样
多的内存， Autoscaler 就会扩容、扩容， 再扩容， 直到达到HPA 资源上配置的最大
pod 数量。

//
you have three types of metrics you can use in an HPA object:
Resource
Pods
Object
...
spec:
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      targetAverageUtilization: 30
...
//与pod相关的metric
//The Pods type is used to refer to any other (including custom) metric related to the pod directly
...
spec:
  metrics:
  - type: Pods
    resource:
      metricName: qps
      targetAverageValue: 100
...

//基于其他的object做出决策
//The Object metric type is used when you want to make the autoscaler scale pods based on a 
metric that doesn’t pertain directly to those pods.
...
spec:
  metrics:
  - type: Object
    resource:
      metricName: latencyMillis
      target:
        apiVersion: extensions/v1beta1
        kind: Ingress
        name: frontend
      targetValue: 20
  scaleTargetRef:

    apiVersion: extensions/v1beta1

    kind: Deployment

    name: kubia

...

空载（ idling ）与解除空载（un-idling），即允许提供特定服务的pod 被缩
容量到0副 本。在新的请求到来时，请求会先被阻塞，直到pod 被启动，从而请求被
转发到新的pod 为止。

15.2 pod 的纵向自动伸缩
并不是所有应用都能被横向伸缩。对这些应用而言，唯一的
选项是纵向伸缩一－给它们更多CPU 和（或）内存

这是一个实验性的特性， 如果新创建的pod的容器没有明确设置CPU与内
存请求， 该特性即会代为设置。这一特性由一个叫作lnitialResources的准入控制
(Ad missionC ontrol)插件提供。 当一个没有资源请求的pod被创建时， 该插件会根
据pod 容器的历史资源使用数据（随容器镜像、tag而变）来设置资源请求。

15.3 集群节点的横向伸缩
ClusterA utoscaler负责在由千节点资源不足， 而无法调度某pod到已有节点时，
自动部署新节点。它也会在节点长时间使用率低下的情况下下线节点。
云服务提供者通常把相同规格（或者有相同特性）的节点聚合成组。因此
Cluster Autoscaler不能单纯地说“给我多一个节点”，它还需要指明节点类型。
Cluster Autoscaler 通过检查可用的节点分组来确定是否有至少一种节点类型能
容纳未被调度的pod。如果只存在唯一一个此种节点分组，ClusterAutoscaler就可以
增加节点分组的大小，让云服务提供商给分组中增加一个节点。但如果存在多个满
足条件的节点分组，ClusterAutoscaler就必须挑一个最合适的。这里“ 最合适” 的
精确含义显然必须是可配置的。在最坏的情况下，它会随机挑选一个。图15.5简单
描述了ClusterAutoscaler面对一个不可调度pod时是如何反应的。

归还节点
当节点利用率不足时， Cluster Autoscaler 也需要能够减少节点的数目。Cluster
Autoscaler 通过监控所有节点上请求的CPU 与内存来实现这一点。如果某个节点上
所有pod请求的CPU 、内存都不到50%, 该节点即被认定为不再需要。
//如果系统pod仅仅运行在这个节点，节点不会下线；
//如果未管理的pod或带有本地存储的pod运行在这个节点，节点也不会下线；
//只有pods能调度其他节点的 节点才能下线 
The Autoscaler also checks to see if any system pods are running (only) on that node 
(apart from those that are run on every node, because they’re deployed by a DaemonSet, for example). 
If a system pod is running on a node, the node won’t be relinquished. The same is also true if an 
unmanaged pod or a pod with local storage is running on the node, because that would cause disruption 
to the service the pod is providing. In other words, a node will only be returned to the cloud provider 
if the Cluster Autoscaler knows the pods running on the node will be rescheduled to other nodes.

当一个节点被选中下线，它首先会被标记为不可调度， 随后运行其上的pod 将
被疏散至其他节点

Manually cordoning and draining nodes
//拉起警戒线
kubectl cordon <node> marks the node as unschedulable (but doesn’t do anything with pods running on that node).
kubectl drain <node> marks the node as unschedulable and then evicts all the pods from the node.
In both cases, no new pods are scheduled to the node until you uncordon it again with kubectl uncordon <node>

Kubemetes可以指定下线等操作时需要保待的最少pod数量，
我们通过创建一个podDisruptionBudget资源的方式来利用这一特性。

$ kubectl create pdb kubia-pdb --selector=app=kubia --min-available=3
poddisruptionbudget "kubia-pdb" created

$ kubectl get pdb kubia-pdb -o yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: kubia-pdb
spec:
  minAvailable: 3
  selector:
    matchLabels:
      app: kubia
status:
  ...

也可以用一个百分比而非绝对数值来写minAvailable字段。比方说，可以
指定60%带app=kubia标签的pod应当时刻保持运行。
注意从Kubemetes 1. 7开始，podDismptionBudget资源也支持maxUnavailable。
如果当很多pod不可用而想要阻止pod被剔除时，就可以用maxUnavailable字段
而不是minAvailable。


———————————————————————————————————————————————————C16——————————————————————————————————————
|            Chapter 16. Advanced scheduling                                                |
—————————————————————————————————————————————————————————————————————————————————————————————

16.1. Using taints and tolerations to repel pods from certain nodes

//查看taints
$ kubectl describe node master.k8s #Taints:输出 
污点包含了一个key、value, 以及一个effect, 表现为<key>=<value>:<effect>,
$ kubectl describe node master.k8s
Name:         master.k8s
Role:
Labels:       beta.kubernetes.io/arch=amd64
              beta.kubernetes.io/os=linux
              kubernetes.io/hostname=master.k8s
              node-role.kubernetes.io/master=
Annotations:  node.alpha.kubernetes.io/ttl=0
              volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:       node-role.kubernetes.io/master:NoSchedule
...
上例中的污点包含：
the key node-role.kubernetes.io/master, a null value (not shown in the taint), and the effect of NoSchedule.

#pod容忍度
$ kubect1 describe po kube-proxy-80wqm -n kube-system #gke上去掉 kube-system 

#当节点处在not-ready或 unreachable状态时，pod被允许运行多长时间 
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s

每一个污点都可以关联一个效果， 效果包含了以下三种：
• NoSchedule 表示如果pod 没有容忍这些污点， pod 则不能被调度到包含这
些污点的节点上。
• PreferNoSchedule 是NoSchedule 的一个宽松的版本， 表示尽量阻止
pod 被调度到这个节点上， 但是如果没有其他节点可以调度， pod 依然会被调
度到这个节点上。
• NoExecute 不同于NoSchedule 以及PreferNoSchedule, 后两者只在
调度期间起作用， 而NoExecute 也会影响正在节点上运行着的pod。如果
在一个节点上添加了NoExecute 污点， 那些在该节点上运行着的pod, 如
果没有容忍这个NoExecute 污点， 将会从这个节点去除。

//给节点添加污点
$ kubectl taint node node1.k8s node-type=production:NoSchedule
node "node1.k8s" tainted
//To remove the taint added by the command above, you can run:
kubectl taint nodes node1 key=value:NoSchedule- //后面一个破折后

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: prod
spec:
  replicas: 5
  template:
    spec:
      ...
      tolerations:
      - key: node-type
        operator: Equal
        value: production
        effect: NoSchedule


//Equal操作符匹配污点的key的特定值；Exists操作符能容忍任何操作符 
taints can only have a key and an effect and don’t require a value. Tolerations can tolerate a 
specific value by specifying the Equal operator (that’s also the default operator if you don’t specify one), 
or they can tolerate any value for a specific taint key if you use the Exists operator.

//配置时间; 默认5分钟 
You can also use a toleration to specify how long Kubernetes should wait before rescheduling a pod to 
another node if the node the pod is running on becomes unready or unreachable.
$ kubectl get po prod-350605-1ph5h -o yaml
...
  tolerations:
  - effect: NoExecute
    key: node.alpha.kubernetes.io/notReady
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.alpha.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
注意当前这是一个alpha阶段的特性，在未来的Kubemetes版本中可能会有所
改变。基于污点信息的pod剔除也不是默认启用的， 如果要启用这个特性， 需要在
运行控制器管理器时使用--feature-gates=TaintBasedEvictions=true
选项。

节点亲缘性根据节点的标签来进行选择
failure-domain.beta.kubernetes.io/region=asia-east2 //地理区域
failure-domain.beta.kubernetes.io/zone=asia-east2-a //可用性区域 
kubernetes.io/hostname=gke-k8s-learn-default-pool-ae3320ed-pf25 //主机名 

node affinity, which allows you to tell Kubernetes to schedule pods only to specific subsets of nodes.
apiVersion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution: //required,强制性亲缘性规则 
        nodeSelectorTerms:
        - matchExpressions:
          - key: gpu         //标签为 gpu 值为 true 
            operator: In
            values:
            - "true"

nodeSelectorTerms 和matchExpressions 字段，这两个宇段定义了节点的标签必须满足哪一种表达式，
才能满足pod 调度的条件

给节点加上标签，首先，节点必须加上合适的标签。每个节点需要包含两个标签， 一个用于表示
所在的这个节点所归属的可用性区域，另一个用于表示这是一个独占的节点还是一
个共享的节点。
$ kubectl label node node1.k8s availability-zone=zone1
node "node1.k8s" labeled
$ kubectl label node node1.k8s share-type=dedicated
node "node1.k8s" labeled
$ kubectl label node node2.k8s availability-zone=zone2
node "node2.k8s" labeled
$ kubectl label node node2.k8s share-type=shared
node "node2.k8s" labeled
$ kubectl get node -L availability-zone -L share-type
NAME         STATUS    AGE       VERSION   AVAILABILITY-ZONE   SHARE-TYPE
master.k8s   Ready     4d        v1.6.4    <none>              <none>
node1.k8s    Ready     4d        v1.6.4    zone1               dedicated
node2.k8s    Ready     4d        v1.6.4    zone2               shared

//p475 gke运行 出错 
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: pref
spec:
  template:
    ...
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution: #优先级节点亲缘
          - weight: 80
            preference:
              matchExpressions:
              - key: availability-zone
                operator: In
                values:
                - zone1
          - weight: 20
            preference:
              matchExpressions:
              - key: share-type
                operator: In
                values:
                - dedicated
      ...
按照优先级，节点可被调度到带有这些标签的节点(排列组合）：
zone1/dedicated,zone1/shared,zone2/dedicated,zone2/shared
除了节点亲缘性的优先
级函数，调度器还是使用其他的优先级函数来决定节点被调度到哪。其中之一就是
Selector SpreadPriority函数，这个函数确保了属于同一个ReplicaSet或者
Service 的pod,将分散部署在不同节点上，以避免单个节点失效导致这个服务也宅机。
这就是有1个pod被调度到node2 的最大可能。

16.3 使用pod亲缘性与非亲缘性对pod进行协同部署
$ kubectl run backend -l app=backend --image busybox -- sleep 999999
deployment "backend" created

如果现在你删除了后端pod, 调度器会将该pod 调度到node2, 即
便后端pod 本身没有定义任何pod 亲缘性规则（只有前端pod 设置了规则）。这种
情况很合理， 因为假设后端pod 被误删除而被调度到其他节点上， 前端pod 的亲缘
性规则就被打破了。

16.3.2 将pod部署在同一机柜、可用性区域或者地理地域
了解topologyKey是如何工作的
//选择一个与正在被部署pod的label selector匹配的pod, 再查看结果pod所在的node的lablel,该node
//lable要匹配topologyKey域
When the Scheduler is deciding where to deploy a pod, it checks the pod’s pod-Affinity config, 
finds the pods that match the label selector, and looks up the nodes they’re running on. Specifically, 
it looks up the nodes’ label whose key matches the topologyKey field specified in podAffinity. 
Then it selects all the nodes whose label matches the values of the pods it found earlie

在同一个可用性区域中协同部署pod
topologyKey属性设置为failure-domain.beta.kubernetes.io/zone,

在同—个地域中协同部署pod
topologyKey属性设置为failure-domain.beta.kubernetes.io/region

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 5
  template:
    ...
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app: backend
      containers: ...

16.3.4 利用pod的非亲缘性分开调度pod
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 5
  template:
    ...
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            podAffinityTerm:                 #podAffinityTerm
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app: backend
      containers: ...



——————————————————————————————————————C17——————————————————————————————————---
Chapter 17. Best practices for developing apps
除了标签，大多数的资源还包含一个描述资源的注解，列出负责该资
源的人员或者团队的联系信息，或者为管理者和其他的工具提供额外的元数据。

17.2 了解pod 的生命周期
//无状态应用
应用开发者必须允许他们的应用可以被相对频繁地迁移。

有状态应用可以通过一个S tatefu!Set 来
运行， S tatefu!Set 会保证在将应用调度到新的节点并启动之后，它可以看到和之前
一样的主机名和持久化状态。当然pod 的IP 还是会发生变化，应用必须能够应对这
种变化。因此应用开发者在一个集群应用中不应该依赖成员的四地址来构建彼此的
关系，另外如果使用主机名来构建关系，必须使用Statefu !Set 。

ReplicaSet 本身并不关心pod 是否
处于死亡状态，它只关心pod 的数量是否匹配期望的副本数量，在这种情况下，副
本数量确实是匹配的。

//POD 正在running不能说明容器也成功running 
容器将会每5分钟重启
一次，在这个过程中Kubemetes期望崩溃的底层原因会被解决。这个机制依据的基
本原理就是将pod重新调度到其他节点通常并不会解决崩溃的问题，因为应用运行
在容器的内部，所有的节点理论上应该都是相同的

Kubemetes API服务器确实是按照YAML/JSON文件中定义的对象的顺序来进
行处理的，但是仅仅意味着它们在被写入到etcd的时候是有顺序的。

//将重要的关闭流程替换为专注于关闭流程的pod
一个解决方案是让应用（在接收到终止信号的时候）创建一个新的Job资源，
这个Job资源会运行一个新的pod, 这个pod唯一的工作就是把被删除的pod的数
据迁移到仍然存活的pod。但是如果你注意到的话， 你就会了解你无法保证应用每
次都能够成功创建这个Job对象。万一当应用要去创建Job的时候节点出现故障呢？
这个问题的合理的解决方案是用一个专门的持续运行中的pod 来持续检查是否
存在孤立的数据。当这个pod发现孤立的数据的时候， 它就可以把它们迁移到仍存
活的pod。当然不一定是一个持续运行的pod, 也可以使用CronJob资源来周期性地
运行这个pod。

//对于stateful set , app upgrade时会删除旧pod 创建新 pod  ， 当cronjos等待时， 新的pod重新绑定到持久卷
你或许以为Statefu!Set在这里会有用处， 但实际上并不是这样。如你所记
起的那样， 给StatefulSet缩容会导致PersistentVolumeClaim处于孤立状态， 这会
导致存储在PersistentVolumeClaim 中的数据搁浅。当然， 在后续的扩容过程中，
Persistent Volume会被附加到新的pod实例中， 但是万一这个扩容操作永远不会发生
（或者很久之后才会发生）呢？因此， 当你在使用Statefu!Set的时候或许想运行一个
数据迁移的pod (这种场景如图17.6所示）。为了避免应用在升级过程中出现数据
迁移， 专门用于数据迁移的pod可以在数据迁移之前配置一个等待时间， 让有状态
的pod有时间启动起来。

17.3 确保所有的客户端请求都得到了妥善处理
你需要做的是当且仅当你的应用准备好处理进来的请求的时候， 才去让就绪探
针返回成功。好的实践第一步是添加一个指向应用根URL 的HTTP GET 请求的就
绪探针

等待足够长的时间让所有的kube-proxy 可以完成它们的工作。
简要概括一下，妥善关闭一个应用包括如下步骤：
·等待几秒钟，然后停止接收新的连接。
·关闭所有没有请求过来的长连接。
·等待所有的请求都完成。
·然后完全关闭应用
lifecycle:
      preStop:
        exec:
          command:
          - sh
          - -c
          - "sleep 5"

17.4 让应用在Kubernetes 中方便运行和管理
如果你使用Go 语言来构建应用，你的镜像除了应用的可执行二进制文件
外不需要任何东西。这样基于Go 语言的容器镜像就会非常小，很适合Kubemetes 。
提示在这些镜像的Dockerfi l e 中使用FROM scratch 指令

必须使用能够指明具体版本的标签而不是latest 。记住如果你使用的
是可更改的标签（总是向相同的标签推送更改），那么你需要在pod spec 中将
imagePullPolicy 设置为Always

17.4.3 使用多维度而不是单维度的标签
别忘了给所有的资源都打上标签，而不仅仅是pod
17.4.4 通过注解描述每个资源
可以使用注解来给你的资源添加额外的信息

17.4.5 给进程终止提供更多的信息
可以让容器中的进程向容器的文件系
统中指定文件写入一个终止消息。这个文件的内容会在容器终止后被Kubelet 读取，
然后显示在kubectl describe pod 中;进程需要写入终止消息的文件默认路径是／dev/term ination-log，当然这个
路径也可以在pod spec 中容器定义的部分设置terminationMessagePath 宇段
来自定义

这个机制并不仅仅适用于崩溃的容器。
如果容器皮有向任何文件写入消息，可以将迳rmina巨onMessage
Policy字段的值设置为FallbackToLogsOnError。在这种情况下，容器的最
后几行日志会被当作终止消息（当然仅当容器没有成功终止的情况下）。

17.4.6 处理应用日志
$ kubectl exec <pod> cat <logfile> //查看日志 

//来回复制日志
//-c containerName option. 指定容器
$ kubectl cp foo-pod:/var/log/foo.log foo.log
$ kubectl cp localfile foo-pod:/etc/remotefile
//集群级日志解决方案
在Google 的Kubemetes引擎上， 这个就更加简单
了。在设置集群的时候选中"Enable Stackdriver Logging"选项即可

//EFK,ELK 
你或许已经听说过由ElasticSearch、Logstash和Kibana组成的ELK栈。一个
稍微更改的变种是EFK栈， 其中Logstash 被FluentD替换了。
当使用EFK作为集中式日志记录的时候， 每个Kubemetes集群节点都会运行
一个FluentD 的代理（ 通过使用DaemonSet作为pod来部署）， 这个代理负责从容
器搜集日志， 给日志打上和pod相关的信息， 然后把它们发送给ElasticSearch, 然
后由ElasticSearch 来永久地存储它们。ElasticSearch在集群中也是作为pod部署
的。这些日志可以通过Kibana在Web浏览器中查看和分析， K巾ana是一个可视化

//多行日志输出
为了解决这个问题， 可以让应用日志输出JSON格式的内容而不是纯文本。这
样的话， 一个多行的日志输出就可以作为一个条目进行存储了， 也可以在Kibana中
以一个条目的方式显示出来。但是这种做法会让通过kubec七l logs命令查看日
志变得不太人性化了。
解决方案或许是输出到标准输出终端的日志仍然是用户可读的日志， 但是写
入日志文件供FluentD处理的日志是JSON格式。这就要求在节点级别合理地配置
FluentD代理或者给每一个pod增加一个轻量级的日志记录容器。

连接到后台服务
在生产环境中， 如果应用连接到后台服务， 并且使用环境变量BACKEND—
SERVICE—HOST和BACKEND_SERVICE_PORT来查找服务的协调者， 可以很容易
地在本地机器上手动设置这些环境变量， 并且把它们指向到后台服务， 不管这个后
台服务是在Kubemetes里面还是外面运行。如果这个服务在Kubemetes里面运行，
总是可以（至少是临时的） 把这个服务改为NodePo江或者LoadBalancer类型
来让这个服务对外可访问

如果你正在使用Minikube进行开发， 并且希望在Kubemetes集群中试验应用
的每个更改， 可以使用mi工kube mount命令将本地的文件系统挂载到Minikube
VM中， 然后通过一个hostPath卷挂载到容器中。可以在Minikube文档中找到
更多的使用说明， 文档链接在 htpt s://github.c01n/kubernetes/minikube/tree/ master/docs
上。

在MinikubeVM中使用DockerDaemon来进行镜像构建，
将你的DOCKER_HOST环境变量指向它
$ eval ${minikube docker-env)

在本地构建镜像然后直接复制到Minikube 
将镜
像直接复制到MinikubeVM中：
$ docker save <image> | (eval $(minikube docker-env) && docker load)
保podspec 中
的imagePullPo巨cy不要设置为Always, 因为这会导致从外部镜像中心拉取镜
像， 从而导致你复制过去的镜像的更改丢失

//版本控制系统
在每次提交更改之后， 可以使用kubectl apply命令将更改反映到部署的资源中
kube-applier

Ksonnet、Jsonnet工具



———————————————————————————————————————C18————————————————————————————————————————
Chapter 18. Extending Kubernetes

18.1.1 CustomResourceDefinitions 介绍
开发者只需向Kubemetes API 服务器提交CRD 对象， 即可定义新的资源类型。
成功提交CRD 之后， 我们就能够通过API 服务器提交JSON 清单或YAML 清单的
方式创建自定义资源，以及其他Kubemetes 资源实例

//这个格式不适用了 
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: websites.extensions.example.com
spec:
  scope: Namespaced
  group: extensions.example.com
  version: v1
  names:

    kind: Website

    singular: website

    plural: websites

18.1.2 使用自定义控制器自动定制资源

18.2. Extending Kubernetes with the Kubernetes Service Catalog
//创建Service Broker
apiVersion: servicecatalog.k8s.io/v1alpha1
kind: ClusterServiceBroker
metadata:
  name: database-broker
spec:
  url: http://database-osbapi.myorganization.org
在检索服务列表后， 就会为每个服务创建一个ClusterServiceCiass资源。每个
ClusterServiceClass资源都描述了一种可供应的服务(" Post担·eSQL数据库” 就是
ClusterServiceClass的一个典型例子）。每个ClusterServiceCiass都有一个或多个与之
关联的服务方案

//检索服务列表
$ kubectl get clusterserviceclasses
NAME                KIND
postgres-database   ClusterServiceClass.v1alpha1.servicecatalog.k8s.io
mysql-database      ServiceClass.v1alpha1.servicecatalog.k8s.io
mongodb-database    ServiceClass.v1alpha1.servicecatalog.k8s.io

//查看服务yaml
$ kubectl get serviceclass postgres-database -o yaml
apiVersion: servicecatalog.k8s.io/v1alpha1
bindable: true
brokerName: database-broker
description: A PostgreSQL database
kind: ClusterServiceClass
metadata:
  name: postgres-database
  ...
planUpdatable: false
plans:
- description: A free (but slow) PostgreSQL instance
  name: free
  osbFree: true
  ...
- description: A paid (very fast) PostgreSQL instance
  name: premium
  osbFree: false
  ...

18.2.4 提供服务与使用服务
提供服务实例
要想预分配数据库，需要做的是创建一个Servicelnstance 资源
apiVersion: servicecatalog.k8s.io/v1alpha1
kind: ServiceInstance
metadata:
  name: my-postgres-db
spec:
  clusterServiceClassName: postgres-database
  clusterServicePlanName: free
  parameters:
    init-db-args: --data-checksums

//查看实例创建结果 
$ kubectl get instance my-postgres-db -o yaml
apiVersion: servicecatalog.k8s.io/v1alpha1
kind: ServiceInstance
...
status:
  asyncOpInProgress: false
  conditions:
  - lastTransitionTime: 2017-05-17T13:57:22Z
    message: The instance was provisioned successfully
    reason: ProvisionedSuccessfully
    status: "True"
    type: Ready

//Binding a ServiceInstance，创建ServiceBindingresource。
apiVersion: servicecatalog.k8s.io/v1alpha1
kind: ServiceBinding
metadata:
  name: my-postgres-db-binding
spec:
  instanceRef:
    name: my-postgres-db
  secretName: postgres-secret

在你将ServiceBinding资源从先前的列表提交到服务目录API服务器时， 控制
器会再次联系数据库代理， 并为之前配置的Servicelnstance创建一个绑定。作为响
应， 这时候代理会返回以连接到数据库所需的凭证和其他数据。随后， 服务目录会
使用在ServiceBinding资源中指定的名称创建一个新的Secret, 并将所有数据存储在
Secret中

在客户端pod中使用新创建的Secret
$ kubectl get secret postgres-secret -o yaml
apiVersion: v1
data:
  host: <base64-encoded hostname of the database>
  username: <base64-encoded username>
  password: <base64-encoded password>
kind: Secret
metadata:
  name: postgres-secret
  namespace: default
  ...
type: Opaque

$ kubectl delete servicebinding my-postgres-db-binding
servicebinding "my-postgres-db-binding" deleted

$ kubectl delete serviceinstance my-postgres-db
serviceinstance "my-postgres-db " deleted

